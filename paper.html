

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Macrolitter video counting on riverbanks using state space models and moving cameras &#8212; Macrolitter video counting on riverbanks using state space models and moving cameras</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'paper';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="#">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Macrolitter video counting on riverbanks using state space models and moving cameras - Home"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Macrolitter video counting on riverbanks using state space models and moving cameras - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1 current active">
                <a class="reference internal" href="#">
                    Macrolitter video counting on riverbanks using state space models and moving cameras
                </a>
            </li>
        </ul>
        
    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/paper.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Macrolitter video counting on riverbanks using state space models and moving cameras</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#abstract">Abstract</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#related-works">Related works</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ai-automated-counting">AI-automated counting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#computer-vision-for-macro-litter-monitoring">Computer vision for macro litter monitoring</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-object-tracking">Multi-object tracking</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#datasets-for-training-and-evaluation">Datasets for training and evaluation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#images">Images</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#data-collection">Data collection</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#bounding-box-annotation">Bounding box annotation</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#video-sequences">Video sequences</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id30">Data collection</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#track-annotation">Track annotation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optical-flow-based-counting-via-bayesian-filtering-and-confidence-regions">Optical flow-based counting via Bayesian filtering and confidence regions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#detector">Detector</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#center-based-anchor-free-detection">Center-based anchor-free detection</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training">Training</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-tracking-with-optical-flow">Bayesian tracking with optical flow</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#optical-flow">Optical flow</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#state-space-model">State space model</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#approximations-of-the-filtering-distributions">Approximations of the filtering distributions</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#generating-potential-object-tracks">Generating potential object tracks</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-association-using-confidence-regions">Data association using confidence regions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#counting">Counting</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#metrics-for-mot-based-counting">Metrics for MOT-based counting</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#count-related-mot-metrics">Count-related MOT metrics</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#detection">Detection</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#association">Association</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#count-metrics">Count metrics</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#count-decomposition">Count decomposition</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#statistics">Statistics</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#experiments">Experiments</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id53">Detection</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#counts">Counts</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#sequence-1">Sequence 1</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#sequence-2">Sequence 2</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#sequence-3">Sequence 3</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#combined-sequences">Combined sequences</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#detailed-results-on-individual-segments">Detailed results on individual segments</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#practical-impact-and-future-goals">Practical impact and future goals</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#supplements">Supplements</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#details-on-the-image-dataset">Details on the image dataset</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#categories">Categories</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#details-on-the-evaluation-videos">Details on the evaluation videos</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#river-segments">River segments</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#track-annotation-protocol">Track annotation protocol</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation-details-for-the-tracking-module">Implementation details for the tracking module</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#covariance-matrices-for-state-and-observation-noises">Covariance matrices for state and observation noises</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#influence-of-tau-and-kappa">Influence of <span class="math notranslate nohighlight">\(\tau\)</span> and <span class="math notranslate nohighlight">\(\kappa\)</span></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-filtering">Bayesian filtering</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#computing-the-confidence-regions">Computing the confidence regions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#impact-of-the-filtering-algorithm">Impact of the filtering algorithm</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#smc-based-tracking">SMC-based tracking</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-comparison">Performance comparison</a></li>
</ul>
</li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <div class="math notranslate nohighlight">
\[\newcommand{\detectset}{\mathcal{D}}
\newcommand{\gauss}{\mathcal{N}}
\newcommand{\prob}{\mathbb{P}}
\newcommand{\esp}{\mathbb{E}}
\newcommand{\var}{\mathbb{V}}
\newcommand{\eqsp}{\,}
\newcommand{\Rset}{\mathbb{R}}
\newcommand{\filtdist}{\mathbb{Q}}
\newcommand{\transdist}{\mathbb{M}}
\newcommand{\SMCfiltdist}{\widehat{\filtdist}^{SMC}}
\newcommand{\rmd}{\mathrm{d}}
\newcommand{\predictdist}{\mathbb{L}}
\newcommand{\SMCpredictdist}{\widehat{\predictdist}^{SMC}}
\newcommand{\MCpredictdist}{\widehat{\predictdist}}
\newcommand{\likel}{\mathbb{G}}
\newcommand{\hatN}{\mathsf{\hat{N}}}
\newcommand{\N}{\mathsf{N}}
\newcommand{\Nmis}{\mathsf{\hat{N}_{mis}}}
\newcommand{\Nred}{\mathsf{\hat{N}_{red}}}
\newcommand{\Nfalse}{\mathsf{\hat{N}_{false}}}
\newcommand{\Ntrue}{\mathsf{\hat{N}_{true}}}
\newcommand{\gtlabels}{i \in[\![1, \N]\!]}
\newcommand{\predlabels}{j \in [\![1, \hatN]\!]}
\newcommand{\detre}{\mathsf{DetRe}}
\newcommand{\detpr}{\mathsf{DetPr}}
\newcommand{\assre}{\mathsf{AssRe}}
\newcommand{\asspr}{\mathsf{AssPr}}
\newcommand{\tp}{\mathsf{TP}}
\newcommand{\fp}{\mathsf{FP}}
\newcommand{\fn}{\mathsf{FN}}
\newcommand{\tpa}{\mathsf{TPA}}
\newcommand{\fpa}{\mathsf{FPA}}
\newcommand{\gtID}{\mathsf{gtID}}
\newcommand{\prID}{\mathsf{prID}}
\newcommand{\fna}{\mathsf{FNA}}
\newcommand{\countre}{\mathsf{CountRe}}
\newcommand{\countpr}{\mathsf{CountPr}}
\newcommand{\tempwindow}{[\![n-\lfloor \kappa/2 \rfloor, n+\lfloor \kappa/2 \rfloor]\!]}\]</div>
<section class="tex2jax_ignore mathjax_ignore" id="macrolitter-video-counting-on-riverbanks-using-state-space-models-and-moving-cameras">
<h1>Macrolitter video counting on riverbanks using state space models and moving cameras<a class="headerlink" href="#macrolitter-video-counting-on-riverbanks-using-state-space-models-and-moving-cameras" title="Permalink to this heading">#</a></h1>
<section id="abstract">
<h2>Abstract<a class="headerlink" href="#abstract" title="Permalink to this heading">#</a></h2>
<p>Litter is a known cause of degradation in marine environments and most of it travels in rivers before reaching the oceans.
In this paper, we present a novel algorithm to assist waste monitoring along watercourses.
While several attempts have been made to quantify litter using neural object detection in photographs of floating items, we tackle the more challenging task of counting directly in videos using boat-embedded cameras.
We rely on multi-object tracking (MOT) but focus on the key pitfalls of false and redundant counts which arise in typical scenarios of poor detection performance.
Our system only requires supervision at the image level and performs Bayesian filtering via a state space model based on optical flow.
We present a new open image dataset gathered through a crowdsourced campaign and used to train a center-based anchor-free object detector.
Realistic video footage assembled by water monitoring experts is annotated and provided for evaluation.
Improvements in count quality are demonstrated against systems built from state-of-the-art multi-object trackers sharing the same detection capabilities.
A precise error decomposition allows clear analysis and highlights the remaining challenges.</p>
</section>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this heading">#</a></h2>
<p>Litter pollution concerns every part of the globe.
Each year, almost ten thousand million tons of plastic waste is generated, among which 80% ends up in landfills or in nature <span id="id1">[<a class="reference internal" href="#id67" title="Roland Geyer, Jenna Jambeck, and Kara Law. Production, use, and fate of all plastics ever made. Science Advances, 3:e1700782, 07 2017. doi:10.1126/sciadv.1700782.">1</a>]</span>, notably threatening all of the world’s oceans, seas and aquatic environments <span id="id2">[<a class="reference internal" href="#id65" title="Natalie Welden. The environmental impacts of plastic pollution. pages 195-222, 01 2020. doi:10.1016/B978-0-12-817880-5.00008-6.">2</a>, <a class="reference internal" href="#id66" title="Thushari Gamage and J.D.M. Senevirathna. Plastic pollution in the marine environment. Heliyon, 6:e04709, 08 2020. doi:10.1016/j.heliyon.2020.e04709.">3</a>]</span>.
Plastic pollution is known to already impact more than 3763 marine species worldwide (see <a class="reference external" href="https://litterbase.awi.de/">this</a> detailed analysis) with risk of proliferation through the whole food chain.
This accumulation of waste is the endpoint of the largely misunderstood path of trash, mainly coming from land-based sources <span id="id3">[<a class="reference internal" href="#id62" title="Chelsea Rochman, Anthony Andrady, Sarah Dudas, Joan Fabres, François Galgani, Denise lead, Valeria Hidalgo-Ruz, Sunny Hong, Peter Kershaw, Laurent Lebreton, Amy Lusher, Ramani Narayan, Sabine Pahl, James Potemra, Chelsea Rochman, Sheck Sherif, Joni Seager, Won Shim, Paula Sobral, and Linda Amaral-Zettler. Sources, fate and effects of microplastics in the marine environment: part 2 of a global assessment. pages, 12 2016.">4</a>]</span>, yet rivers have been identified as a major pathway for the introduction of waste into marine environments <span id="id4">[<a class="reference internal" href="#id64" title="Jenna Jambeck, Roland Geyer, Chris Wilcox, Theodore Siegler, Miriam Perryman, Anthony Andrady, Ramani Narayan, and Kara Law. Marine pollution. plastic waste inputs from land into the ocean. Science (New York, N.Y.), 347:768-771, 02 2015. doi:10.1126/science.1260352.">5</a>]</span>.
Therefore, field data on rivers and monitoring are strongly needed to assess the impact of measures that can be taken. The analysis of such field data over time is pivotal to understand the efficiency of the actions implemented such as choosing zero-waste alternatives to plastic, designing new products to be long-lasting or reusable, introducing policies to reduce over-packing.</p>
<p>Different methods have already been tested to monitor waste in rivers: litter collection and sorting on riverbanks <span id="id5">[<a class="reference internal" href="#id85" title="Antoine Bruge, Cristina Barreau, Jérémy Carlot, Hélène Collin, Clément Moreno, and Philippe Maison. Monitoring litter inputs from the Adour river (southwest France) to the marine environment. Journal of Marine Science and Engineering, 2018. doi:10.3390/jmse6010024.">6</a>]</span>, visual counting of drifting litter from bridges <span id="id6">[<a class="reference internal" href="#id63" title="D. González-Fernández, A. Cózar, G. Hanke, J. Viejo, C. Morales-Caselles, R. Bakiu, D. Barcelo, F. Bessa, Antoine Bruge, M. Cabrera, J. Castro-Jiménez, M. Constant, R. Crosti, Yuri Galletti, A. Kideyş, N. Machitadze, Joana Pereira de Brito, M. Pogojeva, N. Ratola, J. Rigueira, E. Rojo-Nieto, O. Savenko, R. I. Schöneich-Argent, G. Siedlewicz, Giuseppe Suaria, and Myrto Tourgeli. Floating macrolitter leaked from europe into the ocean. Nature Sustainability, 4:474 - 483, 2021.">7</a>]</span>, floating booms <span id="id7">[<a class="reference internal" href="#id96" title="Johnny Gasperi, Rachid Dris, Tiffany Bonin, Vincent Rocher, and Bruno Tassin. Assessment of floating plastic debris in surface water along the seine river. Environmental Pollution, 195:163 - 166, 2014. URL: http://www.sciencedirect.com/science/article/pii/S0269749114003807, doi:https://doi.org/10.1016/j.envpol.2014.09.001.">8</a>]</span> and nets <span id="id8">[<a class="reference internal" href="#id97" title="David Morritt, Paris V. Stefanoudis, Dave Pearce, Oliver A. Crimmen, and Paul F. Clark. Plastic in the thames: a river runs through it. Marine Pollution Bulletin, 78(1):196-200, 2014. URL: https://www.sciencedirect.com/science/article/pii/S0025326X13006565, doi:https://doi.org/10.1016/j.marpolbul.2013.10.035.">9</a>]</span>.
All are helpful to understand the origin and typology of litter pollution yet hardly compatible with long term monitoring at country scales.
Monitoring tools need to be reliable, easy to set up on various types of rivers, and should give an overview of plastic pollution during peak discharge to help locate hotspots and provide trends.
Newer studies suggest that plastic debris transport could be better understood by counting litter trapped on river banks, providing a good indication of the local macrolitter pollution especially after increased river discharge <span id="id9">[<a class="reference internal" href="#id86" title="Tim van Emmerik, Romain Tramoy, Caroline van Calcar, Soline Alligant, Robin Treilles, Bruno Tassin, and Johnny Gasperi. Seine Plastic Debris Transport Tenfolded During Increased River Discharge. Frontiers in Marine Science, 6(October):1–7, 2019. doi:10.3389/fmars.2019.00642.">10</a>, <a class="reference internal" href="#id87" title="Tim van Emmerik and Anna Schwarz. Plastic debris in rivers. WIREs Water, 7(1):e1398, 2020. URL: https://onlinelibrary.wiley.com/doi/abs/10.1002/wat2.1398, arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1002/wat2.1398, doi:https://doi.org/10.1002/wat2.1398.">11</a>]</span>.
Based on these findings, we propose a new method for litter monitoring which relies on videos of river banks directly captured from moving boats.</p>
<p>In this case, object detection with deep neural networks (DNNs) may be used, but new challenges arise.
First, available data is still scarce.
When considering entire portions of river banks from many different locations, the variety of scenes, viewing angles and/or light conditions is not well covered by existing plastic litter datasets like <span id="id10">[<a class="reference internal" href="#id103" title="Pedro F Proença and Pedro Simões. TACO: Trash Annotations in Context for Litter Detection. 2020. URL: http://tacodataset.org/ http://arxiv.org/abs/2003.06975, arXiv:2003.06975.">12</a>]</span>, where litter is usually captured from relatively close distances and many times in urban or domestic backgrounds. Therefore, achieving robust object detection across multiple conditions is still delicate.</p>
<p>Second, counting from videos is a different task than counting from independent images, because individual objects will typically appear in several consecutive frames, yet they must only be counted once. This last problem of association has been extensively studied for the multi-object tracking (MOT) task, which aims at recovering individual trajectories for objects in videos. When successful MOT is achieved, counting objects in videos is equivalent to counting the number of estimated trajectories. Deep learning has been increasingly used to improve MOT solutions <span id="id11">[<a class="reference internal" href="#id81" title="Gioele Ciaparrone, Francisco Luque Sánchez, Siham Tabik, Luigi Troiano, Roberto Tagliaferri, and Francisco Herrera. Deep learning in video multi-object tracking: A survey. Neurocomputing, 381:61–88, 2020. arXiv:1907.12740, doi:10.1016/j.neucom.2019.11.023.">13</a>]</span>. However, newer state-of-the-art techniques require increasingly heavy and costly supervision, typically all object positions provided at every frame. In addition, many successful techniques <span id="id12">[<a class="reference internal" href="#id104" title="P. Bergmann, T. Meinhardt, and L. Leal-Taixe. Tracking without bells and whistles. In 2019 IEEE/CVF International Conference on Computer Vision (ICCV), volume, 941-951. Los Alamitos, CA, USA, nov 2019. IEEE Computer Society. URL: https://doi.ieeecomputersociety.org/10.1109/ICCV.2019.00103, doi:10.1109/ICCV.2019.00103.">14</a>]</span> can hardly be used in scenarios with abrupt and nonlinear camera motion. Finally, while research is still active to rigorously evaluate performance at multi-object <em>tracking</em> <span id="id13">[<a class="reference internal" href="#id105" title="Jonathon Luiten, Aljosa Osep, Patrick Dendorfer, Philip Torr, Andreas Geiger, Laura Leal-Taixé, and Bastian Leibe. Hota: a higher order metric for evaluating multi-object tracking. International journal of computer vision, 129(2):548–578, 2021.">15</a>]</span>, most but not all aspects of the latter may affect global video counts, which calls for a separate evaluation protocol dedicated to multi-object <em>counting</em>.</p>
<p>Our contribution can be summarized as follows.</p>
<ol class="arabic simple">
<li><p>We provide a novel open-source image dataset of macro litter, which includes various objects seen from different rivers and different contexts.
This dataset was produced with a new open-sourced platform for data gathering and annotation developed in conjunction with Surfrider Foundation Europe, continuously growing with more data.</p></li>
<li><p>We propose a new algorithm specifically tailored to count in videos with fast camera movements.
In a nutshell, DNN-based object detection is paired with a robust state space movement model which uses optical flow to perform Bayesian filtering, while confidence regions built on posterior predictive distributions are used for data association.
This framework does not require video annotations at training time: the multi-object tracking module does not require supervision, only the DNN-based object detection does require annotated images.
It also fully leverages optical flow estimates and the uncertainty provided by Bayesian predictions to recover object identities even when detection recall is low.
Contrary to existing MOT solutions, this method ensures that tracks are stable enough to avoid repeated counting of the same object.</p></li>
<li><p>We provide a set of video sequences where litter counts are known and depicted in real conditions.
For these videos only, litter positions are manually annotated at every frame in order to carefully analyze performance.
This allows us to build new informative count metrics.
We compare the count performance of our method against other MOT-based alternatives.</p></li>
</ol>
<p>A first visual illustration of the second claim is presented via the following code chunks: on three selected frames, we present a typical scenario where our strategy can avoid overcounting the same object (we depict internal workings of our solution against the end result of the competitors).</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">myst_nb</span> <span class="kn">import</span> <span class="n">glue</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">surfnet.prepare_data</span> <span class="kn">import</span> <span class="n">download_data</span>
<span class="kn">from</span> <span class="nn">surfnet.track</span> <span class="kn">import</span> <span class="n">default_args</span> <span class="k">as</span> <span class="n">args</span>


<span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;legend.fontsize&#39;</span><span class="p">:</span> <span class="s1">&#39;xx-large&#39;</span><span class="p">,</span>
         <span class="s1">&#39;axes.labelsize&#39;</span><span class="p">:</span> <span class="s1">&#39;xx-large&#39;</span><span class="p">,</span>
         <span class="s1">&#39;axes.titlesize&#39;</span><span class="p">:</span><span class="s1">&#39;xx-large&#39;</span><span class="p">,</span>
         <span class="s1">&#39;xtick.labelsize&#39;</span><span class="p">:</span><span class="s1">&#39;xx-large&#39;</span><span class="p">,</span>
         <span class="s1">&#39;ytick.labelsize&#39;</span><span class="p">:</span><span class="s1">&#39;xx-large&#39;</span><span class="p">}</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>

<span class="c1"># download frames and detections from a given deep detector model</span>
<span class="n">download_data</span><span class="p">()</span>

<span class="c1"># prepare arguments</span>
<span class="n">args</span><span class="o">.</span><span class="n">external_detections</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">args</span><span class="o">.</span><span class="n">data_dir</span> <span class="o">=</span> <span class="s1">&#39;data/external_detections/part_1_segment_0&#39;</span>
<span class="n">args</span><span class="o">.</span><span class="n">output_dir</span> <span class="o">=</span> <span class="s1">&#39;surfnet/results&#39;</span>
<span class="n">args</span><span class="o">.</span><span class="n">noise_covariances_path</span> <span class="o">=</span> <span class="s1">&#39;surfnet/data/tracking_parameters&#39;</span>
<span class="n">args</span><span class="o">.</span><span class="n">confidence_threshold</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">args</span><span class="o">.</span><span class="n">algorithm</span> <span class="o">=</span> <span class="s1">&#39;EKF&#39;</span>
<span class="n">args</span><span class="o">.</span><span class="n">ratio</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">args</span><span class="o">.</span><span class="n">display</span> <span class="o">=</span> <span class="mi">0</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>---Downloading: saved_detections.pickle
---Downloading: saved_frames.pickle
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pickle</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">surfnet.tracking.utils</span> <span class="kn">import</span> <span class="n">resize_external_detections</span><span class="p">,</span> <span class="n">write_tracking_results_to_file</span>
<span class="kn">from</span> <span class="nn">surfnet.tools.video_readers</span> <span class="kn">import</span> <span class="n">FramesWithInfo</span>
<span class="kn">from</span> <span class="nn">surfnet.tracking.trackers</span> <span class="kn">import</span> <span class="n">get_tracker</span>
<span class="kn">from</span> <span class="nn">surfnet.track</span> <span class="kn">import</span> <span class="n">track_video</span>

<span class="c1"># Initialize variances</span>
<span class="n">transition_variance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">noise_covariances_path</span><span class="p">,</span> <span class="s1">&#39;transition_variance.npy&#39;</span><span class="p">))</span>
<span class="n">observation_variance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">noise_covariances_path</span><span class="p">,</span> <span class="s1">&#39;observation_variance.npy&#39;</span><span class="p">))</span>

<span class="c1"># Get tracker algorithm</span>
<span class="n">engine</span> <span class="o">=</span> <span class="n">get_tracker</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">algorithm</span><span class="p">)</span>

<span class="c1"># Open data: detections and frames</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">data_dir</span><span class="p">,</span> <span class="s1">&#39;saved_detections.pickle&#39;</span><span class="p">),</span><span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">detections</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">data_dir</span><span class="p">,</span> <span class="s1">&#39;saved_frames.pickle&#39;</span><span class="p">),</span><span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">frames</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

<span class="c1"># Create frame reader and resize detections</span>
<span class="n">reader</span> <span class="o">=</span> <span class="n">FramesWithInfo</span><span class="p">(</span><span class="n">frames</span><span class="p">)</span>
<span class="n">detections</span> <span class="o">=</span> <span class="n">resize_external_detections</span><span class="p">(</span><span class="n">detections</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">ratio</span><span class="p">)</span>

<span class="c1"># Start tracking, storing intermediate tracklets</span>
<span class="n">results</span><span class="p">,</span> <span class="n">frame_to_trackers</span> <span class="o">=</span> <span class="n">track_video</span><span class="p">(</span><span class="n">reader</span><span class="p">,</span> <span class="n">detections</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">engine</span><span class="p">,</span> 
                                         <span class="n">transition_variance</span><span class="p">,</span> <span class="n">observation_variance</span><span class="p">,</span> <span class="n">return_trackers</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Write final results</span>
<span class="n">write_tracking_results_to_file</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">ratio_x</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">ratio</span><span class="p">,</span> <span class="n">ratio_y</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">ratio</span><span class="p">,</span> <span class="n">output_filename</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">output_dir</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>EKF will be used for tracking.
Tracking...
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Tracking done.
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input tag_remove-output docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">surfnet.track</span> <span class="kn">import</span> <span class="n">build_image_trackers</span>
<span class="c1"># Choose a few indices to display (same for our algorithm and SORT)</span>
<span class="n">idxs</span> <span class="o">=</span> <span class="p">[</span><span class="mi">108</span><span class="p">,</span> <span class="mi">112</span><span class="p">,</span> <span class="mi">117</span><span class="p">]</span>
    
<span class="n">considered_frames</span> <span class="o">=</span> <span class="p">[</span><span class="n">frames</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">idxs</span><span class="p">]</span>
<span class="n">considered_trackers</span> <span class="o">=</span> <span class="p">[</span><span class="n">frame_to_trackers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">idxs</span><span class="p">]</span>
<span class="n">glue</span><span class="p">(</span><span class="s1">&#39;demo_ours&#39;</span><span class="p">,</span> <span class="n">build_image_trackers</span><span class="p">(</span><span class="n">considered_frames</span><span class="p">,</span> <span class="n">considered_trackers</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">reader</span><span class="p">),</span> <span class="n">display</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
</div>
<figure class="align-default" id="demo-ours">
<img alt="_images/459e1b3400f7a2e7687066ae3dc64b1f0cb27d44143c7e10986fb78b74397a89.png" src="_images/459e1b3400f7a2e7687066ae3dc64b1f0cb27d44143c7e10986fb78b74397a89.png" />
<figcaption>
<p><span class="caption-number">Fig. 1 </span><span class="caption-text"><em>Our method</em>: one object (red dot) is correctly detected at every frame and given a consistent identity throughout the sequence with low location uncertainty (red ellipse). Next to it, a false positive detection is generated at the first frame (brown dot) but immediatly lost in the following frames: the associated uncertainty grows fast (brown ellipse). In our solution, this type of track will not be counted. A third correctly detected object (pink) appears in the third frame and begins a new track.</span><a class="headerlink" href="#demo-ours" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Tracker with SORT </span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
<span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">from</span> <span class="nn">sort.sort</span> <span class="kn">import</span> <span class="n">track</span> <span class="k">as</span> <span class="n">sort_tracker</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Tracking with SORT...&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;--- Begin SORT internal logs&#39;</span><span class="p">)</span>
<span class="n">sort_tracker</span><span class="p">(</span><span class="n">detections_dir</span><span class="o">=</span><span class="s1">&#39;data/external_detections&#39;</span><span class="p">,</span> <span class="n">output_dir</span><span class="o">=</span><span class="s1">&#39;sort/results&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;--- End&#39;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">read_sort_output</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Reads the output .txt of Sort (or other tracking algorithm)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">dict_frames</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">items</span> <span class="o">=</span> <span class="n">line</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;,&quot;</span><span class="p">)</span>
            <span class="n">frame</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">items</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="n">objnum</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">items</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">x</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">items</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
            <span class="n">y</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">items</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
            <span class="n">dict_frames</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">items</span><span class="p">[</span><span class="mi">0</span><span class="p">])]</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">objnum</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
    <span class="k">return</span>  <span class="n">dict_frames</span>


<span class="k">def</span> <span class="nf">build_image</span><span class="p">(</span><span class="n">frames</span><span class="p">,</span> <span class="n">trackers</span><span class="p">,</span> <span class="n">image_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">135</span><span class="p">,</span><span class="mi">240</span><span class="p">),</span> <span class="n">downsampling</span><span class="o">=</span><span class="mi">2</span><span class="o">*</span><span class="mi">4</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot; Builds a full image with consecutive frames and their displayed trackers</span>
<span class="sd">    frames: a list of K np.array</span>
<span class="sd">    trackers: a list of K trackers. Each tracker is a per frame list of tracked objects</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">K</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">frames</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">trackers</span><span class="p">)</span> <span class="o">==</span> <span class="n">K</span>
    <span class="n">font</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">FONT_HERSHEY_COMPLEX</span>
    <span class="n">output_img</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">image_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">image_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">K</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
    <span class="n">object_ids</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">tracker</span> <span class="ow">in</span> <span class="n">trackers</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">detection</span> <span class="ow">in</span> <span class="n">tracker</span><span class="p">:</span>
            <span class="n">object_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">detection</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">min_object_id</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">object_ids</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
        <span class="n">frame</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">cv2</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="n">frames</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">image_shape</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2RGB</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">detection</span> <span class="ow">in</span> <span class="n">trackers</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
            <span class="n">cv2</span><span class="o">.</span><span class="n">putText</span><span class="p">(</span><span class="n">frame</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">detection</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="n">min_object_id</span><span class="w"> </span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">detection</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="n">downsampling</span><span class="p">)</span><span class="o">+</span><span class="mi">10</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">detection</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">/</span><span class="n">downsampling</span><span class="p">)</span><span class="o">+</span><span class="mi">10</span><span class="p">),</span> <span class="n">font</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="p">(</span><span class="mi">255</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">LINE_AA</span><span class="p">)</span>

        <span class="n">output_img</span><span class="p">[:,</span><span class="n">i</span><span class="o">*</span><span class="n">image_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">image_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],:]</span> <span class="o">=</span> <span class="n">frame</span>
    <span class="k">return</span> <span class="n">output_img</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Tracking with SORT...
--- Begin SORT internal logs
Total Tracking took: 0.100 seconds for 360 frames or 3611.1 FPS
--- End
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input tag_remove-output docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># open sort output</span>
<span class="n">tracker_file</span> <span class="o">=</span> <span class="s2">&quot;sort/results/part_1_segment_0.txt&quot;</span>
<span class="n">frame_to_track</span> <span class="o">=</span> <span class="n">read_sort_output</span><span class="p">(</span><span class="n">tracker_file</span><span class="p">)</span>

<span class="n">condisered_frames</span> <span class="o">=</span> <span class="p">[</span><span class="n">frames</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">idxs</span><span class="p">]</span>
<span class="n">considered_tracks</span> <span class="o">=</span> <span class="p">[</span><span class="n">frame_to_track</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">idxs</span><span class="p">]</span>
<span class="n">out_img</span> <span class="o">=</span> <span class="n">build_image</span><span class="p">(</span><span class="n">condisered_frames</span><span class="p">,</span> <span class="n">considered_tracks</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">out_img</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">);</span>
<span class="n">glue</span><span class="p">(</span><span class="s1">&#39;sort_demo&#39;</span><span class="p">,</span> <span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">(),</span> <span class="n">display</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
</div>
<figure class="align-default" id="demo-sort">
<img alt="_images/d29fcee6500456cdb357032a67754c62f346698fdb5273980f1ec0fda1935b43.png" src="_images/d29fcee6500456cdb357032a67754c62f346698fdb5273980f1ec0fda1935b43.png" />
<figcaption>
<p><span class="caption-number">Fig. 2 </span><span class="caption-text"><em>SORT</em>: the resulting count is also 2, but both counts arise from tracks generated by the same object, the latter not re-associated at all in the second frame. Additionally, the third object is discarded (in post-processing) by their strategy.</span><a class="headerlink" href="#demo-sort" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="related-works">
<h2>Related works<a class="headerlink" href="#related-works" title="Permalink to this heading">#</a></h2>
<section id="ai-automated-counting">
<h3>AI-automated counting<a class="headerlink" href="#ai-automated-counting" title="Permalink to this heading">#</a></h3>
<p>Counting from images has been an ongoing challenge in computer vision.
Most works can be divided into (i) detection-based methods where objects are individually located for counting, (ii) density-based methods where counts are obtained by summing a predicted density map, and (iii) regression-based methods where counts are directly regressed from input images <span id="id14">[<a class="reference internal" href="#id93" title="Prithvijit Chattopadhyay, Ramakrishna Vedantam, Ramprasaath R Selvaraju, Dhruv Batra, and Devi Parikh. Counting everyday objects in everyday scenes. In Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, volume 2017-Janua, 4428–4437. 2017. arXiv:1604.03505, doi:10.1109/CVPR.2017.471.">16</a>]</span>.
While some of these works tackled the problem of counting in wild scenes <span id="id15">[<a class="reference internal" href="#id100" title="Carlos Arteta, Victor Lempitsky, and Andrew Zisserman. Counting in the wild. 9911:483-498, 10 2016. doi:10.1007/978-3-319-46478-7_30.">17</a>]</span>, most are focused on pedestrian and crowd counting.
Though several works <span id="id16">[<a class="reference internal" href="#id101" title="Xingjiao Wu, Baohan Xu, Yingbin Zheng, Hao Ye, Jing Yang, and Liang He. Fast video crowd counting with a temporal aware network. Neurocomputing, 403:13-20, 2020. URL: https://www.sciencedirect.com/science/article/pii/S0925231220306561, doi:https://doi.org/10.1016/j.neucom.2020.04.071.">18</a>, <a class="reference internal" href="#id108" title="Feng Xiong, Xingjian Shi, and Dit-Yan Yeung. Spatiotemporal modeling for crowd counting in videos. In 2017 IEEE International Conference on Computer Vision (ICCV), volume, 5161-5169. 2017. doi:10.1109/ICCV.2017.551.">19</a>, <a class="reference internal" href="#id109" title="Yunqi Miao, Jungong Han, Yongsheng Gao, and Baochang Zhang. ST-CNN: Spatial-Temporal Convolutional Neural Network for crowd counting in videos. Pattern Recognition Letters, 125:113–118, jul 2019. doi:10.1016/j.patrec.2019.04.012.">20</a>]</span> showed the relevance of leveraging sequential inter-frame information to achieve better counts at every frame, none of these methods actually attempt to produce global video counts.</p>
</section>
<section id="computer-vision-for-macro-litter-monitoring">
<h3>Computer vision for macro litter monitoring<a class="headerlink" href="#computer-vision-for-macro-litter-monitoring" title="Permalink to this heading">#</a></h3>
<p>Automatic macro litter monitoring in rivers is still a relatively nascent initiative, yet there have already been several attempts at using DNN-based object recognition tools to count plastic trash.
Recently, <span id="id17">[<a class="reference internal" href="#id103" title="Pedro F Proença and Pedro Simões. TACO: Trash Annotations in Context for Litter Detection. 2020. URL: http://tacodataset.org/ http://arxiv.org/abs/2003.06975, arXiv:2003.06975.">12</a>]</span> used a combination of two Convolutional Neural Networks (CNNs) to detect and quantify plastic litter using geospatial images from Cambodia.
In <span id="id18">[<a class="reference internal" href="#id92" title="Mattis Wolf, Katelijn van den Berg, Shungudzemwoyo Pascal Garaba, Nina Gnann, Klaus Sattler, Frederic Theodor Stahl, and Oliver Zielinski. Machine learning for aquatic plastic litter detection, classification and quantification (APLASTIC–Q). Environmental Research Letters, 2020. doi:10.1088/1748-9326/abbd01.">21</a>]</span>, reliable estimates of plastic density were obtained using Faster R-CNN <span id="id19">[<a class="reference internal" href="#id102" title="Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: towards real-time object detection with region proposal networks. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS'15, 91–99. Cambridge, MA, USA, 2015. MIT Press.">22</a>]</span> on images extracted from bridge-mounted cameras.
For underwater waste monitoring, <span id="id20">[<a class="reference internal" href="#id98" title="Colin van Lieshout, Kees van Oeveren, Tim van Emmerik, and Eric Postma. Automated River Plastic Monitoring Using Deep Learning and Cameras. Earth and Space Science, 7(8):e2019EA000960, 2020. doi:10.1029/2019EA000960.">23</a>]</span> assembled a dataset with bounding box annotations, and showed promising performance with several object detectors.
They later turned to generative models to obtain more synthetic data from a small dataset <span id="id21">[<a class="reference internal" href="#id107" title="Jungseok Hong, Michael Fulton, and Junaed Sattar. A Generative Approach Towards Improved Robotic Detection of Marine Litter. In Proceedings - IEEE International Conference on Robotics and Automation, 10525–10531. 2020. arXiv:1910.04754, doi:10.1109/ICRA40945.2020.9197575.">24</a>]</span>.
While proving the practicality of deep learning for automatic waste detection in various contexts, these works only provide counts for separate images of photographed litter.
To the best of our knowledge, no solution has been proposed to count litter directly in videos.</p>
</section>
<section id="multi-object-tracking">
<h3>Multi-object tracking<a class="headerlink" href="#multi-object-tracking" title="Permalink to this heading">#</a></h3>
<p>Multi-object tracking usually involves object detection, data association and track management, with a very large number of methods already existing before DNNs <span id="id22">[<a class="reference internal" href="#id110" title="Wenhan Luo, Junliang Xing, Anton Milan, Xiaoqin Zhang, Wei Liu, and Tae-Kyun Kim. Multiple object tracking: a literature review. Artificial Intelligence, 293:103448, 2021. URL: https://www.sciencedirect.com/science/article/pii/S0004370220301958, doi:https://doi.org/10.1016/j.artint.2020.103448.">25</a>]</span>.
MOT approaches now mostly differ in the level of supervision they require for each step: until recently, most successful methods (like <span id="id23">[<a class="reference internal" href="#id111" title="Alex Bewley, Zongyuan Ge, Lionel Ott, Fabio Ramos, and Ben Upcroft. Simple online and realtime tracking. In Proceedings - International Conference on Image Processing, ICIP, volume 2016-Augus, 3464–3468. 2016. URL: https://github.com/abewley/sort, arXiv:1602.00763, doi:10.1109/ICIP.2016.7533003.">26</a>]</span>) have been detection-based, i.e.
involving only a DNN-based object detector trained at the image level and coupled with an unsupervised data association step.
In specific fields such as pedestrian tracking or autonomous driving, vast datasets now provide precise object localisation and identities throughout entire videos <span id="id24">[<a class="reference internal" href="#id112" title="Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. Nuscenes: A multimodal dataset for autonomous driving. In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 11618–11628. 2020. arXiv:1903.11027, doi:10.1109/CVPR42600.2020.01164.">27</a>, <a class="reference internal" href="#id113" title="Patrick Dendorfer, Hamid Rezatofighi, Anton Milan, Javen Shi, Daniel Cremers, Ian Reid, Stefan Roth, Konrad Schindler, and Laura Leal-Taixé. MOT20: A benchmark for multi object tracking in crowded scenes. 2020. arXiv:2003.09003.">28</a>]</span>.
Current state-of-the-art methods leverage this supervision via deep visual feature extraction <span id="id25">[<a class="reference internal" href="#id114" title="Nicolai Wojke, Alex Bewley, and Dietrich Paulus. Simple online and realtime tracking with a deep association metric. In Proceedings - International Conference on Image Processing, ICIP, volume 2017-Septe, 3645–3649. 2018. arXiv:1703.07402, doi:10.1109/ICIP.2017.8296962.">29</a>, <a class="reference internal" href="#id82" title="Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng, and Wenyu Liu. Fairmot: on the fairness of detection and re-identification in multiple object tracking. International Journal of Computer Vision, pages 1–19, 2021.">30</a>]</span> or even self-attention <span id="id26">[<a class="reference internal" href="#id115" title="Peng Chu, Jiang Wang, Quanzeng You, Haibin Ling, and Zicheng Liu. TransMOT: Spatial-Temporal Graph Transformer for Multiple Object Tracking. 2021. URL: http://arxiv.org/abs/2104.00194, arXiv:2104.00194.">31</a>]</span> and graph neural networks <span id="id27">[<a class="reference internal" href="#id116" title="Yongxin Wang, Kris Kitani, and Xinshuo Weng. Joint object detection and multi-object tracking with graph neural networks. May 2021.">32</a>]</span>.
For these applications, motion prediction may be required, yet well-trained appearance models are usually enough to deal with detection failures under simple motion, therefore the linear constant-velocity assumption often prevails (<span id="id28">[<a class="reference internal" href="#id81" title="Gioele Ciaparrone, Francisco Luque Sánchez, Siham Tabik, Luigi Troiano, Roberto Tagliaferri, and Francisco Herrera. Deep learning in video multi-object tracking: A survey. Neurocomputing, 381:61–88, 2020. arXiv:1907.12740, doi:10.1016/j.neucom.2019.11.023.">13</a>]</span>).</p>
<p>In the case of macrolitter monitoring, however, available image datasets are still orders of magnitude smaller, and annotated video datasets do not exist at all.
Even more so, real shooting conditions induce chaotic movements on the boat-embedded cameras.
A close work of ours is that of <span id="id29">[<a class="reference internal" href="#id106" title="Michael Fulton, Jungseok Hong, Md Jahidul Islam, and Junaed Sattar. Robotic detection of marine litter using deep visual detection models. In 2019 International Conference on Robotics and Automation (ICRA), 5752–5758. IEEE, 2019.">33</a>]</span>, who paired Kalman filtering with optical flow to yield fruit count estimates on entire video sequences captured by moving robots.
However, their video footage is captured at night with consistent lighting conditions, backgrounds are largely similar across sequences, and camera movements are less challenging.
In our application context, we find that using MOT for the task of counting objects requires a new movement model, to take into account missing detections and large camera movements.</p>
</section>
</section>
<section id="datasets-for-training-and-evaluation">
<h2>Datasets for training and evaluation<a class="headerlink" href="#datasets-for-training-and-evaluation" title="Permalink to this heading">#</a></h2>
<p>Our main dataset of annotated images is used to train the object detector.
Then, only for evaluation purposes, we provide videos with annotated object positions and known global counts.
Our motivation is to avoid relying on training data that requires this resource-consuming process.</p>
<section id="images">
<h3>Images<a class="headerlink" href="#images" title="Permalink to this heading">#</a></h3>
<section id="data-collection">
<h4>Data collection<a class="headerlink" href="#data-collection" title="Permalink to this heading">#</a></h4>
<p>With help from volunteers, we compile photographs of litter stranded on river banks after increased river discharge, shot directly from kayaks navigating at varying distances from the shore.
Images span multiple rivers with various levels of water current, on different seasons, mostly in southwestern France.
The resulting pictures depict trash items under the same conditions as the video footage we wish to count on, while spanning a wide variety of backgrounds, light conditions, viewing angles and picture quality.</p>
</section>
<section id="bounding-box-annotation">
<h4>Bounding box annotation<a class="headerlink" href="#bounding-box-annotation" title="Permalink to this heading">#</a></h4>
<p>For object detection applications, the images are annotated using a custom online platform where each object is located using a bounding box.
In this work, we focus only on litter counting without classification, however the annotated objects are already classified into specific categories which are described in <a class="reference internal" href="#trash-categories-image"><span class="std std-ref">Trash categories defined to facilitate porting to a counting system that allows trash identification</span></a>.</p>
<p>A few samples are depicted below:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">ExifTags</span>
<span class="kn">from</span> <span class="nn">pycocotools.coco</span> <span class="kn">import</span> <span class="n">COCO</span>


<span class="k">def</span> <span class="nf">draw_bbox</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">anns</span><span class="p">,</span> <span class="n">ratio</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Display the specified annotations.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">ann</span> <span class="ow">in</span> <span class="n">anns</span><span class="p">:</span>
        <span class="p">[</span><span class="n">bbox_x</span><span class="p">,</span> <span class="n">bbox_y</span><span class="p">,</span> <span class="n">bbox_w</span><span class="p">,</span> <span class="n">bbox_h</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">ratio</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">ann</span><span class="p">[</span><span class="s1">&#39;bbox&#39;</span><span class="p">]))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="n">cv2</span><span class="o">.</span><span class="n">rectangle</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="p">(</span><span class="n">bbox_x</span><span class="p">,</span><span class="n">bbox_y</span><span class="p">),(</span><span class="n">bbox_x</span><span class="o">+</span><span class="n">bbox_w</span><span class="p">,</span><span class="n">bbox_y</span><span class="o">+</span><span class="n">bbox_h</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">),</span><span class="n">thickness</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">image</span>

<span class="nb">dir</span> <span class="o">=</span> <span class="s1">&#39;surfnet/data/images&#39;</span>

<span class="n">ann_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">dir</span><span class="p">,</span><span class="s1">&#39;annotations&#39;</span><span class="p">)</span>
<span class="n">data_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">dir</span><span class="p">,</span><span class="s1">&#39;images&#39;</span><span class="p">)</span>
<span class="n">ann_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">ann_dir</span><span class="p">,</span> <span class="s1">&#39;subset_of_annotations.json&#39;</span><span class="p">)</span>
<span class="n">coco</span> <span class="o">=</span> <span class="n">COCO</span><span class="p">(</span><span class="n">ann_file</span><span class="p">)</span>

<span class="n">imgIds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">coco</span><span class="o">.</span><span class="n">getImgIds</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1"> images loaded&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">imgIds</span><span class="p">)))</span>

<span class="k">for</span> <span class="n">imgId</span> <span class="ow">in</span> <span class="n">imgIds</span><span class="p">:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">coco</span><span class="o">.</span><span class="n">loadImgs</span><span class="p">(</span><span class="n">ids</span><span class="o">=</span><span class="p">[</span><span class="n">imgId</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span><span class="n">image</span><span class="p">[</span><span class="s1">&#39;file_name&#39;</span><span class="p">]))</span>
        <span class="c1"># Rotation of the picture in the Exif tags</span>
        <span class="k">for</span> <span class="n">orientation</span> <span class="ow">in</span> <span class="n">ExifTags</span><span class="o">.</span><span class="n">TAGS</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">ExifTags</span><span class="o">.</span><span class="n">TAGS</span><span class="p">[</span><span class="n">orientation</span><span class="p">]</span><span class="o">==</span><span class="s1">&#39;Orientation&#39;</span><span class="p">:</span>
                <span class="k">break</span>
        
        <span class="n">exif</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">_getexif</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">exif</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">exif</span><span class="p">[</span><span class="n">orientation</span><span class="p">]</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
                <span class="n">image</span><span class="o">=</span><span class="n">image</span><span class="o">.</span><span class="n">rotate</span><span class="p">(</span><span class="mi">180</span><span class="p">,</span> <span class="n">expand</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">exif</span><span class="p">[</span><span class="n">orientation</span><span class="p">]</span> <span class="o">==</span> <span class="mi">6</span><span class="p">:</span>
                <span class="n">image</span><span class="o">=</span><span class="n">image</span><span class="o">.</span><span class="n">rotate</span><span class="p">(</span><span class="mi">270</span><span class="p">,</span> <span class="n">expand</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">exif</span><span class="p">[</span><span class="n">orientation</span><span class="p">]</span> <span class="o">==</span> <span class="mi">8</span><span class="p">:</span>
                <span class="n">image</span><span class="o">=</span><span class="n">image</span><span class="o">.</span><span class="n">rotate</span><span class="p">(</span><span class="mi">90</span><span class="p">,</span> <span class="n">expand</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">except</span> <span class="p">(</span><span class="ne">AttributeError</span><span class="p">,</span> <span class="ne">KeyError</span><span class="p">,</span> <span class="ne">IndexError</span><span class="p">):</span>
        <span class="c1"># cases: image don&#39;t have getexif</span>
        <span class="k">pass</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">image</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="s1">&#39;RGB&#39;</span><span class="p">)),</span>  <span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_RGB2BGR</span><span class="p">)</span>
    <span class="n">annIds</span> <span class="o">=</span> <span class="n">coco</span><span class="o">.</span><span class="n">getAnnIds</span><span class="p">(</span><span class="n">imgIds</span><span class="o">=</span><span class="p">[</span><span class="n">imgId</span><span class="p">])</span>
    <span class="n">anns</span> <span class="o">=</span> <span class="n">coco</span><span class="o">.</span><span class="n">loadAnns</span><span class="p">(</span><span class="n">ids</span><span class="o">=</span><span class="n">annIds</span><span class="p">)</span>
    <span class="n">h</span><span class="p">,</span><span class="n">w</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">target_h</span> <span class="o">=</span> <span class="mi">1080</span>
    <span class="n">ratio</span> <span class="o">=</span> <span class="n">target_h</span><span class="o">/</span><span class="n">h</span>
    <span class="n">target_w</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">ratio</span><span class="o">*</span><span class="n">w</span><span class="p">)</span> 
    <span class="n">image</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="n">image</span><span class="p">,(</span><span class="n">target_w</span><span class="p">,</span><span class="n">target_h</span><span class="p">))</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">draw_bbox</span><span class="p">(</span><span class="n">image</span><span class="p">,</span><span class="n">anns</span><span class="p">,</span><span class="n">ratio</span><span class="p">)</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">image</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2RGB</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>loading annotations into memory...
Done (t=0.00s)
creating index...
index created!
15 images loaded
</pre></div>
</div>
<img alt="_images/800e717c7a690746970a836372211975660701f3e6f4fb74aa1da58e3769d8a6.png" src="_images/800e717c7a690746970a836372211975660701f3e6f4fb74aa1da58e3769d8a6.png" />
<img alt="_images/d0e9ddee8bf7fc9c4378f923107b4794b3ac196d325985d560d4d164c780cf8c.png" src="_images/d0e9ddee8bf7fc9c4378f923107b4794b3ac196d325985d560d4d164c780cf8c.png" />
<img alt="_images/26573346858d1c216ad70b712a8a65a25b37cab32cedae6b6c586a4db69210ea.png" src="_images/26573346858d1c216ad70b712a8a65a25b37cab32cedae6b6c586a4db69210ea.png" />
<img alt="_images/2de04ee1ad58b37cb6069659355e3d4b37b6768df83dd5f66e7d5a88b71bb742.png" src="_images/2de04ee1ad58b37cb6069659355e3d4b37b6768df83dd5f66e7d5a88b71bb742.png" />
<img alt="_images/0e9b26bb136bd9947700903c1e461c8f52814500a9dd0bb0f3d74faa08b7ba8a.png" src="_images/0e9b26bb136bd9947700903c1e461c8f52814500a9dd0bb0f3d74faa08b7ba8a.png" />
<img alt="_images/05682f8c3f64afa73dc46ca7bc336188bacd4f87ef45dd08bacac782184209de.png" src="_images/05682f8c3f64afa73dc46ca7bc336188bacd4f87ef45dd08bacac782184209de.png" />
<img alt="_images/8005960174707480171c4d1a9fae08b01c64b58d286f271a6e19594aaff96a75.png" src="_images/8005960174707480171c4d1a9fae08b01c64b58d286f271a6e19594aaff96a75.png" />
<img alt="_images/3d0892b76e17eea4698fe51a4b8e36fb064919196d8e8d6fed7331a5523bfb07.png" src="_images/3d0892b76e17eea4698fe51a4b8e36fb064919196d8e8d6fed7331a5523bfb07.png" />
<img alt="_images/20948a7c132a25a8867a87fb34d8b22a810668e57993b3118cb7fc6600b9788d.png" src="_images/20948a7c132a25a8867a87fb34d8b22a810668e57993b3118cb7fc6600b9788d.png" />
<img alt="_images/323502e3232480f320075332d09ac97817a4b384e1b77ef22d22896a8bd344bb.png" src="_images/323502e3232480f320075332d09ac97817a4b384e1b77ef22d22896a8bd344bb.png" />
<img alt="_images/09007f5a3d22ece3b3f2cb793010949d6f4dc1b700d772a2eca15ed63505d693.png" src="_images/09007f5a3d22ece3b3f2cb793010949d6f4dc1b700d772a2eca15ed63505d693.png" />
<img alt="_images/a874b5fd87f38ca7733669eac3a413da55e9815c4ca166d9752703e64a92c063.png" src="_images/a874b5fd87f38ca7733669eac3a413da55e9815c4ca166d9752703e64a92c063.png" />
<img alt="_images/ecf76094eb7ff144e07f37d795b454551fccb81f49f3be1512ed268e23a6c460.png" src="_images/ecf76094eb7ff144e07f37d795b454551fccb81f49f3be1512ed268e23a6c460.png" />
<img alt="_images/b5299d62a29c03140a41c62c97a7b52bd740369b04540e8a32809e89f9155003.png" src="_images/b5299d62a29c03140a41c62c97a7b52bd740369b04540e8a32809e89f9155003.png" />
<img alt="_images/0c3ca6c3b928aad830fe6dab0b5a8976d8131ccd89eafbe5dd947075ae929d88.png" src="_images/0c3ca6c3b928aad830fe6dab0b5a8976d8131ccd89eafbe5dd947075ae929d88.png" />
</div>
</div>
</section>
</section>
<section id="video-sequences">
<h3>Video sequences<a class="headerlink" href="#video-sequences" title="Permalink to this heading">#</a></h3>
<section id="id30">
<h4>Data collection<a class="headerlink" href="#id30" title="Permalink to this heading">#</a></h4>
<p>For evaluation, an on-field study was conducted with 20 volunteers to manually count litter along three different riverbank sections in April 2021, on the Gave d’Oloron near Auterrive (Pyrénées-Atlantiques, France), using kayaks.
The river sections, each 500 meters long, were precisely defined for their differences in background, vegetation, river current, light conditions and accessibility (see <a class="reference internal" href="#video-dataset-appendix"><span class="std std-ref">this section</span></a> for aerial views of the shooting site and details on the river sections).
In total, the three videos amount to 20 minutes of footage at 24 frames per second (fps) and a resolution of 1920x1080 pixels.</p>
</section>
<section id="track-annotation">
<h4>Track annotation<a class="headerlink" href="#track-annotation" title="Permalink to this heading">#</a></h4>
<p>On video footage, we manually recovered all visible object trajectories on each river section using an online video annotation tool (more details <a class="reference internal" href="#video-dataset-appendix"><span class="std std-ref">here</span></a> for the precise methodology).
From that, we obtained a collection of distinct object tracks spanning the entire footage.</p>
</section>
</section>
</section>
<section id="optical-flow-based-counting-via-bayesian-filtering-and-confidence-regions">
<h2>Optical flow-based counting via Bayesian filtering and confidence regions<a class="headerlink" href="#optical-flow-based-counting-via-bayesian-filtering-and-confidence-regions" title="Permalink to this heading">#</a></h2>
<p>Our counting method is divided into several interacting blocks.
First, a detector outputs a set of predicted positions for objects in the current frame.
The second block is a tracking module designing consistent trajectories of potential objects within the video.
At each frame, a third block links the successive detections together using confidence regions provided by the tracking module, proposing distinct tracks for each object.
A final postprocessing step only keeps the best tracks which are enumerated to yield the final count.</p>
<section id="detector">
<h3>Detector<a class="headerlink" href="#detector" title="Permalink to this heading">#</a></h3>
<section id="center-based-anchor-free-detection">
<h4>Center-based anchor-free detection<a class="headerlink" href="#center-based-anchor-free-detection" title="Permalink to this heading">#</a></h4>
<p>In most benchmarks, the prediction quality of object attributes like bounding boxes is often used to  improve tracking.
For counting, however, point detection is theoretically enough and advantageous in many ways.
First, to build large datasets, a method which only requires the lightest annotation format may benefit from more data due to annotation ease.
Second, contrary to previous popular methods <span id="id31">[<a class="reference internal" href="#id102" title="Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: towards real-time object detection with region proposal networks. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS'15, 91–99. Cambridge, MA, USA, 2015. MIT Press.">22</a>]</span> involving intricate mechanisms for bounding box prediction, center-based and anchor-free detectors <span id="id32">[<a class="reference internal" href="#id79" title="Xingyi Zhou, Dequan Wang, and Philipp Krähenbühl. Objects as points. apr 2019. URL: http://arxiv.org/abs/1904.07850, arXiv:1904.07850.">34</a>, <a class="reference internal" href="#id94" title="Hei Law and Jia Deng. Cornernet: detecting objects as paired keypoints. In Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss, editors, Computer Vision - ECCV 2018 - 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part XIV, volume 11218 of Lecture Notes in Computer Science, 765–781. Springer, 2018. URL: https://doi.org/10.1007/978-3-030-01264-9\_45, doi:10.1007/978-3-030-01264-9\_45.">35</a>]</span> only use additional regression heads which can simply be removed for point detection.
Adding to all this, <span id="id33">[<a class="reference internal" href="#id82" title="Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng, and Wenyu Liu. Fairmot: on the fairness of detection and re-identification in multiple object tracking. International Journal of Computer Vision, pages 1–19, 2021.">30</a>]</span> highlight conceptual and experimental reasons to favor anchor-free detection in tracking-related tasks.</p>
<p><br />
For these reasons, we use a stripped version of CenterNet <span id="id34">[<a class="reference internal" href="#id79" title="Xingyi Zhou, Dequan Wang, and Philipp Krähenbühl. Objects as points. apr 2019. URL: http://arxiv.org/abs/1904.07850, arXiv:1904.07850.">34</a>]</span> where offset and bounding box regression heads are discarded to output bare estimates of center positions on a coarse grid.
An encoder-decoder network takes an input image <span class="math notranslate nohighlight">\(I \in [0,1]^{w \times h \times 3}\)</span> (an RGB image of width <span class="math notranslate nohighlight">\(w\)</span> and height <span class="math notranslate nohighlight">\(h\)</span>), and produces a heatmap <span class="math notranslate nohighlight">\(\hat{Y} \in [0,1]^{\lfloor w/p\rfloor \times \lfloor h/p\rfloor}\)</span> such that  <span class="math notranslate nohighlight">\(\hat{Y}_{xy}\)</span> is the probability that <span class="math notranslate nohighlight">\((x,y)\)</span> is the center of an object (<span class="math notranslate nohighlight">\(p\)</span> being a stride coefficient).
At inference, peak detection and thresholding are applied to <span class="math notranslate nohighlight">\(\hat{Y}\)</span>, yielding the set of detections.
The bulk of this detector relies on the DLA34 architecture <span id="id35">[<a class="reference internal" href="#id60" title="Fisher Yu, Dequan Wang, Evan Shelhamer, and Trevor Darrell. Deep layer aggregation. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, volume, 2403-2412. 2018. doi:10.1109/CVPR.2018.00255.">36</a>]</span>.
In a video, for each frame <span class="math notranslate nohighlight">\(I_n \in [0,1]^{w \times h \times 3}\)</span> (where <span class="math notranslate nohighlight">\(n\)</span> indexes the frame number), the detector outputs a set <span class="math notranslate nohighlight">\(\detectset_n = \{z_n^i\}_{1 \leq i \leq D_n}\)</span> where each <span class="math notranslate nohighlight">\(z_n^i = (x_n^i,y_n^i)\)</span> specifies the coordinates of one of the <span class="math notranslate nohighlight">\(D_n\)</span> detected objects.</p>
</section>
</section>
<section id="training">
<span id="detector-training"></span><h3>Training<a class="headerlink" href="#training" title="Permalink to this heading">#</a></h3>
<p>Training the detector is done similarly as in <span id="id36">[<a class="reference internal" href="#id103" title="Pedro F Proença and Pedro Simões. TACO: Trash Annotations in Context for Litter Detection. 2020. URL: http://tacodataset.org/ http://arxiv.org/abs/2003.06975, arXiv:2003.06975.">12</a>]</span>.
For every image, the corresponding set <span class="math notranslate nohighlight">\(\mathcal{B} = \{(c^w_i,c^h_i,w_i,h_i)\}_{1 \leq i\leq B}\)</span> of <span class="math notranslate nohighlight">\(B\)</span> annotated bounding boxes – <em>i.e.</em> a center <span class="math notranslate nohighlight">\((c^w_i,c^h_i)\)</span>, a width <span class="math notranslate nohighlight">\(w_i\)</span> and a height <span class="math notranslate nohighlight">\(h_i\)</span>– is rendered into a ground truth heatmap <span class="math notranslate nohighlight">\(Y \in [0,1]^{{\lfloor w/p\rfloor \times \lfloor h/p\rfloor}}\)</span> by applying kernels at the bounding box centers and taking element-wise maximum.
For all <span class="math notranslate nohighlight">\(1 \leq x \leq w/p\)</span>, <span class="math notranslate nohighlight">\(1 \leq y \leq h/p\)</span>, the ground truth at <span class="math notranslate nohighlight">\((x,y)\)</span> is</p>
<div class="math notranslate nohighlight">
\[
  Y_{xy} =  \max\limits_{1\leq i\leq B}\left(\exp\left\{-\frac{(x-c_i^w)^2+(y-c_i^h)^2}{2\sigma^2_i}\right\}\right),
\]</div>
<p>where <span class="math notranslate nohighlight">\(\sigma_i\)</span> is a parameter depending on the size of the object.
Training the detector is done by minimizing a penalty-reduced weighted focal loss</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\hat{Y},Y) = -\sum_{x,y} \gamma_{xy}^\beta\left(1-\hat{p}_{xy}\right)^\alpha \log{\left(\hat{p}_{xy}\right)},
\]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha\)</span>, <span class="math notranslate nohighlight">\(\beta\)</span> are hyperparameters and</p>
<div class="math notranslate nohighlight">
\[\begin{split}
(\hat{p}_{xy},\gamma_{xy}) = \left\{
    \begin{array}{ll}
        (\hat{Y}_{xy},1) &amp; \mbox{if } Y_{xy} = 1, \\
        (1 - \hat{Y}_{xy},1 - Y_{xy}) &amp; \mbox{otherwise.}
    \end{array}
\right.
\end{split}\]</div>
</section>
<section id="bayesian-tracking-with-optical-flow">
<span id="bayesian-tracking"></span><h3>Bayesian tracking with optical flow<a class="headerlink" href="#bayesian-tracking-with-optical-flow" title="Permalink to this heading">#</a></h3>
<section id="optical-flow">
<h4>Optical flow<a class="headerlink" href="#optical-flow" title="Permalink to this heading">#</a></h4>
<p>Between two timesteps <span class="math notranslate nohighlight">\(n-1\)</span> and <span class="math notranslate nohighlight">\(n\)</span>, the optical flow <span class="math notranslate nohighlight">\(\Delta_n\)</span> is a mapping  satisfying the following consistency constraint <span id="id37">[<a class="reference internal" href="#id61" title="Nikos Paragios, Yunmei Chen, and Olivier D Faugeras. Handbook of mathematical models in computer vision. Springer Science &amp; Business Media, 2006.">37</a>]</span>:</p>
<div class="math notranslate nohighlight">
\[
\widetilde{I}_n[u] = \widetilde{I}_{n-1}[u+\Delta_n(u)],
\]</div>
<p>where, in our case, <span class="math notranslate nohighlight">\(\widetilde{I}_n\)</span> denotes the frame <span class="math notranslate nohighlight">\(n\)</span> downsampled to dimensions <span class="math notranslate nohighlight">\(\lfloor w/p\rfloor \times \lfloor h/p\rfloor\)</span> and <span class="math notranslate nohighlight">\(u = (x,y)\)</span> is a coordinate on that grid.
To estimate <span class="math notranslate nohighlight">\(\Delta_n\)</span>, we choose a simple unsupervised Gunner-Farneback algorithm which does not require further annotations, see <span id="id38">[<a class="reference internal" href="#id71" title="G. Farnebäck. Two-frame motion estimation based on polynomial expansion. In Scandinavian conference on Image analysis, 363–370. Springer, 2003.">38</a>]</span> for details.</p>
</section>
<section id="state-space-model">
<span id="id39"></span><h4>State space model<a class="headerlink" href="#state-space-model" title="Permalink to this heading">#</a></h4>
<p>Using optical flow as a building block, we posit a state space model where estimates of <span class="math notranslate nohighlight">\(\Delta_n\)</span> are used as a time and state-dependent offset for the state transition.
Let <span class="math notranslate nohighlight">\((X_k)_{k \geq 1}\)</span> and <span class="math notranslate nohighlight">\((Z_k)_{k \geq 1}\)</span> be the true (but hidden) and observed (detected) positions of a target object in <span class="math notranslate nohighlight">\(\Rset^2\)</span>, respectively.
Considering the optical flow value associated with <span class="math notranslate nohighlight">\(X_{k-1}\)</span> on the discrete grid of dimensions <span class="math notranslate nohighlight">\(\lfloor w/p\rfloor \times \lfloor h/p\rfloor\)</span>, write</p>
<div class="math notranslate nohighlight" id="equation-state-transition-eq">
<span class="eqno">(1)<a class="headerlink" href="#equation-state-transition-eq" title="Permalink to this equation">#</a></span>\[X_k = X_{k-1} + \Delta_k(\lfloor X_{k-1} \rfloor) + \eta_k\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
Z_k = X_k + \varepsilon_k,
\]</div>
<p>where <span class="math notranslate nohighlight">\((\eta_k)_{k\geq 1}\)</span> are i.i.d. centered Gaussian random variables with covariance matrix <span class="math notranslate nohighlight">\(Q\)</span> independent of <span class="math notranslate nohighlight">\((\varepsilon_k)_{k\geq 1}\)</span> i.i.d. centered Gaussian random variables with covariance matrix <span class="math notranslate nohighlight">\(R\)</span>.
In the following, <span class="math notranslate nohighlight">\(Q\)</span> and <span class="math notranslate nohighlight">\(R\)</span> are assumed to be diagonal, and are hyperparameters set to values given in <a class="reference internal" href="#covariance-matrices"><span class="std std-ref">Covariance matrices for state and observation noises</span></a>.</p>
</section>
<section id="approximations-of-the-filtering-distributions">
<h4>Approximations of the filtering distributions<a class="headerlink" href="#approximations-of-the-filtering-distributions" title="Permalink to this heading">#</a></h4>
<p>Denoting <span class="math notranslate nohighlight">\(u_{1:k} = (u_1,\ldots,u_k)\)</span> for any <span class="math notranslate nohighlight">\(k\)</span> and sequence <span class="math notranslate nohighlight">\((u_i)_{i \geq 0}\)</span>, Bayesian filtering aims at computing the conditional distribution of <span class="math notranslate nohighlight">\(X_k\)</span> given <span class="math notranslate nohighlight">\(Z_{1:k}\)</span>, referred to as the filtering distribution.
In the case of linear and Gaussian state space models, this distribution is known to be Gaussian, and Kalman filtering allows to update exactly the posterior mean <span class="math notranslate nohighlight">\(\mu_k = \esp[X_k|Z_{1:k}]\)</span> and posterior variance matrix <span class="math notranslate nohighlight">\(\Sigma_k = \var[X_k|Z_{1:k}]\)</span>.
This algorithm and its extensions are prevalent and used extensively in time-series and sequential-data analysis.
As the transition model proposed in <a class="reference internal" href="#equation-state-transition-eq">(1)</a> is nonlinear, Kalman updates cannot be implemented and solving the target tracking task requires resorting to alternatives.
Many solutions have been proposed to deal with strong nonlinearities in the literature, such as unscented Kalman filters (UKF) or Sequential Monte Carlo (SMC) methods (see <span id="id40">[<a class="reference internal" href="#id68" title="S. Särkkä. Bayesian Filtering and Smoothing. Cambridge University Press, New York, NY, USA, 2013.">39</a>]</span> and references therein). Most SMC methods have been widely studied and shown to be very effective even in presence of strongly nonlinear dynamics and/or non-Gaussian noise, however such sample-based solutions are computationally intensive, especially in settings where many objects have to be tracked and false positive detections involve unnecessary sampling steps. On the other hand, UKF requires fewer samples and provides an intermediary solution in presence of mild nonlinearities. In our setting, we find that a linearisation of the model <a class="reference internal" href="#equation-state-transition-eq">(1)</a> yields approximation which is computationally cheap and as robust on our data:</p>
<div class="math notranslate nohighlight">
\[
X_k = X_{k-1} + \Delta_k(\lfloor \mu_{k-1} \rfloor) + \partial_X\Delta_k(\lfloor \mu_{k-1} \rfloor)(X_{k-1}-\mu_{k-1}) + \eta_k .
\]</div>
<p>where <span class="math notranslate nohighlight">\(\partial_X\)</span> is the derivative operator with respect to the 2-dimensional spatial input <span class="math notranslate nohighlight">\(X\)</span>.</p>
<p>This allows the implementation of Kalman updates on the linearised model, a technique named extended Kalman filtering (EKF). For a more complete presentation of Bayesian and Kalman filtering, please refer to <a class="reference internal" href="#bayesian-filtering"><span class="std std-ref">this appendix</span></a>. On the currently available data, we find that the optical flow estimates are very informative and accurate, making this approximation sufficient. For completeness, we present <a class="reference internal" href="#impact-algorithm-appendix"><span class="std std-ref">here</span></a> an SMC-based solution and discuss the empirical differences and use-cases where the latter might be a more relevant choice.</p>
<p>In any case, the state space model naturally accounts for missing observations, as the contribution of <span class="math notranslate nohighlight">\(\Delta_k\)</span> in every transition ensures that each filter can cope with arbitrary inter-frame motion to keep track of its target.</p>
</section>
<section id="generating-potential-object-tracks">
<h4>Generating potential object tracks<a class="headerlink" href="#generating-potential-object-tracks" title="Permalink to this heading">#</a></h4>
<p>The full MOT algorithm consists of a set of single-object trackers following the previous model, but each provided with distinct observations at every frame.
These separate filters provide track proposals for every object detected in the video.</p>
</section>
</section>
<section id="data-association-using-confidence-regions">
<span id="data-association"></span><h3>Data association using confidence regions<a class="headerlink" href="#data-association-using-confidence-regions" title="Permalink to this heading">#</a></h3>
<p>Throughout the video, depending on various conditions on the incoming detections, existing trackers must be updated (with or without a new observation) and others might need to be created. This setup requires a third party data association block to link the incoming detections with the correct filters.</p>
<p>At the frame <span class="math notranslate nohighlight">\(n\)</span>, a set of <span class="math notranslate nohighlight">\(L_n\)</span> Bayesian filters track previously seen objects and a new set of detections <span class="math notranslate nohighlight">\(\mathcal{D}_n\)</span> is provided by the detector.
Denote by <span class="math notranslate nohighlight">\(1 \leq \ell \leq L_n\)</span> the index of each filter at time <span class="math notranslate nohighlight">\(n\)</span>, and by convention write <span class="math notranslate nohighlight">\(Z^\ell_{1:n-1}\)</span>  the previous observed positions associated with index <span class="math notranslate nohighlight">\(\ell\)</span> (even if no observation is available at some past times for that object).
Let <span class="math notranslate nohighlight">\(\rho \in (0,1)\)</span> be a confidence level.</p>
<ol class="arabic simple">
<li><p>For every detected object <span class="math notranslate nohighlight">\(z_n^i \in \detectset_n\)</span> and every filter <span class="math notranslate nohighlight">\(\ell\)</span>, compute <span class="math notranslate nohighlight">\(P(i,\ell) = \prob(Z_n^\ell \in V_\delta(z_n^i)\mid Z^\ell_{1:n-1})\)</span> where <span class="math notranslate nohighlight">\(V_\delta(z)\)</span> is the neighborhood of <span class="math notranslate nohighlight">\(z\)</span> defined as the squared area of width <span class="math notranslate nohighlight">\(2\delta\)</span> centered on <span class="math notranslate nohighlight">\(z\)</span> (see <a class="reference internal" href="#confidence-regions-appendix"><span class="std std-ref">this appendix</span></a> for exact computations).</p></li>
<li><p>Using the Hungarian algorithm <span id="id41">[<a class="reference internal" href="#id72" title="H. W. Kuhn. The hungarian method for the assignment problem. Naval Research Logistics Quarterly, 2(1-2):83-97, 1955. URL: https://onlinelibrary.wiley.com/doi/abs/10.1002/nav.3800020109, arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1002/nav.3800020109, doi:https://doi.org/10.1002/nav.3800020109.">40</a>]</span>, compute the assignment between detections and filters with <span class="math notranslate nohighlight">\(P\)</span> as cost function, but discarding associations <span class="math notranslate nohighlight">\((i,\ell)\)</span> having <span class="math notranslate nohighlight">\(P(i,\ell) &lt; \rho\)</span>.
Formally, <span class="math notranslate nohighlight">\(\rho\)</span> represents the level of a confidence region centered on detections and we use <span class="math notranslate nohighlight">\(\rho = 0.5\)</span>.
Denote <span class="math notranslate nohighlight">\(a_{\rho}\)</span> the resulting assignment map defined as <span class="math notranslate nohighlight">\(a_{\rho}(i) = \ell\)</span> if <span class="math notranslate nohighlight">\(z_n^i\)</span> was associated with the <span class="math notranslate nohighlight">\(\ell\)</span>-th filter, and <span class="math notranslate nohighlight">\(a_{\rho}(i) = 0\)</span> if <span class="math notranslate nohighlight">\(z_n^i\)</span> was not associated with any filter.</p></li>
<li><p>For <span class="math notranslate nohighlight">\(1 \leq i \leq D_n\)</span>, if <span class="math notranslate nohighlight">\(a_{\rho}(i) = \ell\)</span>, use <span class="math notranslate nohighlight">\(z_n^i\)</span> as a new observation to update the <span class="math notranslate nohighlight">\(\ell\)</span>-th filter.
If <span class="math notranslate nohighlight">\(a_{\rho}(i) = 0\)</span>, create a new filter initialized from the prior distribution, i.e.
sample the true location as a Gaussian random variable with mean <span class="math notranslate nohighlight">\(z_n^i\)</span> and variance <span class="math notranslate nohighlight">\(R\)</span>.</p></li>
<li><p>For all filters <span class="math notranslate nohighlight">\(\ell'\)</span> which were not provided a new observation, update only the predictive law of <span class="math notranslate nohighlight">\(X^{\ell'}_{n}\)</span> given <span class="math notranslate nohighlight">\(Z^{\ell'}_{1:n-1}\)</span>.</p></li>
</ol>
<p>In other words, we seek to associate filters and detections by maximising a global cost built from the predictive distributions of the available filters, but an association is only valid if its corresponding predictive probability is high enough.
Though the Hungarian algorithm is a very popular algorithm in MOT, it is often used with the Euclidean distance or an Intersection-over-Union (IoU) criterion.
Using confidence regions for the distributions of <span class="math notranslate nohighlight">\(Z_n\)</span> given <span class="math notranslate nohighlight">\(Z_{1:(n - 1)}\)</span> instead allows to naturally include uncertainty in the decision process.
Note that we deactivate filters whose posterior mean estimates lie outside the image subspace in <span class="math notranslate nohighlight">\(\Rset^2\)</span>.</p>
<p>A visual depiction of the entire pipeline (from detection to final association) is provided below. This way of combining a set of Bayesian filters with a data association step that resorts on the most likely hypothesis is a form of Global Nearest Neighbor (GNN) tracking. Another possibility is to perform multi-target filtering by including the data association step directly into the probabilistic model, as in <span id="id42">[<a class="reference internal" href="#id59" title="R.P.S. Mahler. Multitarget bayes filtering via first-order multitarget moments. IEEE Transactions on Aerospace and Electronic Systems, 39(4):1152-1178, 2003. doi:10.1109/TAES.2003.1261119.">41</a>]</span>. A generalisation of single-target recursive Bayesian filtering, this class of methods is grounded in the point process literature and well motivated theoretically. In case of strong false positive detection rates, close and/or reappearing objects, practical benefits may be obtained from these solutions. Finally, note that another well-motivated choice for <span class="math notranslate nohighlight">\(P(i,\ell)\)</span> could be to use the marginal likelihood <span class="math notranslate nohighlight">\(\prob(Z_n^\ell \in V_\delta(z_n^i))\)</span>, which is standard in modern MOT.</p>
<figure class="align-default" id="diagram">
<img alt="_images/diagram.png" src="_images/diagram.png" />
<figcaption>
<p><span class="caption-number">Fig. 3 </span><span class="caption-text">Visual representation of the tracking pipeline.</span><a class="headerlink" href="#diagram" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="counting">
<h3>Counting<a class="headerlink" href="#counting" title="Permalink to this heading">#</a></h3>
<p>At the end of the video, the previous process returns a set of candidate tracks.
For counting purposes, we find that simple heuristics can be further applied to filter out tracks that do not follow actual objects.
More precisely, we observe that tracks of real objects usually contain more (i) observations and (ii) streams of uninterrupted observations.
Denote by <span class="math notranslate nohighlight">\(T_\ell = \left\{n \in \mathbb{N} \mid \exists  z \in \detectset_n,  Z_n^{\ell} = z\right\}\)</span> all timesteps where the <span class="math notranslate nohighlight">\(\ell\)</span>-th object is observed.
To discard false counts according to (i) and (ii), we compute the moving average <span class="math notranslate nohighlight">\(M_\ell^\kappa\)</span> of <span class="math notranslate nohighlight">\(1_{T_\ell}\)</span> using windows of size <span class="math notranslate nohighlight">\(\kappa\)</span>, i.e. the sequence defined by <span class="math notranslate nohighlight">\(M_\ell^\kappa[n] = \frac{1}{\kappa} \sum_{k \in [\![n - \kappa, n + \kappa]\!]} 1_{T_\ell}[k]\)</span>. We then build <span class="math notranslate nohighlight">\(T_\ell^\kappa = \left\{n \in T_\ell \mid M_\ell^\kappa[n] &gt; \nu\right\}\)</span>, and defining <span class="math notranslate nohighlight">\(\mathcal{N} = \left\{\ell \mid |T_\ell^\kappa| &gt; \tau\right\}\)</span>, the final object count is <span class="math notranslate nohighlight">\(|\mathcal{N}|\)</span>.
We choose <span class="math notranslate nohighlight">\(\nu = 0.6\)</span> while <span class="math notranslate nohighlight">\(\kappa,\tau\)</span> are optimized for best count performance (see <a class="reference internal" href="#tau-kappa-appendix"><span class="std std-ref">here</span></a> for a more comprehensive study).</p>
</section>
</section>
<section id="metrics-for-mot-based-counting">
<h2>Metrics for MOT-based counting<a class="headerlink" href="#metrics-for-mot-based-counting" title="Permalink to this heading">#</a></h2>
<p>Counting in videos using embedded moving cameras is not a common task, and as such it requires a specific evaluation protocol to understand and compare the performance of competing methods.
First, not all MOT metrics are relevant, even if some do provide insights to assist evaluation of count performance.
Second, considering only raw counts on long videos gives little information on which of the final counts effectively arise from well detected objects.</p>
<section id="count-related-mot-metrics">
<h3>Count-related MOT metrics<a class="headerlink" href="#count-related-mot-metrics" title="Permalink to this heading">#</a></h3>
<p>Popular MOT benchmarks usually report several sets of metrics such as ClearMOT <span id="id43">[<a class="reference internal" href="#id117" title="Keni Bernardin and Rainer Stiefelhagen. Evaluating multiple object tracking performance: the clear mot metrics. EURASIP Journal on Image and Video Processing, 2008:, 01 2008. doi:10.1155/2008/246309.">42</a>]</span> or IDF1 <span id="id44">[<a class="reference internal" href="#id118" title="Ergys Ristani, Francesco Solera, Roger Zou, Rita Cucchiara, and Carlo Tomasi. Performance measures and a data set for multi-target, multi-camera tracking. In European conference on computer vision, 17–35. Springer, 2016.">43</a>]</span> which can account for different components of tracking performance.
Recently, <span id="id45">[<a class="reference internal" href="#id105" title="Jonathon Luiten, Aljosa Osep, Patrick Dendorfer, Philip Torr, Andreas Geiger, Laura Leal-Taixé, and Bastian Leibe. Hota: a higher order metric for evaluating multi-object tracking. International journal of computer vision, 129(2):548–578, 2021.">15</a>]</span> built the so-called HOTA metrics that allow separate evaluation of detection and association using the Jaccard index.
The following components of their work are relevant to our task (we provide equation numbers in the original paper for formal definitions).</p>
<section id="detection">
<h4>Detection<a class="headerlink" href="#detection" title="Permalink to this heading">#</a></h4>
<p>First, when considering all frames independently, traditional detection recall (<span class="math notranslate nohighlight">\(\detre\)</span>) and precision (<span class="math notranslate nohighlight">\(\detpr\)</span>) can be computed to assess the capabilities of the object detector. Denoting with <span class="math notranslate nohighlight">\(\tp_n\)</span>, <span class="math notranslate nohighlight">\(\fp_n\)</span>, <span class="math notranslate nohighlight">\(\fn_n\)</span> the number of true positive, false positive and false negative detections at frame <span class="math notranslate nohighlight">\(n\)</span>, respectively, we define <span class="math notranslate nohighlight">\(\tp = \sum_n \tp_n\)</span>, <span class="math notranslate nohighlight">\(\fp = \sum_n \fp_n\)</span> and <span class="math notranslate nohighlight">\(\fn = \sum_n \fn_n\)</span>, then:</p>
<div class="math notranslate nohighlight">
\[\detre = \frac{\tp}{\tp + \fn},\]</div>
<div class="math notranslate nohighlight">
\[\detpr = \frac{\tp}{\tp + \fp}.\]</div>
<p>In classical object detection, those metrics are the main target.
In our context, as the first step of the system, this framewise performance impacts the difficulty of counting.
However, we must keep in mind that these metrics are computed framewise and might not guarantee anything at a video scale.
The next points illustrate that remark.</p>
<ol class="arabic simple">
<li><p>If both <span class="math notranslate nohighlight">\(\detre\)</span> and <span class="math notranslate nohighlight">\(\detpr\)</span> are very high, objects are detected at nearly all frames and most detections come from actual objects.
Therefore, robustness to missing observations is high, but even in this context computing associations may fail if camera movements are nontrivial.</p></li>
<li><p>For an ideal tracking algorithm which never counts individual objects twice and does not confuse separate objects in a video, a detector capturing each object for only one frame could theoretically be used.
Thus, low <span class="math notranslate nohighlight">\(\detre\)</span> could theoretically be compensated with robust tracking.</p></li>
<li><p>If our approach can rule out faulty tracks which do not follow actual objects, then good counts can still be obtained using a detector generating many false positives.
Again, this suggests that low <span class="math notranslate nohighlight">\(\detpr\)</span> may allow decent counting performance.</p></li>
</ol>
</section>
<section id="association">
<h4>Association<a class="headerlink" href="#association" title="Permalink to this heading">#</a></h4>
<p>HOTA association metrics are built to measure tracking performance irrespective of the detection capabilities, by comparing predicted tracks against true object trajectories.
In our experiments, we compute the Association Recall (<span class="math notranslate nohighlight">\(\assre\)</span>) and the Association Precision (<span class="math notranslate nohighlight">\(\asspr\)</span>). Several intermediate quantities are necessary to introduce these final metrics. Following <span id="id46">[<a class="reference internal" href="#id105" title="Jonathon Luiten, Aljosa Osep, Patrick Dendorfer, Philip Torr, Andreas Geiger, Laura Leal-Taixé, and Bastian Leibe. Hota: a higher order metric for evaluating multi-object tracking. International journal of computer vision, 129(2):548–578, 2021.">15</a>]</span>, we denote with <span class="math notranslate nohighlight">\(\prID\)</span> the ID of a predicted track and <span class="math notranslate nohighlight">\(\gtID\)</span> the ID of a ground truth track. Given
<span class="math notranslate nohighlight">\(C\)</span> all couples of <span class="math notranslate nohighlight">\(\prID-\gtID\)</span> found among the true positive detections, and <span class="math notranslate nohighlight">\(c \in C\)</span> one of these couples, <span class="math notranslate nohighlight">\(\tpa(c)\)</span> is the number of frames where <span class="math notranslate nohighlight">\(\prID\)</span> is also associated with <span class="math notranslate nohighlight">\(\gtID\)</span>, <span class="math notranslate nohighlight">\(\fpa(c)\)</span> is the number of frames where <span class="math notranslate nohighlight">\(\prID\)</span> is associated with another ground truth ID or with no ground truth ID, and <span class="math notranslate nohighlight">\(\fna(c)\)</span> is the number of frames where <span class="math notranslate nohighlight">\(\gtID\)</span> is associated with another predicted ID or with no predicted ID. Then:</p>
<div class="math notranslate nohighlight">
\[\asspr = \frac{1}{\tp} \sum_{c \in C} \frac{\tpa(c)}{\tpa(c) + \fpa(c)},\]</div>
<div class="math notranslate nohighlight">
\[\assre = \frac{1}{\tp} \sum_{c \in C} \frac{\tpa(c)}{\tpa(c) + \fna(c)}.\]</div>
<p>See <span id="id47">[<a class="reference internal" href="#id105" title="Jonathon Luiten, Aljosa Osep, Patrick Dendorfer, Philip Torr, Andreas Geiger, Laura Leal-Taixé, and Bastian Leibe. Hota: a higher order metric for evaluating multi-object tracking. International journal of computer vision, 129(2):548–578, 2021.">15</a>]</span> (fig. 2) for a clear illustration of these quantities.</p>
<p>In brief, a low <span class="math notranslate nohighlight">\(\asspr\)</span> implies that several objects are often mingled into only one track, resulting in undercount.
A low <span class="math notranslate nohighlight">\(\assre\)</span> implies that single objects are often associated with multiple tracks.
If no method is used to discard redundant tracks this results in overcount.
Conversely, association precision (<span class="math notranslate nohighlight">\(\asspr\)</span>) measures how exclusive tracks are to each object (it decreases whenever a track covers multiple objects).
Again, it is useful to reconsider and illustrate the meaning of these metrics in the context of MOT-based counting.
Litter items are typically well separated on river banks, thus predicted tracks are not expected to interfere much.
This suggests that reaching high <span class="math notranslate nohighlight">\(\asspr\)</span> on our footage is not challenging.
Contrarily, <span class="math notranslate nohighlight">\(\assre\)</span> is a direct measurement of the capability of the tracker to avoid producing multiple tracks despite missing detections and challenging motion.
A high <span class="math notranslate nohighlight">\(\assre\)</span> therefore typically avoids multiple counts for the same object, which is a key aspect of our work.</p>
<p>Nonetheless, association metrics are only computed for predicted tracks which can effectively be matched with ground truth tracks.
Consequently, <span class="math notranslate nohighlight">\(\assre\)</span> does not account for tracks predicted from streams of false positive detections generated by the detector (e.g.
arising from rocks, water reflections, etc).
Since such tracks induce false counts, a tracker which produces the fewest is better, but MOT metrics do not measure it.</p>
</section>
</section>
<section id="count-metrics">
<h3>Count metrics<a class="headerlink" href="#count-metrics" title="Permalink to this heading">#</a></h3>
<p>Denoting by <span class="math notranslate nohighlight">\(\hatN\)</span> and <span class="math notranslate nohighlight">\(\N\)</span> the respective predicted and ground truth counts for the validation material, the error <span class="math notranslate nohighlight">\(\hatN - \N\)</span> is misleading as no information is provided on the quality of the predicted counts.
Additionally, results on the original validation footage do not measure the statistical variability of the proposed estimators.</p>
<section id="count-decomposition">
<h4>Count decomposition<a class="headerlink" href="#count-decomposition" title="Permalink to this heading">#</a></h4>
<p>Define <span class="math notranslate nohighlight">\(\gtlabels\)</span> and <span class="math notranslate nohighlight">\(\predlabels\)</span> the labels of the annotated ground truth tracks and the predicted tracks, respectively.
At evaluation, we assign each predicted track to either none or at most one ground truth track, writing <span class="math notranslate nohighlight">\(j \rightarrow \emptyset\)</span> or <span class="math notranslate nohighlight">\(j \rightarrow i\)</span> for the corresponding assignments. The association is made whenever a predicted track <span class="math notranslate nohighlight">\(i\)</span> overlaps with a ground truth track <span class="math notranslate nohighlight">\(j\)</span> at any frame, i.e. for a given frame a detection in <span class="math notranslate nohighlight">\(i\)</span> is within a threshold <span class="math notranslate nohighlight">\(\alpha\)</span> of an object in <span class="math notranslate nohighlight">\(j\)</span>. We compute metrics for 20 values of <span class="math notranslate nohighlight">\(\alpha \in [0.05 \alpha_{max}, 0.95 \alpha_{max}]\)</span>, with <span class="math notranslate nohighlight">\(\alpha_{max} = 0.1 \sqrt{w^2 + h^2}\)</span>, then average the results, which is the default method in HOTA to combine results at different thresholds. We keep this default solution, in particular because our results are very consistent accross different thresholds in that range (we only observe a slight decrease in performance for <span class="math notranslate nohighlight">\(\alpha = \alpha_{max}\)</span>, where occasional false detections probably start to lie below the threshold).</p>
<p>Denote <span class="math notranslate nohighlight">\(A_i = \{\predlabels \mid j \rightarrow i\}\)</span> the set of predicted tracks assigned to the <span class="math notranslate nohighlight">\(i\)</span>-th ground truth track.
We define:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\Ntrue = \sum_{i=1}^{\N} 1_{|A_i| &gt; 0}\)</span> the number of ground truth objects successfully counted.</p></li>
<li><p><span class="math notranslate nohighlight">\(\Nred = \sum_{i=1}^{\N} |A_i| - \Ntrue\)</span> the number of redundant counts per ground truth object.</p></li>
<li><p><span class="math notranslate nohighlight">\(\Nmis = \N - \Ntrue\)</span> the number of ground truth objects that are never effectively counted.</p></li>
<li><p><span class="math notranslate nohighlight">\(\Nfalse = \sum_{j=1}^{\hatN} 1_{j \rightarrow \emptyset}\)</span> the number of counts which cannot be associated with any ground truth object and are therefore considered as false counts.</p></li>
</ol>
<p>Using these metrics provides a much better understanding of <span class="math notranslate nohighlight">\(\hatN\)</span> as</p>
<div class="math notranslate nohighlight">
\[
\hatN = \Ntrue + \Nred + \Nfalse,
\]</div>
<p>while <span class="math notranslate nohighlight">\(\Nmis\)</span> completely summarises the number of undetected objects.</p>
<p>Conveniently, the quantities can be used to define the count precision (<span class="math notranslate nohighlight">\(\countpr\)</span>) and count recall (<span class="math notranslate nohighlight">\(\countre\)</span>) as follows:</p>
<div class="math notranslate nohighlight">
\[
\countpr = \frac{\Ntrue}{\Ntrue + \Nred + \Nfalse},
\]</div>
<div class="math notranslate nohighlight">
\[
\countre = \frac{\Ntrue}{\Ntrue + \Nmis},
\]</div>
<p>which provide good summaries for the overall count quality, letting aside the tracking performance.</p>
<p>Note that these metrics and the associated decomposition are only defined if the previous assignment between predicted and ground truth tracks can be obtained.
In our case, predicted tracks never overlap with several ground truth tracks (because true objects are well separated), and therefore this assignment is straightforward. More involved metrics have been studied at the trajectory level (see for example <span id="id48">[<a class="reference internal" href="#id58" title="Ángel F. García-Fernández, Abu Sajana Rahmathullah, and Lennart Svensson. A metric on the space of finite sets of trajectories for evaluation of multi-target tracking algorithms. IEEE Transactions on Signal Processing, 68:3917–3928, 2020. doi:10.1109/TSP.2020.3005309.">44</a>]</span> and the references therein), though not specifically tailored to the restricted task of counting. For more complicated data, an adaptation of such contributions into proper counting metrics could be valuable.</p>
</section>
<section id="statistics">
<h4>Statistics<a class="headerlink" href="#statistics" title="Permalink to this heading">#</a></h4>
<p>Since the original validation set comprises only a few unequally long videos, only absolute results are available.
Splitting the original sequences into shorter independent sequences of equal length allows to compute basic statistics.
For any quantity <span class="math notranslate nohighlight">\(\hatN_\bullet\)</span> defined above, we provide <span class="math notranslate nohighlight">\(\hat{\sigma}_{\hatN_\bullet}\)</span> the associated empirical standard deviations computed on the set of short sequences.</p>
</section>
</section>
</section>
<section id="experiments">
<h2>Experiments<a class="headerlink" href="#experiments" title="Permalink to this heading">#</a></h2>
<p>We denote by <span class="math notranslate nohighlight">\(S_1\)</span>, <span class="math notranslate nohighlight">\(S_2\)</span> and <span class="math notranslate nohighlight">\(S_3\)</span> the three river sections of the evaluation material and split the associated footage into independent segments of 30 seconds. We further divide this material into two distinct validation (6min30) and test (7min) splits.</p>
<p>To demonstrate the benefits of our work, we select two multi-object trackers and build competing counting systems from them. Our first choice is SORT <span id="id49">[<a class="reference internal" href="#id111" title="Alex Bewley, Zongyuan Ge, Lionel Ott, Fabio Ramos, and Ben Upcroft. Simple online and realtime tracking. In Proceedings - International Conference on Image Processing, ICIP, volume 2016-Augus, 3464–3468. 2016. URL: https://github.com/abewley/sort, arXiv:1602.00763, doi:10.1109/ICIP.2016.7533003.">26</a>]</span>, which relies on Kalman filtering with velocity updated using the latest past estimates of object positions. Similar to our system, it only relies on image supervision for training, and though DeepSORT <span id="id50">[<a class="reference internal" href="#id114" title="Nicolai Wojke, Alex Bewley, and Dietrich Paulus. Simple online and realtime tracking with a deep association metric. In Proceedings - International Conference on Image Processing, ICIP, volume 2017-Septe, 3645–3649. 2018. arXiv:1703.07402, doi:10.1109/ICIP.2017.8296962.">29</a>]</span> is a more recent alternative with better performance, the associated deep appearance network cannot be used without additional video annotations. FairMOT <span id="id51">[<a class="reference internal" href="#id82" title="Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng, and Wenyu Liu. Fairmot: on the fairness of detection and re-identification in multiple object tracking. International Journal of Computer Vision, pages 1–19, 2021.">30</a>]</span>, a more recent alternative, is similarly intended for use with video supervision but allows self-supervised training using only an image dataset. Built as a new baseline for MOT, it combines linear constant-velocity Kalman filtering with visual features computed by an additional network branch and extracted at the position of the estimated object centers, as introduced in CenterTrack <span id="id52">[<a class="reference internal" href="#id83" title="Xingyi Zhou, Vladlen Koltun, and Philipp Krähenbühl. Tracking objects as points. pages 474-490, 10 2020. doi:10.1007/978-3-030-58548-8_28.">45</a>]</span>. We choose FairMOT to compare our method to a solution based on deep visual feature extraction.</p>
<p>Similar to our work, FairMOT uses CenterNet for the detection part and the latter is therefore trained as in <a class="reference internal" href="#detector-training"><span class="std std-ref">Training</span></a>. We train it using hyperparameters from the original paper. The detection outputs are then shared between all counting methods, allowing fair comparison of counting performance given a fixed object detector. We run all experiments at 12fps, an intermediate framerate to capture all objects while reducing the computational burden.</p>
<section id="id53">
<h3>Detection<a class="headerlink" href="#id53" title="Permalink to this heading">#</a></h3>
<p>In the following section, we present the performance of the trained detector.
Having annotated all frames of the evaluation videos, we directly compute <span class="math notranslate nohighlight">\(\detre\)</span> and <span class="math notranslate nohighlight">\(\detpr\)</span> on those instead of a test split of the image dataset used for training.
This allows realistic assessment of the detection quality of our system on true videos that may include blurry frames or artifacts caused by strong motion.
We observe low <span class="math notranslate nohighlight">\(\detre\)</span>, suggesting that objects are only captured on a fraction of the frames they appear on.
To better focus on count performance in the next sections, we remove segments that do not generate any correct detection: performance on the remaining footage is increased and given by <span class="math notranslate nohighlight">\(\detre^{*}\)</span> and <span class="math notranslate nohighlight">\(\detpr^{*}\)</span>.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">fps</span> <span class="o">=</span> <span class="mi">12</span>
<span class="n">fps</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">fps</span><span class="si">}</span><span class="s1">fps&#39;</span>

<span class="n">split</span> <span class="o">=</span> <span class="s1">&#39;test&#39;</span>

<span class="n">long_segments_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;part_1_1&#39;</span><span class="p">,</span>
                    <span class="s1">&#39;part_1_2&#39;</span><span class="p">,</span>
                    <span class="s1">&#39;part_2&#39;</span><span class="p">,</span>
                    <span class="s1">&#39;part_3&#39;</span><span class="p">]</span>

<span class="n">indices_test</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">9</span><span class="p">,</span><span class="mi">13</span><span class="p">]</span>
<span class="n">indices_val</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">9</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">14</span><span class="p">]</span>
<span class="n">indices_det</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">17</span><span class="p">,</span><span class="mi">24</span><span class="p">,</span><span class="mi">38</span><span class="p">]</span>

<span class="n">alpha_type</span> <span class="o">=</span> <span class="s1">&#39;___50&#39;</span>

<span class="k">def</span> <span class="nf">set_split</span><span class="p">(</span><span class="n">split</span><span class="p">):</span>

    <span class="k">if</span> <span class="n">split</span> <span class="o">==</span> <span class="s1">&#39;val&#39;</span><span class="p">:</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">indices_val</span>
    <span class="k">elif</span> <span class="n">split</span> <span class="o">==</span> <span class="s1">&#39;test&#39;</span><span class="p">:</span>
        <span class="n">indices</span> <span class="o">=</span> <span class="n">indices_test</span>
    <span class="n">gt_dir_short</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;TrackEval/data/gt/surfrider_short_segments_</span><span class="si">{</span><span class="n">fps</span><span class="si">}</span><span class="s1">&#39;</span>
    <span class="n">eval_dir_short</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;TrackEval/data/trackers/surfrider_short_segments_</span><span class="si">{</span><span class="n">fps</span><span class="si">}</span><span class="s1">&#39;</span>
    <span class="k">if</span> <span class="n">split</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span> 
        <span class="n">gt_dir_short</span> <span class="o">+=</span> <span class="sa">f</span><span class="s1">&#39;_</span><span class="si">{</span><span class="n">split</span><span class="si">}</span><span class="s1">&#39;</span>
        <span class="n">eval_dir_short</span> <span class="o">+=</span> <span class="sa">f</span><span class="s1">&#39;_</span><span class="si">{</span><span class="n">split</span><span class="si">}</span><span class="s1">&#39;</span>
    <span class="n">gt_dir_short</span> <span class="o">+=</span> <span class="s1">&#39;/surfrider-test&#39;</span>
    <span class="k">return</span> <span class="n">indices</span><span class="p">,</span> <span class="n">eval_dir_short</span><span class="p">,</span> <span class="n">gt_dir_short</span>

<span class="n">indices</span><span class="p">,</span> <span class="n">eval_dir_short</span><span class="p">,</span> <span class="n">gt_dir_short</span> <span class="o">=</span> <span class="n">set_split</span><span class="p">(</span><span class="n">split</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">get_det_values</span><span class="p">(</span><span class="n">index_start</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">index_stop</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>

    <span class="n">results_for_det</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;TrackEval/data/trackers/surfrider_short_segments_</span><span class="si">{</span><span class="n">fps</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span><span class="s1">&#39;surfrider-test&#39;</span><span class="p">,</span><span class="s1">&#39;ours_EKF_1_kappa_1_tau_0&#39;</span><span class="p">,</span><span class="s1">&#39;pedestrian_detailed.csv&#39;</span><span class="p">))</span>
    <span class="n">results_det</span> <span class="o">=</span> <span class="n">results_for_det</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,[</span><span class="sa">f</span><span class="s1">&#39;DetRe</span><span class="si">{</span><span class="n">alpha_type</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span><span class="sa">f</span><span class="s1">&#39;DetPr</span><span class="si">{</span><span class="n">alpha_type</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;HOTA_TP</span><span class="si">{</span><span class="n">alpha_type</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span><span class="sa">f</span><span class="s1">&#39;HOTA_FN</span><span class="si">{</span><span class="n">alpha_type</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span><span class="sa">f</span><span class="s1">&#39;HOTA_FP</span><span class="si">{</span><span class="n">alpha_type</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">index_start</span><span class="p">:</span><span class="n">index_stop</span><span class="p">]</span>
    <span class="n">results_det</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;hota_det_re&#39;</span><span class="p">,</span><span class="s1">&#39;hota_det_pr&#39;</span><span class="p">,</span><span class="s1">&#39;hota_det_tp&#39;</span><span class="p">,</span><span class="s1">&#39;hota_det_fn&#39;</span><span class="p">,</span><span class="s1">&#39;hota_det_fp&#39;</span><span class="p">]</span>
    <span class="n">hota_det_re</span> <span class="o">=</span> <span class="n">results_det</span><span class="p">[</span><span class="s1">&#39;hota_det_re&#39;</span><span class="p">]</span>
    <span class="n">hota_det_pr</span> <span class="o">=</span> <span class="n">results_det</span><span class="p">[</span><span class="s1">&#39;hota_det_pr&#39;</span><span class="p">]</span>
    <span class="n">hota_det_tp</span> <span class="o">=</span> <span class="n">results_det</span><span class="p">[</span><span class="s1">&#39;hota_det_tp&#39;</span><span class="p">]</span>
    <span class="n">hota_det_fn</span> <span class="o">=</span> <span class="n">results_det</span><span class="p">[</span><span class="s1">&#39;hota_det_fn&#39;</span><span class="p">]</span>
    <span class="n">hota_det_fp</span> <span class="o">=</span> <span class="n">results_det</span><span class="p">[</span><span class="s1">&#39;hota_det_fp&#39;</span><span class="p">]</span>

    <span class="n">denom_hota_det_re</span> <span class="o">=</span> <span class="n">hota_det_tp</span> <span class="o">+</span> <span class="n">hota_det_fn</span> 
    <span class="n">denom_hota_det_pr</span> <span class="o">=</span> <span class="n">hota_det_tp</span> <span class="o">+</span> <span class="n">hota_det_fp</span> 

    <span class="n">hota_det_re_cb</span> <span class="o">=</span> <span class="p">(</span><span class="n">hota_det_re</span> <span class="o">*</span> <span class="n">denom_hota_det_re</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">denom_hota_det_re</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="n">hota_det_pr_cb</span> <span class="o">=</span> <span class="p">(</span><span class="n">hota_det_pr</span> <span class="o">*</span> <span class="n">denom_hota_det_pr</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">denom_hota_det_pr</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

    <span class="k">return</span> <span class="p">[</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">hota_det_re_cb</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">hota_det_pr_cb</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">]</span>
    
<span class="k">def</span> <span class="nf">get_table_det</span><span class="p">():</span>

    <span class="n">table_values</span> <span class="o">=</span> <span class="p">[</span><span class="n">get_det_values</span><span class="p">(</span><span class="n">index_start</span><span class="p">,</span> <span class="n">index_stop</span><span class="p">)</span> <span class="k">for</span> <span class="p">(</span><span class="n">index_start</span><span class="p">,</span> <span class="n">index_stop</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">indices_det</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">indices_det</span><span class="p">[</span><span class="mi">1</span><span class="p">:])]</span>
    <span class="n">table_values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">get_det_values</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">table_values</span><span class="p">)</span>

<span class="n">table_det</span> <span class="o">=</span> <span class="n">get_table_det</span><span class="p">()</span>
<span class="n">table_det</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;DetRe*&#39;</span><span class="p">,</span><span class="s1">&#39;DetPr*&#39;</span><span class="p">]</span>
<span class="n">table_det</span><span class="o">.</span><span class="n">index</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;S1&#39;</span><span class="p">,</span><span class="s1">&#39;S2&#39;</span><span class="p">,</span><span class="s1">&#39;S3&#39;</span><span class="p">,</span><span class="s1">&#39;All&#39;</span><span class="p">]</span>
<span class="n">display</span><span class="p">(</span><span class="n">table_det</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>DetRe*</th>
      <th>DetPr*</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>S1</th>
      <td>37.2</td>
      <td>60.7</td>
    </tr>
    <tr>
      <th>S2</th>
      <td>29.4</td>
      <td>38.2</td>
    </tr>
    <tr>
      <th>S3</th>
      <td>35.1</td>
      <td>53.6</td>
    </tr>
    <tr>
      <th>All</th>
      <td>35.5</td>
      <td>55.1</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="counts">
<h3>Counts<a class="headerlink" href="#counts" title="Permalink to this heading">#</a></h3>
<p>To fairly compare the three solutions, we calibrate the hyperparameters of our postprocessing block on the validation split and keep the values that minimize the overall count error <span class="math notranslate nohighlight">\(\hatN\)</span> for each of them separately (see <a class="reference internal" href="#tau-kappa-appendix"><span class="std std-ref">this appendix</span></a> for more information). All methods are found to work optimally at <span class="math notranslate nohighlight">\(\kappa = 7\)</span>, but our solution requires <span class="math notranslate nohighlight">\(\tau = 8\)</span> instead of <span class="math notranslate nohighlight">\(\tau = 9\)</span> for other solutions: this lower level of thresholding suggests that raw output of our tracking system is more reliable.</p>
<p>We report results using the count-related tracking metrics and count decompositions defined in the previous section. To provide a clear but thorough summary of the performance, we report <span class="math notranslate nohighlight">\(\assre\)</span>, <span class="math notranslate nohighlight">\(\countre\)</span> and <span class="math notranslate nohighlight">\(\countpr\)</span> as tabled values (the first gives a simple overview of the quality of the predicted tacks while the latter two concisely summarise the count performance). For a more detailed visualisation of the different types of errors, we plot the count error decomposition for all sequences in a separate graph. Note that across all videos and all methods, we find <span class="math notranslate nohighlight">\(\asspr\)</span> between 98.6 and 99.2 which shows that this application context is unconcerned with tracks spanning multiple ground truth objects, therefore we do not conduct a more detailed interpretation of <span class="math notranslate nohighlight">\(\asspr\)</span> values.</p>
<p>First, the higher values of AssRe confirm the robustness of our solution in assigning consistent tracks to individual objects. This is directly reflected into the count precision performance - with an overall value of <span class="math notranslate nohighlight">\(\countpr\)</span> 17.6 points higher than the next best method (SORT) - or even more so in the complete disappearance of orange (redundant) counts in the graph. A key aspect is that these improvements are not counteracted by a lower <span class="math notranslate nohighlight">\(\countre\)</span>: on the contrary, our tracker, which is more stable, also captures more object (albeit still missing most of them, with a <span class="math notranslate nohighlight">\(\countre\)</span> below 50%). Note finally, that the strongest improvements are obtained for sequence 2 which is also the part with the strongest motion.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_summary</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">index_start</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">index_stop</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>

    <span class="n">results</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,[</span><span class="sa">f</span><span class="s1">&#39;Correct_IDs</span><span class="si">{</span><span class="n">alpha_type</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span><span class="sa">f</span><span class="s1">&#39;Redundant_IDs</span><span class="si">{</span><span class="n">alpha_type</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span><span class="sa">f</span><span class="s1">&#39;False_IDs</span><span class="si">{</span><span class="n">alpha_type</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span><span class="sa">f</span><span class="s1">&#39;Missing_IDs</span><span class="si">{</span><span class="n">alpha_type</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span><span class="sa">f</span><span class="s1">&#39;Fused_IDs</span><span class="si">{</span><span class="n">alpha_type</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;GT_IDs&#39;</span><span class="p">,</span><span class="sa">f</span><span class="s1">&#39;HOTA_TP</span><span class="si">{</span><span class="n">alpha_type</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span><span class="sa">f</span><span class="s1">&#39;AssRe</span><span class="si">{</span><span class="n">alpha_type</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">index_start</span><span class="p">:</span><span class="n">index_stop</span><span class="p">]</span>

    <span class="n">results</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;correct&#39;</span><span class="p">,</span><span class="s1">&#39;redundant&#39;</span><span class="p">,</span><span class="s1">&#39;false&#39;</span><span class="p">,</span><span class="s1">&#39;missing&#39;</span><span class="p">,</span><span class="s1">&#39;mingled&#39;</span><span class="p">,</span><span class="s1">&#39;gt&#39;</span><span class="p">,</span><span class="s1">&#39;hota_tp&#39;</span><span class="p">,</span><span class="s1">&#39;ass_re&#39;</span><span class="p">]</span>

    <span class="n">ass_re</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="s1">&#39;ass_re&#39;</span><span class="p">]</span>
    <span class="n">hota_tp</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="s1">&#39;hota_tp&#39;</span><span class="p">]</span>


    <span class="n">ass_re_cb</span> <span class="o">=</span> <span class="p">(</span><span class="n">ass_re</span> <span class="o">*</span> <span class="n">hota_tp</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">hota_tp</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

    <span class="n">correct</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="s1">&#39;correct&#39;</span><span class="p">]</span>
    <span class="n">redundant</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="s1">&#39;redundant&#39;</span><span class="p">]</span>
    <span class="n">false</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="s1">&#39;false&#39;</span><span class="p">]</span>
    <span class="n">missing</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="s1">&#39;missing&#39;</span><span class="p">]</span>
    <span class="c1"># mingled = results[&#39;mingled&#39;] </span>
    <span class="c1"># gt = results[&#39;gt&#39;]</span>
    <span class="c1"># count_error = false + redundant - missing</span>


    
    <span class="n">summary</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
    <span class="c1"># summary[&#39;missing&#39;], summary[&#39;missing_mean&#39;], summary[&#39;missing_std&#39;] = f&#39;{missing.sum()}&#39;,f&#39;{missing.mean():.1f}&#39;,f&#39;{np.nan_to_num(missing.std()):.1f}&#39;</span>
    <span class="c1"># summary[&#39;false&#39;], summary[&#39;false_mean&#39;], summary[&#39;false_std&#39;] = f&#39;{false.sum()}&#39;, f&#39;{false.mean():.1f}&#39;, f&#39;{np.nan_to_num(false.std()):.1f}&#39;</span>
    <span class="c1"># summary[&#39;redundant&#39;], summary[&#39;redundant_mean&#39;], summary[&#39;redundant_std&#39;] = f&#39;{redundant.sum()}&#39;, f&#39;{redundant.mean():.1f}&#39;, f&#39;{np.nan_to_num(redundant.std()):.1f}&#39;</span>
    <span class="c1"># summary[&#39;gt&#39;] = f&#39;{gt.sum()}&#39;</span>
    <span class="n">summary</span><span class="p">[</span><span class="s1">&#39;ass_re_cb&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">ass_re_cb</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">&#39;</span>

    <span class="c1"># summary[&#39;count_error&#39;], summary[&#39;count_error_mean&#39;], summary[&#39;count_error_std&#39;] = f&#39;{count_error.sum()}&#39;,f&#39;{count_error.mean():.1f}&#39;,f&#39;{np.nan_to_num(count_error.std()):.1f}&#39;</span>
    
    <span class="n">count_re</span> <span class="o">=</span> <span class="n">correct</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="n">correct</span> <span class="o">+</span> <span class="n">missing</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="n">summary</span><span class="p">[</span><span class="s1">&#39;count_re_cb&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">count_re</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">&#39;</span>
    
    <span class="c1"># count_re_mean = (correct / (correct + missing)).mean()</span>
    <span class="c1"># summary[&#39;count_re_mean&#39;] = f&#39;{100*count_re_mean:.1f}&#39;</span>
    
    <span class="n">count_re_std</span> <span class="o">=</span> <span class="p">(</span><span class="n">correct</span> <span class="o">/</span> <span class="p">(</span><span class="n">correct</span> <span class="o">+</span> <span class="n">missing</span><span class="p">))</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
    <span class="n">summary</span><span class="p">[</span><span class="s1">&#39;count_re_std&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">count_re_std</span><span class="p">)</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">&#39;</span>

    <span class="n">count_pr</span> <span class="o">=</span> <span class="n">correct</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="n">correct</span> <span class="o">+</span> <span class="n">false</span> <span class="o">+</span> <span class="n">redundant</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="n">summary</span><span class="p">[</span><span class="s1">&#39;count_pr_cb&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">count_pr</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">&#39;</span>
    
    <span class="c1"># count_pr_mean = (correct / (correct + false + redundant)).mean()</span>
    <span class="c1"># summary[&#39;count_pr_mean&#39;] = f&#39;{100*count_pr_mean:.1f}&#39;</span>
    
    <span class="n">count_pr_std</span> <span class="o">=</span> <span class="p">(</span><span class="n">correct</span> <span class="o">/</span> <span class="p">(</span><span class="n">correct</span> <span class="o">+</span> <span class="n">false</span> <span class="o">+</span> <span class="n">redundant</span><span class="p">))</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
    <span class="n">summary</span><span class="p">[</span><span class="s1">&#39;count_pr_std&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">nan_to_num</span><span class="p">(</span><span class="n">count_pr_std</span><span class="p">)</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">&#39;</span>

    <span class="k">return</span> <span class="n">summary</span> 





<span class="k">def</span> <span class="nf">get_summaries</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">sequence_names</span><span class="p">):</span>

    <span class="n">summaries</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>

    <span class="k">for</span> <span class="p">(</span><span class="n">sequence_name</span><span class="p">,</span> <span class="n">index_start</span><span class="p">,</span> <span class="n">index_stop</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">sequence_names</span><span class="p">,</span> <span class="n">indices</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">indices</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>

        <span class="n">summaries</span><span class="p">[</span><span class="n">sequence_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">get_summary</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">index_start</span><span class="p">,</span> <span class="n">index_stop</span><span class="p">)</span>
    
    <span class="n">summaries</span><span class="p">[</span><span class="s1">&#39;All&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">get_summary</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">summaries</span>




<span class="n">summaries</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">method_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;fairmot_kappa_7_tau_9&#39;</span><span class="p">,</span> <span class="s1">&#39;sort_kappa_7_tau_9&#39;</span><span class="p">,</span><span class="s1">&#39;ours_EKF_1_kappa_7_tau_8&#39;</span><span class="p">]</span>
<span class="n">pretty_method_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;FairMOT&#39;</span><span class="p">,</span><span class="s1">&#39;SORT&#39;</span><span class="p">,</span><span class="s1">&#39;Ours&#39;</span><span class="p">]</span>
<span class="k">for</span> <span class="n">tracker_name</span> <span class="ow">in</span> <span class="n">method_names</span><span class="p">:</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">eval_dir_short</span><span class="p">,</span><span class="s1">&#39;surfrider-test&#39;</span><span class="p">,</span><span class="n">tracker_name</span><span class="p">,</span><span class="s1">&#39;pedestrian_detailed.csv&#39;</span><span class="p">))</span>
    <span class="n">sequence_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;S1&#39;</span><span class="p">,</span><span class="s1">&#39;S2&#39;</span><span class="p">,</span><span class="s1">&#39;S3&#39;</span><span class="p">]</span>
    <span class="n">summaries</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">get_summaries</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">sequence_names</span><span class="p">))</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>


<span class="n">fairmot_star</span><span class="p">,</span> <span class="n">sort</span><span class="p">,</span> <span class="n">ours</span> <span class="o">=</span> <span class="n">summaries</span>

<span class="c1"># nmis = &#39;$\hat{\mathsf{N}}_{mis}$&#39;</span>
<span class="c1"># mu_nmis =&#39;$\hat{\mu}_{\hat{\mathsf{N}}_{mis}}$&#39;</span>
<span class="c1"># sigma_nmis = &#39;$\hat{\sigma}_{\hat{\mathsf{N}}_{mis}}$&#39;</span>

<span class="c1"># nfalse = &#39;$\hat{\mathsf{N}}_{false}$&#39;</span>
<span class="c1"># mu_nfalse = &#39;$\hat{\mu}_{\hat{\mathsf{N}}_{false}}$&#39;</span>
<span class="c1"># sigma_nfalse = &#39;$\hat{\sigma}_{\hat{\mathsf{N}}_{false}}$&#39;</span>

<span class="c1"># nred =  &#39;$\hat{\mathsf{N}}_{red}$&#39;</span>
<span class="c1"># mu_nred = &#39;$\hat{\mu}_{\hat{\mathsf{N}}_{red}}$&#39;</span>
<span class="c1"># sigma_nred = &#39;$\hat{\sigma}_{\hat{\mathsf{N}}_{red}}$&#39;</span>

<span class="n">ass_re</span> <span class="o">=</span> <span class="s1">&#39;$\mathsf</span><span class="si">{AssRe}</span><span class="s1">$&#39;</span>
<span class="n">count_re</span> <span class="o">=</span> <span class="s1">&#39;$\mathsf</span><span class="si">{CountRe}</span><span class="s1">$&#39;</span>
<span class="n">count_pr</span> <span class="o">=</span> <span class="s1">&#39;$\mathsf</span><span class="si">{CountPr}</span><span class="s1">$&#39;</span>

<span class="n">rows</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;fairmot_kappa_7_tau_9&#39;</span><span class="p">,</span><span class="s1">&#39;sort_kappa_7_tau_9&#39;</span><span class="p">,</span><span class="s1">&#39;ours_EKF_1_kappa_7_tau_8&#39;</span><span class="p">]</span>
<span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;ass_re&#39;</span><span class="p">,</span><span class="s1">&#39;count_re&#39;</span><span class="p">,</span> <span class="s1">&#39;count_re_std&#39;</span><span class="p">,</span> <span class="s1">&#39;count_pr&#39;</span><span class="p">,</span> <span class="s1">&#39;count_pr_std&#39;</span><span class="p">]</span>

<span class="k">for</span> <span class="n">summary</span> <span class="ow">in</span> <span class="n">summaries</span><span class="p">:</span>    
    <span class="n">summary</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="n">columns</span>

<span class="n">results_S1</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="n">summary</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">summary</span> <span class="ow">in</span> <span class="n">summaries</span><span class="p">])</span>
<span class="n">results_S1</span><span class="o">.</span><span class="n">index</span> <span class="o">=</span> <span class="n">method_names</span>

<span class="n">results_S2</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="n">summary</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">summary</span> <span class="ow">in</span> <span class="n">summaries</span><span class="p">])</span>
<span class="n">results_S2</span><span class="o">.</span><span class="n">index</span> <span class="o">=</span> <span class="n">method_names</span>

<span class="n">results_S3</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="n">summary</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="k">for</span> <span class="n">summary</span> <span class="ow">in</span> <span class="n">summaries</span><span class="p">])</span>
<span class="n">results_S3</span><span class="o">.</span><span class="n">index</span> <span class="o">=</span> <span class="n">method_names</span>

<span class="n">results_All</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="n">summary</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="k">for</span> <span class="n">summary</span> <span class="ow">in</span> <span class="n">summaries</span><span class="p">])</span>
<span class="n">results_All</span><span class="o">.</span><span class="n">index</span> <span class="o">=</span> <span class="n">method_names</span>

<span class="n">results</span> <span class="o">=</span> <span class="p">[</span><span class="n">results_S1</span><span class="p">,</span> <span class="n">results_S2</span><span class="p">,</span> <span class="n">results_S3</span><span class="p">,</span> <span class="n">results_All</span><span class="p">]</span>

<span class="n">results_S1</span><span class="o">.</span><span class="n">to_latex</span><span class="p">()</span>

<span class="k">for</span> <span class="n">sequence_name</span><span class="p">,</span> <span class="n">result</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([</span><span class="s1">&#39;S_1&#39;</span><span class="p">,</span><span class="s1">&#39;S_2&#39;</span><span class="p">,</span><span class="s1">&#39;S_3&#39;</span><span class="p">,</span><span class="s1">&#39;All&#39;</span><span class="p">],</span> <span class="n">results</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">rows</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">columns</span><span class="p">:</span> 
            <span class="n">glue</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">sequence_name</span><span class="si">}</span><span class="s1">_</span><span class="si">{</span><span class="n">row</span><span class="si">}</span><span class="s1">_</span><span class="si">{</span><span class="n">col</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">result</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">row</span><span class="p">,</span><span class="n">col</span><span class="p">],</span> <span class="n">display</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
</div>
<section id="sequence-1">
<h4>Sequence 1<a class="headerlink" href="#sequence-1" title="Permalink to this heading">#</a></h4>
<table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p></p></th>
<th class="head text-right"><p><span class="math notranslate nohighlight">\(\mathsf{AssRe}\)</span></p></th>
<th class="head text-right"><p><span class="math notranslate nohighlight">\(\mathsf{CountRe}\)</span></p></th>
<th class="head text-right"><p><span class="math notranslate nohighlight">\(\hat{\sigma}_\mathsf{CountRe}\)</span></p></th>
<th class="head text-right"><p><span class="math notranslate nohighlight">\(\mathsf{CountPr}\)</span></p></th>
<th class="head text-right"><p><span class="math notranslate nohighlight">\(\hat{\sigma}_\mathsf{CountPr}\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>FairMOT</p></td>
<td class="text-right"><p><span class="pasted-text">62.0</span></p></td>
<td class="text-right"><p><span class="pasted-text">31.2</span></p></td>
<td class="text-right"><p><span class="pasted-text">25.6</span></p></td>
<td class="text-right"><p><span class="pasted-text">52.6</span></p></td>
<td class="text-right"><p><span class="pasted-text">24.6</span></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Sort</p></td>
<td class="text-right"><p><span class="pasted-text">65.6</span></p></td>
<td class="text-right"><p><span class="pasted-text">43.8</span></p></td>
<td class="text-right"><p><span class="pasted-text">26.4</span></p></td>
<td class="text-right"><p><span class="pasted-text">53.8</span></p></td>
<td class="text-right"><p><span class="pasted-text">20.2</span></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Ours</p></td>
<td class="text-right"><p><span class="pasted-text">79.5</span></p></td>
<td class="text-right"><p><span class="pasted-text">50.0</span></p></td>
<td class="text-right"><p><span class="pasted-text">27.9</span></p></td>
<td class="text-right"><p><span class="pasted-text">64.0</span></p></td>
<td class="text-right"><p><span class="pasted-text">23.8</span></p></td>
</tr>
</tbody>
</table>
</section>
<section id="sequence-2">
<h4>Sequence 2<a class="headerlink" href="#sequence-2" title="Permalink to this heading">#</a></h4>
<table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p></p></th>
<th class="head text-right"><p><span class="math notranslate nohighlight">\(\mathsf{AssRe}\)</span></p></th>
<th class="head text-right"><p><span class="math notranslate nohighlight">\(\mathsf{CountRe}\)</span></p></th>
<th class="head text-right"><p><span class="math notranslate nohighlight">\(\hat{\sigma}_\mathsf{CountRe}\)</span></p></th>
<th class="head text-right"><p><span class="math notranslate nohighlight">\(\mathsf{CountPr}\)</span></p></th>
<th class="head text-right"><p><span class="math notranslate nohighlight">\(\hat{\sigma}_\mathsf{CountPr}\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>FairMOT</p></td>
<td class="text-right"><p><span class="pasted-text">8.7</span></p></td>
<td class="text-right"><p><span class="pasted-text">12.5</span></p></td>
<td class="text-right"><p><span class="pasted-text">35.4</span></p></td>
<td class="text-right"><p><span class="pasted-text">50.0</span></p></td>
<td class="text-right"><p><span class="pasted-text">0.0</span></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Sort</p></td>
<td class="text-right"><p><span class="pasted-text">20.7</span></p></td>
<td class="text-right"><p><span class="pasted-text">12.5</span></p></td>
<td class="text-right"><p><span class="pasted-text">35.4</span></p></td>
<td class="text-right"><p><span class="pasted-text">33.3</span></p></td>
<td class="text-right"><p><span class="pasted-text">0.0</span></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Ours</p></td>
<td class="text-right"><p><span class="pasted-text">72.7</span></p></td>
<td class="text-right"><p><span class="pasted-text">50.0</span></p></td>
<td class="text-right"><p><span class="pasted-text">0.0</span></p></td>
<td class="text-right"><p><span class="pasted-text">100.0</span></p></td>
<td class="text-right"><p><span class="pasted-text">0.0</span></p></td>
</tr>
</tbody>
</table>
</section>
<section id="sequence-3">
<h4>Sequence 3<a class="headerlink" href="#sequence-3" title="Permalink to this heading">#</a></h4>
<table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p></p></th>
<th class="head text-right"><p><span class="math notranslate nohighlight">\(\mathsf{AssRe}\)</span></p></th>
<th class="head text-right"><p><span class="math notranslate nohighlight">\(\mathsf{CountRe}\)</span></p></th>
<th class="head text-right"><p><span class="math notranslate nohighlight">\(\hat{\sigma}_\mathsf{CountRe}\)</span></p></th>
<th class="head text-right"><p><span class="math notranslate nohighlight">\(\mathsf{CountPr}\)</span></p></th>
<th class="head text-right"><p><span class="math notranslate nohighlight">\(\hat{\sigma}_\mathsf{CountPr}\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>FairMOT</p></td>
<td class="text-right"><p><span class="pasted-text">17.4</span></p></td>
<td class="text-right"><p><span class="pasted-text">25.0</span></p></td>
<td class="text-right"><p><span class="pasted-text">47.1</span></p></td>
<td class="text-right"><p><span class="pasted-text">50.0</span></p></td>
<td class="text-right"><p><span class="pasted-text">50.0</span></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Sort</p></td>
<td class="text-right"><p><span class="pasted-text">19.6</span></p></td>
<td class="text-right"><p><span class="pasted-text">25.0</span></p></td>
<td class="text-right"><p><span class="pasted-text">47.1</span></p></td>
<td class="text-right"><p><span class="pasted-text">40.0</span></p></td>
<td class="text-right"><p><span class="pasted-text">50.9</span></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Ours</p></td>
<td class="text-right"><p><span class="pasted-text">24.6</span></p></td>
<td class="text-right"><p><span class="pasted-text">37.5</span></p></td>
<td class="text-right"><p><span class="pasted-text">41.7</span></p></td>
<td class="text-right"><p><span class="pasted-text">60.0</span></p></td>
<td class="text-right"><p><span class="pasted-text">47.9</span></p></td>
</tr>
</tbody>
</table>
</section>
<section id="combined-sequences">
<h4>Combined sequences<a class="headerlink" href="#combined-sequences" title="Permalink to this heading">#</a></h4>
<table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p></p></th>
<th class="head text-right"><p><span class="math notranslate nohighlight">\(\mathsf{AssRe}\)</span></p></th>
<th class="head text-right"><p><span class="math notranslate nohighlight">\(\mathsf{CountRe}\)</span></p></th>
<th class="head text-right"><p><span class="math notranslate nohighlight">\(\hat{\sigma}_\mathsf{CountRe}\)</span></p></th>
<th class="head text-right"><p><span class="math notranslate nohighlight">\(\mathsf{CountPr}\)</span></p></th>
<th class="head text-right"><p><span class="math notranslate nohighlight">\(\hat{\sigma}_\mathsf{CountPr}\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>FairMOT</p></td>
<td class="text-right"><p><span class="pasted-text">56.7</span></p></td>
<td class="text-right"><p><span class="pasted-text">27.1</span></p></td>
<td class="text-right"><p><span class="pasted-text">31.6</span></p></td>
<td class="text-right"><p><span class="pasted-text">52.0</span></p></td>
<td class="text-right"><p><span class="pasted-text">30.2</span></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Sort</p></td>
<td class="text-right"><p><span class="pasted-text">59.8</span></p></td>
<td class="text-right"><p><span class="pasted-text">35.4</span></p></td>
<td class="text-right"><p><span class="pasted-text">32.7</span></p></td>
<td class="text-right"><p><span class="pasted-text">50.0</span></p></td>
<td class="text-right"><p><span class="pasted-text">30.2</span></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Ours</p></td>
<td class="text-right"><p><span class="pasted-text">76.0</span></p></td>
<td class="text-right"><p><span class="pasted-text">47.9</span></p></td>
<td class="text-right"><p><span class="pasted-text">28.8</span></p></td>
<td class="text-right"><p><span class="pasted-text">67.6</span></p></td>
<td class="text-right"><p><span class="pasted-text">32.0</span></p></td>
</tr>
</tbody>
</table>
</section>
<section id="detailed-results-on-individual-segments">
<h4>Detailed results on individual segments<a class="headerlink" href="#detailed-results-on-individual-segments" title="Permalink to this heading">#</a></h4>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">set_split</span><span class="p">(</span><span class="s1">&#39;test&#39;</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span><span class="mi">10</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">title</span><span class="p">,</span> <span class="n">tracker_name</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">pretty_method_names</span><span class="p">,</span>  <span class="n">method_names</span><span class="p">):</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">eval_dir_short</span><span class="p">,</span><span class="s1">&#39;surfrider-test&#39;</span><span class="p">,</span><span class="n">tracker_name</span><span class="p">,</span><span class="s1">&#39;pedestrian_detailed.csv&#39;</span><span class="p">))</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,[</span><span class="sa">f</span><span class="s1">&#39;Redundant_IDs</span><span class="si">{</span><span class="n">alpha_type</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span><span class="sa">f</span><span class="s1">&#39;False_IDs</span><span class="si">{</span><span class="n">alpha_type</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span><span class="sa">f</span><span class="s1">&#39;Missing_IDs</span><span class="si">{</span><span class="n">alpha_type</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">results</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;redundant&#39;</span><span class="p">,</span> <span class="s1">&#39;false&#39;</span><span class="p">,</span> <span class="s1">&#39;missing&#39;</span><span class="p">]</span>
    <span class="n">results</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span><span class="s1">&#39;missing&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span> <span class="n">results</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span><span class="s1">&#39;missing&#39;</span><span class="p">]</span>
    <span class="n">results</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;$\hat{\mathsf</span><span class="si">{N}</span><span class="s1">}_</span><span class="si">{red}</span><span class="s1">$&#39;</span><span class="p">,</span> <span class="s1">&#39;$\hat{\mathsf</span><span class="si">{N}</span><span class="s1">}_</span><span class="si">{false}</span><span class="s1">$&#39;</span><span class="p">,</span> <span class="s1">&#39;$-\hat{\mathsf</span><span class="si">{N}</span><span class="s1">}_</span><span class="si">{mis}</span><span class="s1">$&#39;</span><span class="p">]</span>
    <span class="n">results</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s1">&#39;bar&#39;</span><span class="p">,</span> <span class="n">stacked</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="s1">&#39;black&#39;</span><span class="p">],</span> <span class="n">title</span><span class="o">=</span><span class="n">title</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;Sequence nb&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="_images/eaf992ec1ecb3e5fcbbf11ecc1e0a36098bc648c2bb0a3023973cb21091e7d25.png" src="_images/eaf992ec1ecb3e5fcbbf11ecc1e0a36098bc648c2bb0a3023973cb21091e7d25.png" />
</div>
</div>
<!-- ```{glue:} fairmot_star_tracking_results
``` --></section>
</section>
</section>
<section id="practical-impact-and-future-goals">
<h2>Practical impact and future goals<a class="headerlink" href="#practical-impact-and-future-goals" title="Permalink to this heading">#</a></h2>
<p>We successfully tackled video object counting on river banks, in particular issues which could be addressed independently of detection quality.
Moreover the methodology developed to assess count quality enables us to precisely highlight the challenges that pertain to video object counting on river banks.
Conducted in coordination with Surfrider Foundation Europe, an NGO specialized on water preservation, our work marks an important milestone in a broader campaign for macrolitter monitoring and is already being used in a production version of a monitoring system.
That said, large amounts of litter items are still not detected.
Solving this problem is largely a question of augmenting the object detector training dataset through crowdsourced images.
A <a class="reference external" href="https://www.trashroulette.com">specific annotation platform</a> is online, thus the amount of annotated images is expected to continuously increase, while training is provided to volunteers collecting data on the field to ensure data quality.
Finally, several expeditions on different rivers are already underway and new video footage is expected to be annotated in the near future for better evaluation.
All data is made freely available.
Future goals include downsizing the algorithm, a possibility given the architectural simplicity of anchor-free detection and the relatively low computational complexity of EKF.
In a citizen science perspective, a fully embedded version for portable devices will allow a larger deployment.
The resulting field data will help better understand litter origin, allowing to model and predict litter density in non surveyed areas.
Correlations between macro litter density and environmental parameters will be studied (e.g., population density, catchment size, land use and hydromorphology).
Finally, our work naturally benefits any extension of macrolitter monitoring in other areas (urban, coastal, etc) that may rely on a similar setup of moving cameras.</p>
</section>
<section id="supplements">
<h2>Supplements<a class="headerlink" href="#supplements" title="Permalink to this heading">#</a></h2>
<section id="details-on-the-image-dataset">
<span id="image-dataset-appendix"></span><h3>Details on the image dataset<a class="headerlink" href="#details-on-the-image-dataset" title="Permalink to this heading">#</a></h3>
<section id="categories">
<h4>Categories<a class="headerlink" href="#categories" title="Permalink to this heading">#</a></h4>
<p>In this work, we do not seek to precisely predict the proportions of the different types of counted litter.
However, we build our dataset to allow classification tasks.
Though litter classifications built by experts already exist, most are based on semantic rather than visual features and do not particularly consider the problem of class imbalance, which makes statistical learning more delicate.
In conjunction with water pollution experts, we therefore define a custom macrolitter taxonomy which balances annotation ease and pragmatic decisions for computer vision applications.
This classification, depicted in <a class="reference internal" href="#trash-categories-image"><span class="std std-numref">Fig. 4</span></a> can be understood as follows.</p>
<ol class="arabic simple">
<li><p>We define a set of frequently observed classes that annotateors can choose from, divided into:</p>
<ul class="simple">
<li><p>Classes for rigid and easily recognisable items which are often observed and have definite shapes</p></li>
<li><p>Classes for fragmented objects which are often found along river banks but whose aspects are more varied</p></li>
</ul>
</li>
<li><p>We define two supplementary categories used whenever the annotater cannot classify the item they are observing in an image using classes given in 1.</p>
<ul class="simple">
<li><p>A first category is used whenever the item is clearly identifiable but its class is not proposed.
This will ensure that our classification can be improved in the future, as images with items in this category will be checked regularly to decide whether a new class needs to be created.</p></li>
<li><p>Another category is used whenever the annotater does not understand the item they are seeing.
Images containing items denoted as such will not be used for applications involving classification.</p></li>
</ul>
</li>
</ol>
<figure class="align-default" id="trash-categories-image">
<a class="reference internal image-reference" href="_images/trash_categories.png"><img alt="_images/trash_categories.png" src="_images/trash_categories.png" style="height: 400px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4 </span><span class="caption-text">Trash categories defined to facilitate porting to a counting system that allows trash identification</span><a class="headerlink" href="#trash-categories-image" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="details-on-the-evaluation-videos">
<span id="video-dataset-appendix"></span><h3>Details on the evaluation videos<a class="headerlink" href="#details-on-the-evaluation-videos" title="Permalink to this heading">#</a></h3>
<section id="river-segments">
<h4>River segments<a class="headerlink" href="#river-segments" title="Permalink to this heading">#</a></h4>
<p>In this section, we provide further details on the evaluation material.
<a class="reference internal" href="#river-sections"><span class="std std-numref">Fig. 5</span></a> shows the setup and positioning of the three river segments <span class="math notranslate nohighlight">\(S_1\)</span>, <span class="math notranslate nohighlight">\(S_2\)</span> and <span class="math notranslate nohighlight">\(S_3\)</span> used to evaluate the methods.
The segments differ in the following aspects.</p>
<ul class="simple">
<li><p>Segment 1: Medium current, high and dense vegetation not obstructing vision of the right riverbank from watercrafts, extra objects installed before the field experiment.</p></li>
<li><p>Segment 2: High current, low and dense vegetation obstructing vision of the right riverbank from watercrafts.</p></li>
<li><p>Segment 3: Medium current, high and little vegetation not obstructing vision  of the left riverbank from watercrafts.</p></li>
</ul>
<figure class="align-default" id="river-sections">
<a class="reference internal image-reference" href="_images/river_sections.png"><img alt="_images/river_sections.png" src="_images/river_sections.png" style="height: 400px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 5 </span><span class="caption-text">Aerial view of the three river segments of the evaluation material</span><a class="headerlink" href="#river-sections" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="track-annotation-protocol">
<h4>Track annotation protocol<a class="headerlink" href="#track-annotation-protocol" title="Permalink to this heading">#</a></h4>
<p>To annotate tracks on the evaluation sequences, we used the online tool “CVAT” which allows to locate bounding boxes on video frames and propagate them in time.
The following items provide further details on the exact annotation process.</p>
<ul class="simple">
<li><p>Object tracks start whenever a litter item becomes fully visible and identifiable by the naked eye.</p></li>
<li><p>Positions and sizes of objects are given at nearly every second of the video with automatic interpolation for frames in-between: this yields clean tracks with precise positions at 24fps.</p></li>
<li><p>We do not provide inferred locations when an object is fully occluded, but tracks restart with the same identity whenever the object becomes visible again.</p></li>
<li><p>Tracks stop whenever an object becomes indistinguishable and will not reappear again.</p></li>
</ul>
</section>
</section>
<section id="implementation-details-for-the-tracking-module">
<span id="tracking-module-appendix"></span><h3>Implementation details for the tracking module<a class="headerlink" href="#implementation-details-for-the-tracking-module" title="Permalink to this heading">#</a></h3>
<section id="covariance-matrices-for-state-and-observation-noises">
<span id="covariance-matrices"></span><h4>Covariance matrices for state and observation noises<a class="headerlink" href="#covariance-matrices-for-state-and-observation-noises" title="Permalink to this heading">#</a></h4>
<p>In our state space model, <span class="math notranslate nohighlight">\(Q\)</span> models the noise associated with the movement model we posit in <a class="reference internal" href="#bayesian-tracking"><span class="std std-ref">Bayesian tracking with optical flow</span></a> involving optical flow estimates, while <span class="math notranslate nohighlight">\(R\)</span> models the noise associated with the observation of the true position via our object detector.
An attempt to estimate the diagonal values of these matrices was the following.</p>
<ul class="simple">
<li><p>To estimate <span class="math notranslate nohighlight">\(R\)</span>, we computed a mean <span class="math notranslate nohighlight">\(L_2\)</span> error between the known positions of objects and the associated predictions by the object detector, for images in our training dataset.</p></li>
<li><p>To estimate <span class="math notranslate nohighlight">\(Q\)</span>, we built a small synthetic dataset of consecutive frames taken from videos, where positions of objects in two consecutive frames are known.
We computed a mean <span class="math notranslate nohighlight">\(L_2\)</span> error between the known positions in the second frame and the positions estimated by shifting the positions in the first frame with the estimated optical flow values.</p></li>
</ul>
<p>This led to <span class="math notranslate nohighlight">\(R_{00} = R_{11} = 1.1\)</span>, <span class="math notranslate nohighlight">\(Q_{00} = 4.7\)</span> and <span class="math notranslate nohighlight">\(Q_{11} = 0.9\)</span>, for grids of dimensions <span class="math notranslate nohighlight">\(\lfloor w/p\rfloor \times \lfloor h/p\rfloor = 480 \times 270\)</span>.
All other coefficients were not estimated and supposed to be 0.</p>
<p>An important remark is that though we use these values in practice, we found that tracking results are largely unaffected by small variations of <span class="math notranslate nohighlight">\(R\)</span> and <span class="math notranslate nohighlight">\(Q\)</span>.
As long as values are meaningful relative to the image dimensions and the size of the objects, most noise levels show relatively similar performance.</p>
</section>
<section id="influence-of-tau-and-kappa">
<span id="tau-kappa-appendix"></span><h4>Influence of <span class="math notranslate nohighlight">\(\tau\)</span> and <span class="math notranslate nohighlight">\(\kappa\)</span><a class="headerlink" href="#influence-of-tau-and-kappa" title="Permalink to this heading">#</a></h4>
<p>An understanding of <span class="math notranslate nohighlight">\(\kappa\)</span>, <span class="math notranslate nohighlight">\(\tau\)</span> and <span class="math notranslate nohighlight">\(\nu\)</span> can be stated as follows.
For any track, given a value for <span class="math notranslate nohighlight">\(\kappa\)</span> and <span class="math notranslate nohighlight">\(\nu\)</span>, an observation at time <span class="math notranslate nohighlight">\(n\)</span> is only kept if there are also <span class="math notranslate nohighlight">\(\nu \cdot \kappa\)</span> observations in the temporal window of size <span class="math notranslate nohighlight">\(\kappa\)</span> that surrounds <span class="math notranslate nohighlight">\(n\)</span> (windows are centered around <span class="math notranslate nohighlight">\(n\)</span> except at the start and end of the track).
The track is only counted if the remaining number of observations is strictly higher than <span class="math notranslate nohighlight">\(\tau\)</span>.
At a given <span class="math notranslate nohighlight">\(\nu &gt; 0.5\)</span>, <span class="math notranslate nohighlight">\(\kappa\)</span> and <span class="math notranslate nohighlight">\(\tau\)</span> should ideally be chosen to jointly decrease <span class="math notranslate nohighlight">\(\Nfalse\)</span> and <span class="math notranslate nohighlight">\(\Nred\)</span> as much as possible without increasing <span class="math notranslate nohighlight">\(\Nmis\)</span> (true objects become uncounted if tracks are discarded too easily).</p>
<p>In the following code cell, we plot the error decomposition of the counts for several values of <span class="math notranslate nohighlight">\(\kappa\)</span> and <span class="math notranslate nohighlight">\(\tau\)</span> with <span class="math notranslate nohighlight">\(\nu=0.6\)</span> for the outputs of the three different trackers. We choose <span class="math notranslate nohighlight">\(\nu = 0.7\)</span> and compute the optimal point as the one which minimizes the overall count error <span class="math notranslate nohighlight">\(\hatN (= \Nmis + \Nred + \Nfalse)\)</span>.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">set_split</span><span class="p">(</span><span class="s1">&#39;val&#39;</span><span class="p">)</span>

<span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;legend.fontsize&#39;</span><span class="p">:</span> <span class="s1">&#39;x-large&#39;</span><span class="p">,</span>
         <span class="s1">&#39;axes.labelsize&#39;</span><span class="p">:</span> <span class="s1">&#39;x-large&#39;</span><span class="p">,</span>
         <span class="s1">&#39;axes.titlesize&#39;</span><span class="p">:</span><span class="s1">&#39;x-large&#39;</span><span class="p">,</span>
         <span class="s1">&#39;xtick.labelsize&#39;</span><span class="p">:</span><span class="s1">&#39;x-large&#39;</span><span class="p">,</span>
         <span class="s1">&#39;ytick.labelsize&#39;</span><span class="p">:</span><span class="s1">&#39;x-large&#39;</span><span class="p">}</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">hyperparameters</span><span class="p">(</span><span class="n">method_name</span><span class="p">,</span> <span class="n">pretty_method_name</span><span class="p">):</span>
    <span class="n">tau_values</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">)]</span>
    <span class="n">kappa_values</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">7</span><span class="p">]</span>
    <span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax0</span><span class="p">,</span> <span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">pretty_names</span><span class="o">=</span><span class="p">[</span><span class="sa">f</span><span class="s1">&#39;$\kappa=</span><span class="si">{</span><span class="n">kappa</span><span class="si">}</span><span class="s1">$&#39;</span> <span class="k">for</span> <span class="n">kappa</span> <span class="ow">in</span> <span class="n">kappa_values</span><span class="p">]</span>
    <span class="n">n_count</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">kappa</span><span class="p">,</span> <span class="n">pretty_name</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">kappa_values</span><span class="p">,</span> <span class="n">pretty_names</span><span class="p">):</span>
        <span class="n">tracker_names</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">method_name</span><span class="si">}</span><span class="s1">_kappa_</span><span class="si">{</span><span class="n">kappa</span><span class="si">}</span><span class="s1">_tau_</span><span class="si">{</span><span class="n">tau</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">tau</span> <span class="ow">in</span> <span class="n">tau_values</span><span class="p">]</span>
        <span class="n">all_results</span> <span class="o">=</span> <span class="p">{</span><span class="n">tracker_name</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">eval_dir_short</span><span class="p">,</span><span class="s1">&#39;surfrider-test&#39;</span><span class="p">,</span><span class="n">tracker_name</span><span class="p">,</span><span class="s1">&#39;pedestrian_detailed.csv&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">tracker_name</span> <span class="ow">in</span> <span class="n">tracker_names</span><span class="p">}</span>

        <span class="n">n_missing</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">n_false</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">n_redundant</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">tracker_name</span><span class="p">,</span> <span class="n">tracker_results</span> <span class="ow">in</span> <span class="n">all_results</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">missing</span> <span class="o">=</span> <span class="p">(</span><span class="n">tracker_results</span><span class="p">[</span><span class="s1">&#39;GT_IDs&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">-</span> <span class="n">tracker_results</span><span class="p">[</span><span class="s1">&#39;Correct_IDs___50&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
            <span class="n">false</span> <span class="o">=</span> <span class="n">tracker_results</span><span class="p">[</span><span class="s1">&#39;False_IDs___50&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
            <span class="n">redundant</span> <span class="o">=</span> <span class="n">tracker_results</span><span class="p">[</span><span class="s1">&#39;Redundant_IDs___50&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

            <span class="n">n_missing</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">missing</span><span class="p">)</span>
            <span class="n">n_false</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">false</span><span class="p">)</span>
            <span class="n">n_redundant</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">redundant</span><span class="p">)</span>
            <span class="n">n_count</span><span class="p">[</span><span class="n">tracker_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">missing</span> <span class="o">+</span> <span class="n">false</span> <span class="o">+</span> <span class="n">redundant</span>
        



        <span class="n">ax0</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">tau_values</span><span class="p">,</span> <span class="n">n_missing</span><span class="p">)</span>
        <span class="n">ax0</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">tau_values</span><span class="p">,</span> <span class="n">n_missing</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">pretty_name</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">)</span>
        <span class="c1"># ax0.set_xlabel(&#39;$\\tau$&#39;)</span>
        <span class="n">ax0</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$N_</span><span class="si">{mis}</span><span class="s1">$&#39;</span><span class="p">)</span>

        <span class="n">ax1</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">tau_values</span><span class="p">,</span> <span class="n">n_false</span><span class="p">)</span>
        <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">tau_values</span><span class="p">,</span> <span class="n">n_false</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">)</span>

        <span class="c1"># ax1.set_xlabel(&#39;$\\tau$&#39;)</span>
        <span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$N_</span><span class="si">{false}</span><span class="s1">$&#39;</span><span class="p">)</span>

        <span class="n">ax2</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">tau_values</span><span class="p">,</span> <span class="n">n_redundant</span><span class="p">)</span>
        <span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">tau_values</span><span class="p">,</span> <span class="n">n_redundant</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">)</span>
        <span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">tau$&#39;</span><span class="p">)</span>
        <span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$N_</span><span class="si">{red}</span><span class="s1">$&#39;</span><span class="p">)</span>


    <span class="n">best_value</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span>
    <span class="n">best_key</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>

    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">n_count</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">v</span> <span class="o">&lt;</span> <span class="n">best_value</span><span class="p">:</span> <span class="n">best_key</span> <span class="o">=</span> <span class="n">k</span>
        <span class="n">best_value</span> <span class="o">=</span> <span class="n">v</span>
    <span class="n">best_key</span> <span class="o">=</span> <span class="n">best_key</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;kappa&#39;</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">best_kappa</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">best_key</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;_&#39;</span><span class="p">)[</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">best_tau</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">best_key</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;_&#39;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Best parameters for </span><span class="si">{</span><span class="n">pretty_method_name</span><span class="si">}</span><span class="s1">: (kappa, tau) = (</span><span class="si">{</span><span class="n">best_kappa</span><span class="si">}</span><span class="s1">, </span><span class="si">{</span><span class="n">best_tau</span><span class="si">}</span><span class="s1">)&#39;</span><span class="p">)</span>
    <span class="n">handles</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">ax0</span><span class="o">.</span><span class="n">get_legend_handles_labels</span><span class="p">()</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">handles</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">autoscale</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

<span class="k">for</span> <span class="n">method_name</span><span class="p">,</span> <span class="n">pretty_method_name</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">method_names</span><span class="p">,</span> <span class="n">pretty_method_names</span><span class="p">):</span>
    <span class="n">hyperparameters</span><span class="p">(</span><span class="n">method_name</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;kappa&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">][:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">pretty_method_name</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Best parameters for FairMOT: (kappa, tau) = (7, 9)
</pre></div>
</div>
<img alt="_images/a59fa3cfe3ebf200fc87178b6710c34e608a2e609b08c54dd507a758f7f3a9a8.png" src="_images/a59fa3cfe3ebf200fc87178b6710c34e608a2e609b08c54dd507a758f7f3a9a8.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Best parameters for SORT: (kappa, tau) = (7, 9)
</pre></div>
</div>
<img alt="_images/7013b3e9fea2399a851b32175c3d5d2922fe8a4f44a205d18c6af2bbdd22c1af.png" src="_images/7013b3e9fea2399a851b32175c3d5d2922fe8a4f44a205d18c6af2bbdd22c1af.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Best parameters for Ours: (kappa, tau) = (7, 8)
</pre></div>
</div>
<img alt="_images/5b1ee5eca146244d29a59849149474bc9ecd821e6bfaff0e97a70a0a93efea96.png" src="_images/5b1ee5eca146244d29a59849149474bc9ecd821e6bfaff0e97a70a0a93efea96.png" />
</div>
</div>
</section>
</section>
<section id="bayesian-filtering">
<span id="id54"></span><h3>Bayesian filtering<a class="headerlink" href="#bayesian-filtering" title="Permalink to this heading">#</a></h3>
<p>Considering a state space model with <span class="math notranslate nohighlight">\((X_k, Z_k)_{k \geq 0}\)</span> the random processes for the states and observations, respectively, the filtering recursions are given by:</p>
<ul class="simple">
<li><p>The predict step: <span class="math notranslate nohighlight">\(p(x_{k+1}|z_{1:k}) = \int p(x_{k+1}|x_k)p(x_k|z_{1:k})\mathrm{d}x_k.\)</span></p></li>
<li><p>The update step: <span class="math notranslate nohighlight">\(p(x_{k+1}|z_{1:k+1}) \propto p(z_{k+1} | x_{k+1})p(x_{k+1}|z_{1:k}).\)</span></p></li>
</ul>
<p>The recursions are intractable in most cases, but when the model is linear and Gaussian, i.e. such that:</p>
<div class="math notranslate nohighlight">
\[X_{k} = A_kX_{k-1} + a_k + \eta_k\]</div>
<div class="math notranslate nohighlight">
\[Z_{k} = B_kX_{k} + b_k + \epsilon_k\]</div>
<p>with <span class="math notranslate nohighlight">\(\eta_k \sim \mathcal{N}(0,Q_k)\)</span> and <span class="math notranslate nohighlight">\(\epsilon_k \sim \mathcal{N}(0,R_k)\)</span>, then the distribution of <span class="math notranslate nohighlight">\(X_k\)</span> given <span class="math notranslate nohighlight">\(Z_{1:k}\)</span> is a Gaussian <span class="math notranslate nohighlight">\(\mathcal{N}(\mu_k,\Sigma_k)\)</span> following:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mu_{k|k-1} = A_k\mu_{k-1} + a_k\)</span> and <span class="math notranslate nohighlight">\(\Sigma_{k|k-1} = A_k \Sigma_{k-1} A_k^T + Q_k\)</span> (Kalman predict step),</p></li>
<li><p><span class="math notranslate nohighlight">\(\mu_{k} = \mu_{k|k-1} + K_k\left[Z_k - (B_k\mu_{k|k-1} + b_k)\right]\)</span> and <span class="math notranslate nohighlight">\(\Sigma_{k} = (I - K_kB_k)\Sigma_{k|k-1}\)</span> (Kalman update step),</p></li>
</ul>
<p>where <span class="math notranslate nohighlight">\(K_k = \Sigma_{k|k-1}B_k^T(B_k \Sigma_{k|k-1} B_k^T + R_k)^{-1}\)</span>.</p>
<p>In the case of the linearized model in <a class="reference internal" href="#state-space-model"><span class="std std-ref">State space model</span></a>, EKF consists in applying these updates with:</p>
<div class="math notranslate nohighlight">
\[A_k = (I + \partial_X\Delta_k(\lfloor \mu_{k-1} \rfloor),\]</div>
<div class="math notranslate nohighlight">
\[a_k = \Delta_k(\lfloor \mu_{k-1} \rfloor) - \partial_X\Delta_k(\lfloor \mu_{k-1} \rfloor)\mu_{k-1},\]</div>
<div class="math notranslate nohighlight">
\[Q_k = Q, R_k = R,\]</div>
<div class="math notranslate nohighlight">
\[B_k = I, b_k = 0.\]</div>
</section>
<section id="computing-the-confidence-regions">
<span id="confidence-regions-appendix"></span><h3>Computing the confidence regions<a class="headerlink" href="#computing-the-confidence-regions" title="Permalink to this heading">#</a></h3>
<p>In words, <span class="math notranslate nohighlight">\(P(i,\ell)\)</span> is the mass in <span class="math notranslate nohighlight">\(V_\delta(z_n^i) \subset \Rset^2\)</span> of the probability distribution of <span class="math notranslate nohighlight">\(Z_n^\ell\)</span> given <span class="math notranslate nohighlight">\(Z_{1:n-1}^\ell\)</span>. It is related to the filtering distribution at the previous timestep via</p>
<div class="math notranslate nohighlight">
\[
p(z_n | z_{1:n-1}) = \int \int p(z_n | x_n) p(x_n | x_{n-1}) p(x_{n-1} | z_{1:n-1}) \rmd x_{n} \rmd x_{n-1}
\]</div>
<p>When using EKF, this distribution is a multivariate Gaussian whose moments can be analytically obtained from the filtering mean and variance and the parameters of the linear model, i.e.</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E} \left[Z_n^\ell | Z_{1:n-1}^\ell \right] = B_k (A_k \mu_{k-1} + a_k) + b_k 
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
\mathbb{V} \left[Z_n^\ell | Z_{1:n-1}^\ell \right] = B_k (A_k \Sigma_k A_k^T + Q_k) B_k^T + R_k  
\]</div>
<p>following the previously introduced notation. Note that given the values of <span class="math notranslate nohighlight">\(A_k, B_k, a_k, b_k\)</span> in our model these equations are simplified in practice, e.g. <span class="math notranslate nohighlight">\(B_k = I, b_k = 0\)</span> and <span class="math notranslate nohighlight">\(A_k \mu_{k-1} + a_k = \mu_{k-1} + \Delta_k(\lfloor \mu_{k-1} \rfloor)\)</span>.</p>
<p>In <span class="math notranslate nohighlight">\(\Rset^2\)</span>, values of the cumulative distribution function (cdf) of a multivariate Gaussian distribution are easy to compute.
Denote with <span class="math notranslate nohighlight">\(F_n^\ell\)</span> the cdf of <span class="math notranslate nohighlight">\(\predictdist_n^\ell\)</span>.
If <span class="math notranslate nohighlight">\(V_\delta(z)\)</span> is a squared neighborhood of size <span class="math notranslate nohighlight">\(\delta\)</span> and centered on <span class="math notranslate nohighlight">\(z=(x,y) \in \Rset^2\)</span>, then, denoting with <span class="math notranslate nohighlight">\(\predictdist_n^\ell\)</span> the distribution of <span class="math notranslate nohighlight">\(Z_n^\ell\)</span> given <span class="math notranslate nohighlight">\(Z_{1:n-1}^\ell\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\predictdist_n^{\ell}(V_\delta(z)) = F_n^\ell(x+\delta,y+\delta) + F_n^\ell(x-\delta,y-\delta) - \left[F_n^\ell(x+\delta,y-\delta) + F_n^\ell(x-\delta,y+\delta)\right]
\]</div>
<p>This allows easy computation of <span class="math notranslate nohighlight">\(P(i,\ell) = \predictdist_n^\ell(V_\delta(z_n^i))\)</span>.</p>
</section>
<section id="impact-of-the-filtering-algorithm">
<span id="impact-algorithm-appendix"></span><h3>Impact of the filtering algorithm<a class="headerlink" href="#impact-of-the-filtering-algorithm" title="Permalink to this heading">#</a></h3>
<p>An advantage of the data association method proposed in <a class="reference internal" href="#data-association"><span class="std std-ref">Data association using confidence regions</span></a> is that it is very generic and does not constrain the tracking solution to any particular choice of filtering algorithm.
As for EKF, UKF implementations are already available to compute the distribution of <span class="math notranslate nohighlight">\(Z_k\)</span> given <span class="math notranslate nohighlight">\(Z_{1:k-1}\)</span> and the corresponding confidence regions (see <a class="reference internal" href="#tracking-module-appendix"><span class="std std-ref">the section</span></a> above).
We propose a solution to compute this distribution when SMC is used, and performance comparisons between the EKF, UKF and SMC versions of our trackers are discussed.</p>
<section id="smc-based-tracking">
<h4>SMC-based tracking<a class="headerlink" href="#smc-based-tracking" title="Permalink to this heading">#</a></h4>
<p>Denote <span class="math notranslate nohighlight">\(\filtdist_k\)</span> the filtering distribution (ie. that of <span class="math notranslate nohighlight">\(Z_k\)</span> given <span class="math notranslate nohighlight">\(X_{1:k}\)</span>) for the HMM <span class="math notranslate nohighlight">\((X_k,Z_k)_{k \geq 1}\)</span>  (omitting the dependency on the observations for notation ease).
Using a set of samples <span class="math notranslate nohighlight">\(\{X_k^i\}_{1 \leq i \leq N}\)</span> and importance weights <span class="math notranslate nohighlight">\(\{w_k^i\}_{1 \leq i \leq N}\)</span>, SMC methods build an approximation of the following form:</p>
<div class="math notranslate nohighlight">
\[
\SMCfiltdist_k(\rmd x_k) = \sum_{i=1}^N w_k^i \delta_{X_k^i}(\rmd x_k) \eqsp.
\]</div>
<p>Contrary to EKF and UKF, the distribution <span class="math notranslate nohighlight">\(\mathbb{L}_k\)</span> of <span class="math notranslate nohighlight">\(Z_k\)</span> given <span class="math notranslate nohighlight">\(Z_{1:k-1}\)</span> is not directly available but can be obtained via an additional Monte Carlo sampling step.
Marginalizing over <span class="math notranslate nohighlight">\((X_{k-1}\)</span>, <span class="math notranslate nohighlight">\(X_k)\)</span> and using the conditional independence properties of HMMs, we decompose <span class="math notranslate nohighlight">\(\mathbb{L}_k\)</span> using the conditional state transition <span class="math notranslate nohighlight">\(\transdist_k(x,\rmd x')\)</span> and the likelihood of <span class="math notranslate nohighlight">\(Z_k\)</span> given <span class="math notranslate nohighlight">\(X_k\)</span>, denoted by <span class="math notranslate nohighlight">\(\likel_k(x, \rmd z)\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{L}_k(\rmd z_k) = \int \int \likel_k(x_k, \rmd z_k)\transdist_k(x_{k-1}, \rmd x_k)\filtdist_{k-1}(\rmd x_{k-1}) \eqsp.
\]</div>
<p>Replacing <span class="math notranslate nohighlight">\(\filtdist_{k-1}\)</span> with <span class="math notranslate nohighlight">\(\SMCfiltdist_{k-1}\)</span> into the previous equation yields</p>
<div class="math notranslate nohighlight">
\[
\SMCpredictdist_k(\rmd z_k) = \sum_{k=1}^N w_k^i \int \likel_k(x_k,\rmd z_k) \transdist_k(X_{k-1}^i, \rmd x_k)  \eqsp.
\]</div>
<p>In our model, the state transition is Gaussian and therefore easy to sample from.
Thus an approximated predictive distribution <span class="math notranslate nohighlight">\(\MCpredictdist_k\)</span> can be obtained using Monte Carlo estimates built from random samples <span class="math notranslate nohighlight">\(\{X_k^{i,j}\}_{1 \leq i \leq N}^{1 \leq j \leq M}\)</span> drawn from <span class="math notranslate nohighlight">\(\transdist_k(X_{k-1}^i, \rmd x_k)\)</span>.
This leads to</p>
<div class="math notranslate nohighlight">
\[
\MCpredictdist_k(\rmd z_k) = \sum_{i=1}^N \sum_{j=1}^M w_k^i \likel_k(X_k^{i,j},\rmd z_k) \eqsp.
\]</div>
<p>Since the observation likelihood is also Gaussian, <span class="math notranslate nohighlight">\(\MCpredictdist_k\)</span> is a Gaussian mixture, thus values of <span class="math notranslate nohighlight">\(\MCpredictdist_k(\mathsf{A})\)</span> for any <span class="math notranslate nohighlight">\(\mathsf{A} \subset \Rset^2\)</span> can be computed by applying the tools from <a class="reference internal" href="#confidence-regions-appendix"><span class="std std-ref">the section above</span></a> to all mixture components.
Similar to EKF and UKF, this approximated predictive distribution is used to recover object identities via <span class="math notranslate nohighlight">\(\MCpredictdist_n^{\ell}(V_\delta(z_n^i))\)</span> computed for all incoming detections <span class="math notranslate nohighlight">\(\detectset_n = \{z_n^i\}_{1 \leq i \leq D_n}\)</span> and each of the <span class="math notranslate nohighlight">\(1 \leq \ell \leq L_n\)</span> filters, where <span class="math notranslate nohighlight">\(\MCpredictdist_n^{\ell}\)</span> is the predictive distribution associated with the <span class="math notranslate nohighlight">\(\ell\)</span>-th filter.</p>
</section>
<section id="performance-comparison">
<h4>Performance comparison<a class="headerlink" href="#performance-comparison" title="Permalink to this heading">#</a></h4>
<p>In theory, sampling-based methods like UKF and SMC are better suited for nonlinear state space models like the one we propose in <a class="reference internal" href="#state-space-model"><span class="std std-ref">State space model</span></a>.
However, we observe very few differences in count results when upgrading from EKF to UKF to SMC.
In practise, there is no difference at all between our EKF and UKF implementations, which show strictly identical values for <span class="math notranslate nohighlight">\(\Ntrue\)</span>, <span class="math notranslate nohighlight">\(\Nfalse\)</span> and <span class="math notranslate nohighlight">\(\Nred\)</span>.
For the SMC version, values for <span class="math notranslate nohighlight">\(\Nfalse\)</span> and <span class="math notranslate nohighlight">\(\Nred\)</span> improve by a very small amount (2 and 1, respectively), but <span class="math notranslate nohighlight">\(\Nmis\)</span> is slightly worse (one more object missed), and these results depend loosely on the number of samples used to approximate the filtering distributions and the number of samples for the Monte Carlo scheme.
Therefore, our motion estimates via the optical flow <span class="math notranslate nohighlight">\(\Delta_n\)</span> prove very reliable in our application context, so much that EKF, though suboptimal, brings equivalent results.
This comforts us into keeping it as a faster and computationally simpler option.
That said, this conclusion might not hold in scenarios where camera motion is even stronger, which was our main motivation to develop a flexible tracking solution and to provide implementations of UKF and SMC versions.
This allows easier extension of our work to more challenging data.</p>
<div class="docutils container" id="id55">
<div class="citation" id="id67" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>Roland Geyer, Jenna Jambeck, and Kara Law. Production, use, and fate of all plastics ever made. <em>Science Advances</em>, 3:e1700782, 07 2017. <a class="reference external" href="https://doi.org/10.1126/sciadv.1700782">doi:10.1126/sciadv.1700782</a>.</p>
</div>
<div class="citation" id="id65" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">2</a><span class="fn-bracket">]</span></span>
<p>Natalie Welden. The environmental impacts of plastic pollution. pages 195–222, 01 2020. <a class="reference external" href="https://doi.org/10.1016/B978-0-12-817880-5.00008-6">doi:10.1016/B978-0-12-817880-5.00008-6</a>.</p>
</div>
<div class="citation" id="id66" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">3</a><span class="fn-bracket">]</span></span>
<p>Thushari Gamage and J.D.M. Senevirathna. Plastic pollution in the marine environment. <em>Heliyon</em>, 6:e04709, 08 2020. <a class="reference external" href="https://doi.org/10.1016/j.heliyon.2020.e04709">doi:10.1016/j.heliyon.2020.e04709</a>.</p>
</div>
<div class="citation" id="id62" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">4</a><span class="fn-bracket">]</span></span>
<p>Chelsea Rochman, Anthony Andrady, Sarah Dudas, Joan Fabres, François Galgani, Denise lead, Valeria Hidalgo-Ruz, Sunny Hong, Peter Kershaw, Laurent Lebreton, Amy Lusher, Ramani Narayan, Sabine Pahl, James Potemra, Chelsea Rochman, Sheck Sherif, Joni Seager, Won Shim, Paula Sobral, and Linda Amaral-Zettler. Sources, fate and effects of microplastics in the marine environment: part 2 of a global assessment. pages, 12 2016.</p>
</div>
<div class="citation" id="id64" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">5</a><span class="fn-bracket">]</span></span>
<p>Jenna Jambeck, Roland Geyer, Chris Wilcox, Theodore Siegler, Miriam Perryman, Anthony Andrady, Ramani Narayan, and Kara Law. Marine pollution. plastic waste inputs from land into the ocean. <em>Science (New York, N.Y.)</em>, 347:768–771, 02 2015. <a class="reference external" href="https://doi.org/10.1126/science.1260352">doi:10.1126/science.1260352</a>.</p>
</div>
<div class="citation" id="id85" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">6</a><span class="fn-bracket">]</span></span>
<p>Antoine Bruge, Cristina Barreau, Jérémy Carlot, Hélène Collin, Clément Moreno, and Philippe Maison. Monitoring litter inputs from the Adour river (southwest France) to the marine environment. <em>Journal of Marine Science and Engineering</em>, 2018. <a class="reference external" href="https://doi.org/10.3390/jmse6010024">doi:10.3390/jmse6010024</a>.</p>
</div>
<div class="citation" id="id63" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">7</a><span class="fn-bracket">]</span></span>
<p>D. González-Fernández, A. Cózar, G. Hanke, J. Viejo, C. Morales-Caselles, R. Bakiu, D. Barcelo, F. Bessa, Antoine Bruge, M. Cabrera, J. Castro-Jiménez, M. Constant, R. Crosti, Yuri Galletti, A. Kideyş, N. Machitadze, Joana Pereira de Brito, M. Pogojeva, N. Ratola, J. Rigueira, E. Rojo-Nieto, O. Savenko, R. I. Schöneich-Argent, G. Siedlewicz, Giuseppe Suaria, and Myrto Tourgeli. Floating macrolitter leaked from europe into the ocean. <em>Nature Sustainability</em>, 4:474 – 483, 2021.</p>
</div>
<div class="citation" id="id96" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id7">8</a><span class="fn-bracket">]</span></span>
<p>Johnny Gasperi, Rachid Dris, Tiffany Bonin, Vincent Rocher, and Bruno Tassin. Assessment of floating plastic debris in surface water along the seine river. <em>Environmental Pollution</em>, 195:163 – 166, 2014. URL: <a class="reference external" href="http://www.sciencedirect.com/science/article/pii/S0269749114003807">http://www.sciencedirect.com/science/article/pii/S0269749114003807</a>, <a class="reference external" href="https://doi.org/https://doi.org/10.1016/j.envpol.2014.09.001">doi:https://doi.org/10.1016/j.envpol.2014.09.001</a>.</p>
</div>
<div class="citation" id="id97" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id8">9</a><span class="fn-bracket">]</span></span>
<p>David Morritt, Paris V. Stefanoudis, Dave Pearce, Oliver A. Crimmen, and Paul F. Clark. Plastic in the thames: a river runs through it. <em>Marine Pollution Bulletin</em>, 78(1):196–200, 2014. URL: <a class="reference external" href="https://www.sciencedirect.com/science/article/pii/S0025326X13006565">https://www.sciencedirect.com/science/article/pii/S0025326X13006565</a>, <a class="reference external" href="https://doi.org/https://doi.org/10.1016/j.marpolbul.2013.10.035">doi:https://doi.org/10.1016/j.marpolbul.2013.10.035</a>.</p>
</div>
<div class="citation" id="id86" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id9">10</a><span class="fn-bracket">]</span></span>
<p>Tim van Emmerik, Romain Tramoy, Caroline van Calcar, Soline Alligant, Robin Treilles, Bruno Tassin, and Johnny Gasperi. Seine Plastic Debris Transport Tenfolded During Increased River Discharge. <em>Frontiers in Marine Science</em>, 6(October):1–7, 2019. <a class="reference external" href="https://doi.org/10.3389/fmars.2019.00642">doi:10.3389/fmars.2019.00642</a>.</p>
</div>
<div class="citation" id="id87" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id9">11</a><span class="fn-bracket">]</span></span>
<p>Tim van Emmerik and Anna Schwarz. Plastic debris in rivers. <em>WIREs Water</em>, 7(1):e1398, 2020. URL: <a class="reference external" href="https://onlinelibrary.wiley.com/doi/abs/10.1002/wat2.1398">https://onlinelibrary.wiley.com/doi/abs/10.1002/wat2.1398</a>, <a class="reference external" href="https://arxiv.org/abs/https://onlinelibrary.wiley.com/doi/pdf/10.1002/wat2.1398">arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1002/wat2.1398</a>, <a class="reference external" href="https://doi.org/https://doi.org/10.1002/wat2.1398">doi:https://doi.org/10.1002/wat2.1398</a>.</p>
</div>
<div class="citation" id="id103" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>12<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id10">1</a>,<a role="doc-backlink" href="#id17">2</a>,<a role="doc-backlink" href="#id36">3</a>)</span>
<p>Pedro F Proença and Pedro Simões. TACO: Trash Annotations in Context for Litter Detection. 2020. URL: <a class="reference external" href="http://tacodataset.org/ http://arxiv.org/abs/2003.06975">http://tacodataset.org/ http://arxiv.org/abs/2003.06975</a>, <a class="reference external" href="https://arxiv.org/abs/2003.06975">arXiv:2003.06975</a>.</p>
</div>
<div class="citation" id="id81" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>13<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id11">1</a>,<a role="doc-backlink" href="#id28">2</a>)</span>
<p>Gioele Ciaparrone, Francisco Luque Sánchez, Siham Tabik, Luigi Troiano, Roberto Tagliaferri, and Francisco Herrera. Deep learning in video multi-object tracking: A survey. <em>Neurocomputing</em>, 381:61–88, 2020. <a class="reference external" href="https://arxiv.org/abs/1907.12740">arXiv:1907.12740</a>, <a class="reference external" href="https://doi.org/10.1016/j.neucom.2019.11.023">doi:10.1016/j.neucom.2019.11.023</a>.</p>
</div>
<div class="citation" id="id104" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id12">14</a><span class="fn-bracket">]</span></span>
<p>P. Bergmann, T. Meinhardt, and L. Leal-Taixe. Tracking without bells and whistles. In <em>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</em>, volume, 941–951. Los Alamitos, CA, USA, nov 2019. IEEE Computer Society. URL: <a class="reference external" href="https://doi.ieeecomputersociety.org/10.1109/ICCV.2019.00103">https://doi.ieeecomputersociety.org/10.1109/ICCV.2019.00103</a>, <a class="reference external" href="https://doi.org/10.1109/ICCV.2019.00103">doi:10.1109/ICCV.2019.00103</a>.</p>
</div>
<div class="citation" id="id105" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>15<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id13">1</a>,<a role="doc-backlink" href="#id45">2</a>,<a role="doc-backlink" href="#id46">3</a>,<a role="doc-backlink" href="#id47">4</a>)</span>
<p>Jonathon Luiten, Aljosa Osep, Patrick Dendorfer, Philip Torr, Andreas Geiger, Laura Leal-Taixé, and Bastian Leibe. Hota: a higher order metric for evaluating multi-object tracking. <em>International journal of computer vision</em>, 129(2):548–578, 2021.</p>
</div>
<div class="citation" id="id93" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id14">16</a><span class="fn-bracket">]</span></span>
<p>Prithvijit Chattopadhyay, Ramakrishna Vedantam, Ramprasaath R Selvaraju, Dhruv Batra, and Devi Parikh. Counting everyday objects in everyday scenes. In <em>Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017</em>, volume 2017-Janua, 4428–4437. 2017. <a class="reference external" href="https://arxiv.org/abs/1604.03505">arXiv:1604.03505</a>, <a class="reference external" href="https://doi.org/10.1109/CVPR.2017.471">doi:10.1109/CVPR.2017.471</a>.</p>
</div>
<div class="citation" id="id100" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id15">17</a><span class="fn-bracket">]</span></span>
<p>Carlos Arteta, Victor Lempitsky, and Andrew Zisserman. Counting in the wild. 9911:483–498, 10 2016. <a class="reference external" href="https://doi.org/10.1007/978-3-319-46478-7_30">doi:10.1007/978-3-319-46478-7_30</a>.</p>
</div>
<div class="citation" id="id101" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id16">18</a><span class="fn-bracket">]</span></span>
<p>Xingjiao Wu, Baohan Xu, Yingbin Zheng, Hao Ye, Jing Yang, and Liang He. Fast video crowd counting with a temporal aware network. <em>Neurocomputing</em>, 403:13–20, 2020. URL: <a class="reference external" href="https://www.sciencedirect.com/science/article/pii/S0925231220306561">https://www.sciencedirect.com/science/article/pii/S0925231220306561</a>, <a class="reference external" href="https://doi.org/https://doi.org/10.1016/j.neucom.2020.04.071">doi:https://doi.org/10.1016/j.neucom.2020.04.071</a>.</p>
</div>
<div class="citation" id="id108" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id16">19</a><span class="fn-bracket">]</span></span>
<p>Feng Xiong, Xingjian Shi, and Dit-Yan Yeung. Spatiotemporal modeling for crowd counting in videos. In <em>2017 IEEE International Conference on Computer Vision (ICCV)</em>, volume, 5161–5169. 2017. <a class="reference external" href="https://doi.org/10.1109/ICCV.2017.551">doi:10.1109/ICCV.2017.551</a>.</p>
</div>
<div class="citation" id="id109" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id16">20</a><span class="fn-bracket">]</span></span>
<p>Yunqi Miao, Jungong Han, Yongsheng Gao, and Baochang Zhang. ST-CNN: Spatial-Temporal Convolutional Neural Network for crowd counting in videos. <em>Pattern Recognition Letters</em>, 125:113–118, jul 2019. <a class="reference external" href="https://doi.org/10.1016/j.patrec.2019.04.012">doi:10.1016/j.patrec.2019.04.012</a>.</p>
</div>
<div class="citation" id="id92" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id18">21</a><span class="fn-bracket">]</span></span>
<p>Mattis Wolf, Katelijn van den Berg, Shungudzemwoyo Pascal Garaba, Nina Gnann, Klaus Sattler, Frederic Theodor Stahl, and Oliver Zielinski. Machine learning for aquatic plastic litter detection, classification and quantification (APLASTIC–Q). <em>Environmental Research Letters</em>, 2020. <a class="reference external" href="https://doi.org/10.1088/1748-9326/abbd01">doi:10.1088/1748-9326/abbd01</a>.</p>
</div>
<div class="citation" id="id102" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>22<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id19">1</a>,<a role="doc-backlink" href="#id31">2</a>)</span>
<p>Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: towards real-time object detection with region proposal networks. In <em>Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1</em>, NIPS'15, 91–99. Cambridge, MA, USA, 2015. MIT Press.</p>
</div>
<div class="citation" id="id98" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id20">23</a><span class="fn-bracket">]</span></span>
<p>Colin van Lieshout, Kees van Oeveren, Tim van Emmerik, and Eric Postma. Automated River Plastic Monitoring Using Deep Learning and Cameras. <em>Earth and Space Science</em>, 7(8):e2019EA000960, 2020. <a class="reference external" href="https://doi.org/10.1029/2019EA000960">doi:10.1029/2019EA000960</a>.</p>
</div>
<div class="citation" id="id107" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id21">24</a><span class="fn-bracket">]</span></span>
<p>Jungseok Hong, Michael Fulton, and Junaed Sattar. A Generative Approach Towards Improved Robotic Detection of Marine Litter. In <em>Proceedings - IEEE International Conference on Robotics and Automation</em>, 10525–10531. 2020. <a class="reference external" href="https://arxiv.org/abs/1910.04754">arXiv:1910.04754</a>, <a class="reference external" href="https://doi.org/10.1109/ICRA40945.2020.9197575">doi:10.1109/ICRA40945.2020.9197575</a>.</p>
</div>
<div class="citation" id="id110" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id22">25</a><span class="fn-bracket">]</span></span>
<p>Wenhan Luo, Junliang Xing, Anton Milan, Xiaoqin Zhang, Wei Liu, and Tae-Kyun Kim. Multiple object tracking: a literature review. <em>Artificial Intelligence</em>, 293:103448, 2021. URL: <a class="reference external" href="https://www.sciencedirect.com/science/article/pii/S0004370220301958">https://www.sciencedirect.com/science/article/pii/S0004370220301958</a>, <a class="reference external" href="https://doi.org/https://doi.org/10.1016/j.artint.2020.103448">doi:https://doi.org/10.1016/j.artint.2020.103448</a>.</p>
</div>
<div class="citation" id="id111" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>26<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id23">1</a>,<a role="doc-backlink" href="#id49">2</a>)</span>
<p>Alex Bewley, Zongyuan Ge, Lionel Ott, Fabio Ramos, and Ben Upcroft. Simple online and realtime tracking. In <em>Proceedings - International Conference on Image Processing, ICIP</em>, volume 2016-Augus, 3464–3468. 2016. URL: <a class="github reference external" href="https://github.com/abewley/sort">abewley/sort</a>, <a class="reference external" href="https://arxiv.org/abs/1602.00763">arXiv:1602.00763</a>, <a class="reference external" href="https://doi.org/10.1109/ICIP.2016.7533003">doi:10.1109/ICIP.2016.7533003</a>.</p>
</div>
<div class="citation" id="id112" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id24">27</a><span class="fn-bracket">]</span></span>
<p>Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. Nuscenes: A multimodal dataset for autonomous driving. In <em>Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</em>, 11618–11628. 2020. <a class="reference external" href="https://arxiv.org/abs/1903.11027">arXiv:1903.11027</a>, <a class="reference external" href="https://doi.org/10.1109/CVPR42600.2020.01164">doi:10.1109/CVPR42600.2020.01164</a>.</p>
</div>
<div class="citation" id="id113" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id24">28</a><span class="fn-bracket">]</span></span>
<p>Patrick Dendorfer, Hamid Rezatofighi, Anton Milan, Javen Shi, Daniel Cremers, Ian Reid, Stefan Roth, Konrad Schindler, and Laura Leal-Taixé. MOT20: A benchmark for multi object tracking in crowded scenes. 2020. <a class="reference external" href="https://arxiv.org/abs/2003.09003">arXiv:2003.09003</a>.</p>
</div>
<div class="citation" id="id114" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>29<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id25">1</a>,<a role="doc-backlink" href="#id50">2</a>)</span>
<p>Nicolai Wojke, Alex Bewley, and Dietrich Paulus. Simple online and realtime tracking with a deep association metric. In <em>Proceedings - International Conference on Image Processing, ICIP</em>, volume 2017-Septe, 3645–3649. 2018. <a class="reference external" href="https://arxiv.org/abs/1703.07402">arXiv:1703.07402</a>, <a class="reference external" href="https://doi.org/10.1109/ICIP.2017.8296962">doi:10.1109/ICIP.2017.8296962</a>.</p>
</div>
<div class="citation" id="id82" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>30<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id25">1</a>,<a role="doc-backlink" href="#id33">2</a>,<a role="doc-backlink" href="#id51">3</a>)</span>
<p>Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng, and Wenyu Liu. Fairmot: on the fairness of detection and re-identification in multiple object tracking. <em>International Journal of Computer Vision</em>, pages 1–19, 2021.</p>
</div>
<div class="citation" id="id115" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id26">31</a><span class="fn-bracket">]</span></span>
<p>Peng Chu, Jiang Wang, Quanzeng You, Haibin Ling, and Zicheng Liu. TransMOT: Spatial-Temporal Graph Transformer for Multiple Object Tracking. 2021. URL: <a class="reference external" href="http://arxiv.org/abs/2104.00194">http://arxiv.org/abs/2104.00194</a>, <a class="reference external" href="https://arxiv.org/abs/2104.00194">arXiv:2104.00194</a>.</p>
</div>
<div class="citation" id="id116" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id27">32</a><span class="fn-bracket">]</span></span>
<p>Yongxin Wang, Kris Kitani, and Xinshuo Weng. Joint object detection and multi-object tracking with graph neural networks. May 2021.</p>
</div>
<div class="citation" id="id106" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id29">33</a><span class="fn-bracket">]</span></span>
<p>Michael Fulton, Jungseok Hong, Md Jahidul Islam, and Junaed Sattar. Robotic detection of marine litter using deep visual detection models. In <em>2019 International Conference on Robotics and Automation (ICRA)</em>, 5752–5758. IEEE, 2019.</p>
</div>
<div class="citation" id="id79" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>34<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id32">1</a>,<a role="doc-backlink" href="#id34">2</a>)</span>
<p>Xingyi Zhou, Dequan Wang, and Philipp Krähenbühl. Objects as points. apr 2019. URL: <a class="reference external" href="http://arxiv.org/abs/1904.07850">http://arxiv.org/abs/1904.07850</a>, <a class="reference external" href="https://arxiv.org/abs/1904.07850">arXiv:1904.07850</a>.</p>
</div>
<div class="citation" id="id94" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id32">35</a><span class="fn-bracket">]</span></span>
<p>Hei Law and Jia Deng. Cornernet: detecting objects as paired keypoints. In Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss, editors, <em>Computer Vision - ECCV 2018 - 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part XIV</em>, volume 11218 of Lecture Notes in Computer Science, 765–781. Springer, 2018. URL: <a class="reference external" href="https://doi.org/10.1007/978-3-030-01264-9\_45">https://doi.org/10.1007/978-3-030-01264-9\_45</a>, <a class="reference external" href="https://doi.org/10.1007/978-3-030-01264-9\_45">doi:10.1007/978-3-030-01264-9\_45</a>.</p>
</div>
<div class="citation" id="id60" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id35">36</a><span class="fn-bracket">]</span></span>
<p>Fisher Yu, Dequan Wang, Evan Shelhamer, and Trevor Darrell. Deep layer aggregation. In <em>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, volume, 2403–2412. 2018. <a class="reference external" href="https://doi.org/10.1109/CVPR.2018.00255">doi:10.1109/CVPR.2018.00255</a>.</p>
</div>
<div class="citation" id="id61" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id37">37</a><span class="fn-bracket">]</span></span>
<p>Nikos Paragios, Yunmei Chen, and Olivier D Faugeras. <em>Handbook of mathematical models in computer vision</em>. Springer Science &amp; Business Media, 2006.</p>
</div>
<div class="citation" id="id71" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id38">38</a><span class="fn-bracket">]</span></span>
<p>G. Farnebäck. Two-frame motion estimation based on polynomial expansion. In <em>Scandinavian conference on Image analysis</em>, 363–370. Springer, 2003.</p>
</div>
<div class="citation" id="id68" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id40">39</a><span class="fn-bracket">]</span></span>
<p>S. Särkkä. <em>Bayesian Filtering and Smoothing</em>. Cambridge University Press, New York, NY, USA, 2013.</p>
</div>
<div class="citation" id="id72" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id41">40</a><span class="fn-bracket">]</span></span>
<p>H. W. Kuhn. The hungarian method for the assignment problem. <em>Naval Research Logistics Quarterly</em>, 2(1-2):83–97, 1955. URL: <a class="reference external" href="https://onlinelibrary.wiley.com/doi/abs/10.1002/nav.3800020109">https://onlinelibrary.wiley.com/doi/abs/10.1002/nav.3800020109</a>, <a class="reference external" href="https://arxiv.org/abs/https://onlinelibrary.wiley.com/doi/pdf/10.1002/nav.3800020109">arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1002/nav.3800020109</a>, <a class="reference external" href="https://doi.org/https://doi.org/10.1002/nav.3800020109">doi:https://doi.org/10.1002/nav.3800020109</a>.</p>
</div>
<div class="citation" id="id59" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id42">41</a><span class="fn-bracket">]</span></span>
<p>R.P.S. Mahler. Multitarget bayes filtering via first-order multitarget moments. <em>IEEE Transactions on Aerospace and Electronic Systems</em>, 39(4):1152–1178, 2003. <a class="reference external" href="https://doi.org/10.1109/TAES.2003.1261119">doi:10.1109/TAES.2003.1261119</a>.</p>
</div>
<div class="citation" id="id117" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id43">42</a><span class="fn-bracket">]</span></span>
<p>Keni Bernardin and Rainer Stiefelhagen. Evaluating multiple object tracking performance: the clear mot metrics. <em>EURASIP Journal on Image and Video Processing</em>, 2008:, 01 2008. <a class="reference external" href="https://doi.org/10.1155/2008/246309">doi:10.1155/2008/246309</a>.</p>
</div>
<div class="citation" id="id118" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id44">43</a><span class="fn-bracket">]</span></span>
<p>Ergys Ristani, Francesco Solera, Roger Zou, Rita Cucchiara, and Carlo Tomasi. Performance measures and a data set for multi-target, multi-camera tracking. In <em>European conference on computer vision</em>, 17–35. Springer, 2016.</p>
</div>
<div class="citation" id="id58" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id48">44</a><span class="fn-bracket">]</span></span>
<p>Ángel F. García-Fernández, Abu Sajana Rahmathullah, and Lennart Svensson. A metric on the space of finite sets of trajectories for evaluation of multi-target tracking algorithms. <em>IEEE Transactions on Signal Processing</em>, 68:3917–3928, 2020. <a class="reference external" href="https://doi.org/10.1109/TSP.2020.3005309">doi:10.1109/TSP.2020.3005309</a>.</p>
</div>
<div class="citation" id="id83" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id52">45</a><span class="fn-bracket">]</span></span>
<p>Xingyi Zhou, Vladlen Koltun, and Philipp Krähenbühl. Tracking objects as points. pages 474–490, 10 2020. <a class="reference external" href="https://doi.org/10.1007/978-3-030-58548-8_28">doi:10.1007/978-3-030-58548-8_28</a>.</p>
</div>
</div>
</div>
</section>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#abstract">Abstract</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#related-works">Related works</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ai-automated-counting">AI-automated counting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#computer-vision-for-macro-litter-monitoring">Computer vision for macro litter monitoring</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-object-tracking">Multi-object tracking</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#datasets-for-training-and-evaluation">Datasets for training and evaluation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#images">Images</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#data-collection">Data collection</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#bounding-box-annotation">Bounding box annotation</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#video-sequences">Video sequences</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id30">Data collection</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#track-annotation">Track annotation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optical-flow-based-counting-via-bayesian-filtering-and-confidence-regions">Optical flow-based counting via Bayesian filtering and confidence regions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#detector">Detector</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#center-based-anchor-free-detection">Center-based anchor-free detection</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training">Training</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-tracking-with-optical-flow">Bayesian tracking with optical flow</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#optical-flow">Optical flow</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#state-space-model">State space model</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#approximations-of-the-filtering-distributions">Approximations of the filtering distributions</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#generating-potential-object-tracks">Generating potential object tracks</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-association-using-confidence-regions">Data association using confidence regions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#counting">Counting</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#metrics-for-mot-based-counting">Metrics for MOT-based counting</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#count-related-mot-metrics">Count-related MOT metrics</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#detection">Detection</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#association">Association</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#count-metrics">Count metrics</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#count-decomposition">Count decomposition</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#statistics">Statistics</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#experiments">Experiments</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id53">Detection</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#counts">Counts</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#sequence-1">Sequence 1</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#sequence-2">Sequence 2</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#sequence-3">Sequence 3</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#combined-sequences">Combined sequences</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#detailed-results-on-individual-segments">Detailed results on individual segments</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#practical-impact-and-future-goals">Practical impact and future goals</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#supplements">Supplements</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#details-on-the-image-dataset">Details on the image dataset</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#categories">Categories</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#details-on-the-evaluation-videos">Details on the evaluation videos</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#river-segments">River segments</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#track-annotation-protocol">Track annotation protocol</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation-details-for-the-tracking-module">Implementation details for the tracking module</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#covariance-matrices-for-state-and-observation-noises">Covariance matrices for state and observation noises</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#influence-of-tau-and-kappa">Influence of <span class="math notranslate nohighlight">\(\tau\)</span> and <span class="math notranslate nohighlight">\(\kappa\)</span></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-filtering">Bayesian filtering</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#computing-the-confidence-regions">Computing the confidence regions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#impact-of-the-filtering-algorithm">Impact of the filtering algorithm</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#smc-based-tracking">SMC-based tracking</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-comparison">Performance comparison</a></li>
</ul>
</li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By The Jupyter Book Community
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>