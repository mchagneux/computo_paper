
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Macrolitter video counting on river banks with state space models for moving cameras &#8212; My sample book</title>
    
  <link href="_static/css/theme.css" rel="stylesheet">
  <link href="_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">My sample book</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1 current">
  <a class="reference internal" href="#">
   Macrolitter video counting on river banks with state space models for moving cameras
  </a>
 </li>
</ul>
    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        <a class="dropdown-buttons"
            href="_sources/paper.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download notebook file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/paper.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/computorg/template-computo-myst/master?urlpath=tree/paper.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Macrolitter video counting on river banks with state space models for moving cameras
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#abstract">
     Abstract
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#introduction">
     Introduction
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#related-works">
     Related works
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#ai-automated-counting">
       AI-automated counting
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#computer-vision-for-macro-litter-monitoring">
       Computer vision for macro litter monitoring
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#multi-object-tracking">
       Multi-object tracking
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#datasets-for-training-and-evaluation">
     Datasets for training and evaluation
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#images">
       Images
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#data-collection">
         Data collection
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#bounding-box-annotation">
         Bounding box annotation
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#video-sequences">
       Video sequences
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#id30">
         Data collection
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#track-annotation">
         Track annotation
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#optical-flow-based-counting-via-bayesian-filtering-and-confidence-regions">
     Optical flow-based counting via Bayesian filtering and confidence regions
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#detector">
       Detector
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#center-based-anchor-free-detection">
         Center-based anchor-free detection
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#training">
         Training
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#bayesian-tracking-with-optical-flow">
       Bayesian tracking with optical flow
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#optical-flow">
         Optical flow
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#state-space-model">
         State-space model
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#approximations-of-the-filtering-distributions">
         Approximations of the filtering distributions
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#generating-potential-object-tracks">
         Generating potential object tracks
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#data-association-using-confidence-regions">
       Data association using confidence regions
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#counting">
       Counting
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#metrics-for-mot-based-counting">
     Metrics for MOT-based counting
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#count-related-mot-metrics">
       Count-related MOT metrics
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#detection">
         Detection
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#association">
         Association
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#count-metrics">
       Count metrics
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#count-decomposition">
         Count decomposition
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#statistics">
         Statistics
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#experiments">
     Experiments
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id48">
       Detection
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#counts">
       Counts
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#practical-impact-and-future-goals">
     Practical impact and future goals
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#supplements">
   Supplements
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#details-on-the-image-dataset">
     Details on the image dataset
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#categories">
       Categories
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#details-on-the-evaluation-videos">
     Details on the evaluation videos
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#river-segments">
       River segments
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#track-annotation-protocol">
       Track annotation protocol
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#implementation-details-for-the-tracking-module">
     Implementation details for the tracking module
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#covariance-matrices-for-state-and-observation-noises">
       Covariance matrices for state and observation noises
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#computing-the-confidence-regions">
       Computing the confidence regions
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#influence-of-tau-and-kappa">
       Influence of
       <span class="math notranslate nohighlight">
        \(\tau\)
       </span>
       and
       <span class="math notranslate nohighlight">
        \(\kappa\)
       </span>
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#impact-of-the-filtering-algorithm">
     Impact of the filtering algorithm
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#smc-based-tracking">
       SMC-based tracking
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#performance-comparison">
       Performance comparison
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Macrolitter video counting on river banks with state space models for moving cameras</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Macrolitter video counting on river banks with state space models for moving cameras
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#abstract">
     Abstract
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#introduction">
     Introduction
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#related-works">
     Related works
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#ai-automated-counting">
       AI-automated counting
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#computer-vision-for-macro-litter-monitoring">
       Computer vision for macro litter monitoring
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#multi-object-tracking">
       Multi-object tracking
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#datasets-for-training-and-evaluation">
     Datasets for training and evaluation
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#images">
       Images
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#data-collection">
         Data collection
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#bounding-box-annotation">
         Bounding box annotation
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#video-sequences">
       Video sequences
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#id30">
         Data collection
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#track-annotation">
         Track annotation
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#optical-flow-based-counting-via-bayesian-filtering-and-confidence-regions">
     Optical flow-based counting via Bayesian filtering and confidence regions
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#detector">
       Detector
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#center-based-anchor-free-detection">
         Center-based anchor-free detection
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#training">
         Training
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#bayesian-tracking-with-optical-flow">
       Bayesian tracking with optical flow
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#optical-flow">
         Optical flow
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#state-space-model">
         State-space model
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#approximations-of-the-filtering-distributions">
         Approximations of the filtering distributions
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#generating-potential-object-tracks">
         Generating potential object tracks
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#data-association-using-confidence-regions">
       Data association using confidence regions
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#counting">
       Counting
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#metrics-for-mot-based-counting">
     Metrics for MOT-based counting
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#count-related-mot-metrics">
       Count-related MOT metrics
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#detection">
         Detection
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#association">
         Association
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#count-metrics">
       Count metrics
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#count-decomposition">
         Count decomposition
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#statistics">
         Statistics
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#experiments">
     Experiments
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id48">
       Detection
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#counts">
       Counts
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#practical-impact-and-future-goals">
     Practical impact and future goals
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#supplements">
   Supplements
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#details-on-the-image-dataset">
     Details on the image dataset
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#categories">
       Categories
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#details-on-the-evaluation-videos">
     Details on the evaluation videos
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#river-segments">
       River segments
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#track-annotation-protocol">
       Track annotation protocol
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#implementation-details-for-the-tracking-module">
     Implementation details for the tracking module
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#covariance-matrices-for-state-and-observation-noises">
       Covariance matrices for state and observation noises
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#computing-the-confidence-regions">
       Computing the confidence regions
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#influence-of-tau-and-kappa">
       Influence of
       <span class="math notranslate nohighlight">
        \(\tau\)
       </span>
       and
       <span class="math notranslate nohighlight">
        \(\kappa\)
       </span>
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#impact-of-the-filtering-algorithm">
     Impact of the filtering algorithm
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#smc-based-tracking">
       SMC-based tracking
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#performance-comparison">
       Performance comparison
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="math notranslate nohighlight">
\[\newcommand{\detectset}{\mathcal{D}}
\newcommand{\gauss}{\mathcal{N}}
\newcommand{\prob}{\mathbb{P}}
\newcommand{\esp}{\mathbb{E}}
\newcommand{\var}{\mathbb{V}}
\newcommand{\eqsp}{\,}
\newcommand{\Rset}{\mathbb{R}}
\newcommand{\filtdist}{\mathbb{Q}}
\newcommand{\transdist}{\mathbb{M}}
\newcommand{\SMCfiltdist}{\widehat{\filtdist}^{SMC}}
\newcommand{\rmd}{\mathrm{d}}
\newcommand{\predictdist}{\mathbb{L}}
\newcommand{\SMCpredictdist}{\widehat{\predictdist}^{SMC}}
\newcommand{\MCpredictdist}{\widehat{\predictdist}}
\newcommand{\likel}{\mathbb{G}}
\newcommand{\hatN}{\mathsf{\hat{N}}}
\newcommand{\N}{\mathsf{N}}
\newcommand{\Nmis}{\mathsf{\hat{N}_{mis}}}
\newcommand{\Nred}{\mathsf{\hat{N}_{red}}}
\newcommand{\Nfalse}{\mathsf{\hat{N}_{false}}}
\newcommand{\Ntrue}{\mathsf{\hat{N}_{true}}}
\newcommand{\gtlabels}{i \in[\![1, \N]\!]}
\newcommand{\predlabels}{j \in [\![1, \hatN]\!]}
\newcommand{\detre}{\mathsf{DetRe}}
\newcommand{\detpr}{\mathsf{DetPr}}
\newcommand{\assre}{\mathsf{AssRe}}
\newcommand{\asspr}{\mathsf{AssPr}}
\newcommand{\tempwindow}{[\![n-\lfloor \kappa/2 \rfloor, n+\lfloor \kappa/2 \rfloor]\!]}\]</div>
<div class="tex2jax_ignore mathjax_ignore section" id="macrolitter-video-counting-on-river-banks-with-state-space-models-for-moving-cameras">
<h1>Macrolitter video counting on river banks with state space models for moving cameras<a class="headerlink" href="#macrolitter-video-counting-on-river-banks-with-state-space-models-for-moving-cameras" title="Permalink to this headline">¶</a></h1>
<div class="section" id="abstract">
<h2>Abstract<a class="headerlink" href="#abstract" title="Permalink to this headline">¶</a></h2>
<p>Litter is a known cause of degradation in marine environments and most of it travels in rivers before reaching the oceans.
In this paper, we present a novel algorithm to assist waste monitoring along watercourses.
While several attempts have been made to quantify litter using neural object detection in photographs of floating items, we tackle the more challenging task of counting directly in videos using boat-embedded cameras.
We rely on multi-object tracking (MOT) but focus on the key pitfalls of false and redundant counts which arise in typical scenarios of poor detection performance.
Our system only requires supervision at the image level and performs Bayesian filtering via a state space model based on optical flow.
We present a new open image dataset gathered through a crowdsourced campaign and used to train a center-based anchor-free object detector.
Realistic video footage assembled by water monitoring experts is annotated and provided for evaluation.
Improvements in count quality are demonstrated against systems built from state-of-the-art multi-object trackers sharing the same detection capabilities.
A precise error decomposition allows clear analysis and highlights the remaining challenges.</p>
</div>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>Litter pollution concerns every part of the globe.
Each year, almost ten thousand million tons of plastic waste is generated, among which 80% ends up in landfills or in nature <span id="id1">[<a class="reference internal" href="#id59" title="Roland Geyer, Jenna Jambeck, and Kara Law. Production, use, and fate of all plastics ever made. Science Advances, 3:e1700782, 07 2017. doi:10.1126/sciadv.1700782.">1</a>]</span>, notably threatening all of the world’s oceans, seas and aquatic environments <span id="id2">[<a class="reference internal" href="#id57" title="Natalie Welden. The environmental impacts of plastic pollution. pages 195-222, 01 2020. doi:10.1016/B978-0-12-817880-5.00008-6.">2</a>, <a class="reference internal" href="#id58" title="Thushari Gamage and J.D.M. Senevirathna. Plastic pollution in the marine environment. Heliyon, 6:e04709, 08 2020. doi:10.1016/j.heliyon.2020.e04709.">3</a>]</span>.
Plastic pollution is known to already impact more than 3763 marine species worldwide (see <a class="reference external" href="https://litterbase.awi.de/">this</a> detailed analysis) with risk of proliferation through the whole food chain.
This accumulation of waste is the endpoint of the largely misunderstood path of trash, mainly coming from land-based sources <span id="id3">[<a class="reference internal" href="#id54" title="Chelsea Rochman, Anthony Andrady, Sarah Dudas, Joan Fabres, François Galgani, Denise lead, Valeria Hidalgo-Ruz, Sunny Hong, Peter Kershaw, Laurent Lebreton, Amy Lusher, Ramani Narayan, Sabine Pahl, James Potemra, Chelsea Rochman, Sheck Sherif, Joni Seager, Won Shim, Paula Sobral, and Linda Amaral-Zettler. Sources, fate and effects of microplastics in the marine environment: part 2 of a global assessment. pages, 12 2016.">4</a>]</span>, yet rivers have been identified as a major pathway for the introduction of waste into marine environments <span id="id4">[<a class="reference internal" href="#id56" title="Jenna Jambeck, Roland Geyer, Chris Wilcox, Theodore Siegler, Miriam Perryman, Anthony Andrady, Ramani Narayan, and Kara Law. Marine pollution. plastic waste inputs from land into the ocean. Science (New York, N.Y.), 347:768-771, 02 2015. doi:10.1126/science.1260352.">5</a>]</span>.
Therefore, field data on rivers and monitoring are strongly needed to assess the impact of measures that can be taken. The analysis of such field data over time is pivotal to understand the efficiency of the actions implemented such as choosing zero-waste alternatives to plastic, designing new products to be long-lasting or reusable, introducing policies to reduce over-packing.</p>
<p>Different methods have already been tested to monitor waste in rivers: litter collection and sorting on riverbanks <span id="id5">[<a class="reference internal" href="#id77" title="Antoine Bruge, Cristina Barreau, Jérémy Carlot, Hélène Collin, Clément Moreno, and Philippe Maison. Monitoring litter inputs from the Adour river (southwest France) to the marine environment. Journal of Marine Science and Engineering, 2018. doi:10.3390/jmse6010024.">6</a>]</span>, visual counting of drifting litter from bridges <span id="id6">[<a class="reference internal" href="#id55" title="D. González-Fernández, A. Cózar, G. Hanke, J. Viejo, C. Morales-Caselles, R. Bakiu, D. Barcelo, F. Bessa, Antoine Bruge, M. Cabrera, J. Castro-Jiménez, M. Constant, R. Crosti, Yuri Galletti, A. Kideyş, N. Machitadze, Joana Pereira de Brito, M. Pogojeva, N. Ratola, J. Rigueira, E. Rojo-Nieto, O. Savenko, R. I. Schöneich-Argent, G. Siedlewicz, Giuseppe Suaria, and Myrto Tourgeli. Floating macrolitter leaked from europe into the ocean. Nature Sustainability, 4:474 - 483, 2021.">7</a>]</span>, floating booms <span id="id7">[<a class="reference internal" href="#id88" title="Johnny Gasperi, Rachid Dris, Tiffany Bonin, Vincent Rocher, and Bruno Tassin. Assessment of floating plastic debris in surface water along the seine river. Environmental Pollution, 195:163 - 166, 2014. URL: http://www.sciencedirect.com/science/article/pii/S0269749114003807, doi:https://doi.org/10.1016/j.envpol.2014.09.001.">8</a>]</span> and nets <span id="id8">[<a class="reference internal" href="#id89" title="David Morritt, Paris V. Stefanoudis, Dave Pearce, Oliver A. Crimmen, and Paul F. Clark. Plastic in the thames: a river runs through it. Marine Pollution Bulletin, 78(1):196-200, 2014. URL: https://www.sciencedirect.com/science/article/pii/S0025326X13006565, doi:https://doi.org/10.1016/j.marpolbul.2013.10.035.">9</a>]</span>.
All are helpful to understand the origin and typology of litter pollution yet hardly compatible with long term monitoring at country scales.
Monitoring tools need to be reliable, easy to set up on various types of rivers, and should give an overview of plastic pollution during peak discharge to help locate hotspots and provide trends.
Newer studies suggest that plastic debris transport could be better understood by counting litter trapped on river banks, providing a good indication of the local macrolitter pollution especially after increased river discharge <span id="id9">[<a class="reference internal" href="#id78" title="Tim van Emmerik, Romain Tramoy, Caroline van Calcar, Soline Alligant, Robin Treilles, Bruno Tassin, and Johnny Gasperi. Seine Plastic Debris Transport Tenfolded During Increased River Discharge. Frontiers in Marine Science, 6(October):1–7, 2019. doi:10.3389/fmars.2019.00642.">10</a>, <a class="reference internal" href="#id79" title="Tim van Emmerik and Anna Schwarz. Plastic debris in rivers. WIREs Water, 7(1):e1398, 2020. URL: https://onlinelibrary.wiley.com/doi/abs/10.1002/wat2.1398, arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1002/wat2.1398, doi:https://doi.org/10.1002/wat2.1398.">11</a>]</span>.
Based on these findings, we propose a new method for litter monitoring which relies on videos of river banks directly captured from moving boats.</p>
<p>In this case, object detection with deep neural networks (DNNs) may be used, but new challenges arise.
First, available data is still scarce.
When considering entire portions of river banks from many different locations, the variety of scenes, viewing angles and/or light conditions is not well covered by existing plastic litter datasets like <span id="id10">[<a class="reference internal" href="#id95" title="Pedro F Proença and Pedro Simões. TACO: Trash Annotations in Context for Litter Detection. 2020. URL: http://tacodataset.org/ http://arxiv.org/abs/2003.06975, arXiv:2003.06975.">12</a>]</span>, where litter is usually captured from relatively close distances and many times in urban or domestic backgrounds. Therefore, achieving robust object detection across multiple conditions is still delicate.</p>
<p>Second, counting from videos is a different task than counting from independent images, because individual objects will typically appear in several consecutive frames, yet they must only be counted once. This last problem of association has been extensively studied for the multi-object tracking (MOT) task, which aims at recovering individual trajectories for objects in videos. When successful MOT is achieved, counting objects in videos is equivalent to counting the number of estimated trajectories. Deep learning has been increasingly used to improve MOT solutions <span id="id11">[<a class="reference internal" href="#id73" title="Gioele Ciaparrone, Francisco Luque Sánchez, Siham Tabik, Luigi Troiano, Roberto Tagliaferri, and Francisco Herrera. Deep learning in video multi-object tracking: A survey. Neurocomputing, 381:61–88, 2020. arXiv:1907.12740, doi:10.1016/j.neucom.2019.11.023.">13</a>]</span>. However, newer state-of-the-art techniques require increasingly heavy and costly supervision, typically all object positions provided at every frame. In addition, many successful techniques <span id="id12">[<a class="reference internal" href="#id96" title="P. Bergmann, T. Meinhardt, and L. Leal-Taixe. Tracking without bells and whistles. In 2019 IEEE/CVF International Conference on Computer Vision (ICCV), volume, 941-951. Los Alamitos, CA, USA, nov 2019. IEEE Computer Society. URL: https://doi.ieeecomputersociety.org/10.1109/ICCV.2019.00103, doi:10.1109/ICCV.2019.00103.">14</a>]</span> can hardly be used in scenarios with abrupt and nonlinear camera motion. Finally, while research is still active to rigorously evaluate performance at multi-object <em>tracking</em> <span id="id13">[<a class="reference internal" href="#id97" title="Jonathon Luiten, Aljosa Osep, Patrick Dendorfer, Philip Torr, Andreas Geiger, Laura Leal-Taixé, and Bastian Leibe. Hota: a higher order metric for evaluating multi-object tracking. International journal of computer vision, 129(2):548–578, 2021.">15</a>]</span>, most but not all aspects of the latter may affect global video counts, which calls for a separate evaluation protocol dedicated to multi-object <em>counting</em>.</p>
<p>Our contribution can be summarized as follows.</p>
<ol class="simple">
<li><p>We provide a novel open-source image dataset of macro litter, which includes various objects seen from different rivers and different contexts.
This dataset was produced with a new open-sourced platform for data gathering and annotation developed in conjunction with Surfrider Foundation Europe, continuously growing with more data.</p></li>
<li><p>We propose a new algorithm specifically tailored to count in videos with fast camera movements.
In a nutshell, DNN-based object detection is paired with a robust state space movement model which uses optical flow to perform Bayesian filtering, while confidence regions built on posterior predictive distributions are used for data association.
This framework does not require video annotations at training time: the multi-object tracking module does not require supervision, only the DNN-based object detection does require annotated images.
It also fully leverages optical flow estimates and the uncertainty provided by Bayesian predictions to recover object identities even when detection recall is low.
Contrary to existing MOT solutions, this method ensures that tracks are stable enough to avoid repeated counting of the same object.</p></li>
<li><p>We provide a set of video sequences where litter counts are known and depicted in real conditions.
For these videos only, litter positions are manually annotated at every frame in order to carefully analyze performance.
This allows us to build new informative count metrics.
We compare the count performance of our method against other MOT-based alternatives.</p></li>
</ol>
<p>A first visual illustation of the second claim is presented via the following code chunks: on three selected frames, we present a typical scenario where our strategy can avoid overcounting the same object (we depict internal workings of our solution against the end result of the competitors).</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">myst_nb</span> <span class="kn">import</span> <span class="n">glue</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">matplotlib</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;agg&#39;</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="kn">from</span> <span class="nn">surfnet.prepare_data</span> <span class="kn">import</span> <span class="n">download_data</span>
<span class="kn">from</span> <span class="nn">surfnet.track</span> <span class="kn">import</span> <span class="n">default_args</span> <span class="k">as</span> <span class="n">args</span>

<span class="c1"># download frames and detections from a given deep detector model</span>
<span class="n">download_data</span><span class="p">()</span>

<span class="c1"># prepare arguments</span>
<span class="n">args</span><span class="o">.</span><span class="n">external_detections</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">args</span><span class="o">.</span><span class="n">data_dir</span> <span class="o">=</span> <span class="s1">&#39;data/external_detections/part_1_segment_0&#39;</span>
<span class="n">args</span><span class="o">.</span><span class="n">output_dir</span> <span class="o">=</span> <span class="s1">&#39;surfnet/results&#39;</span>
<span class="n">args</span><span class="o">.</span><span class="n">noise_covariances_path</span> <span class="o">=</span> <span class="s1">&#39;surfnet/data/tracking_parameters&#39;</span>
<span class="n">args</span><span class="o">.</span><span class="n">confidence_threshold</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">args</span><span class="o">.</span><span class="n">algorithm</span> <span class="o">=</span> <span class="s1">&#39;EKF&#39;</span>
<span class="n">args</span><span class="o">.</span><span class="n">ratio</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">args</span><span class="o">.</span><span class="n">display</span> <span class="o">=</span> <span class="mi">0</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">NameError</span><span class="g g-Whitespace">                                 </span>Traceback (most recent call last)
<span class="nn">Input In [1],</span> in <span class="ni">&lt;cell line: 3&gt;</span><span class="nt">()</span>
<span class="g g-Whitespace">      </span><span class="mi">1</span> <span class="kn">from</span> <span class="nn">myst_nb</span> <span class="kn">import</span> <span class="n">glue</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="ne">----&gt; </span><span class="mi">3</span> <span class="n">matplotlib</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;agg&#39;</span><span class="p">)</span>
<span class="g g-Whitespace">      </span><span class="mi">4</span> <span class="kn">import</span> <span class="nn">os</span>
<span class="g g-Whitespace">      </span><span class="mi">6</span> <span class="kn">from</span> <span class="nn">surfnet.prepare_data</span> <span class="kn">import</span> <span class="n">download_data</span>

<span class="ne">NameError</span>: name &#39;matplotlib&#39; is not defined
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pickle</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">surfnet.tracking.utils</span> <span class="kn">import</span> <span class="n">resize_external_detections</span><span class="p">,</span> <span class="n">write_tracking_results_to_file</span>
<span class="kn">from</span> <span class="nn">surfnet.tools.video_readers</span> <span class="kn">import</span> <span class="n">FramesWithInfo</span>
<span class="kn">from</span> <span class="nn">surfnet.tracking.trackers</span> <span class="kn">import</span> <span class="n">get_tracker</span>
<span class="kn">from</span> <span class="nn">surfnet.track</span> <span class="kn">import</span> <span class="n">track_video</span>

<span class="c1"># Initialize variances</span>
<span class="n">transition_variance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">noise_covariances_path</span><span class="p">,</span> <span class="s1">&#39;transition_variance.npy&#39;</span><span class="p">))</span>
<span class="n">observation_variance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">noise_covariances_path</span><span class="p">,</span> <span class="s1">&#39;observation_variance.npy&#39;</span><span class="p">))</span>

<span class="c1"># Get tracker algorithm</span>
<span class="n">engine</span> <span class="o">=</span> <span class="n">get_tracker</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">algorithm</span><span class="p">)</span>

<span class="c1"># Open data: detections and frames</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">data_dir</span><span class="p">,</span> <span class="s1">&#39;saved_detections.pickle&#39;</span><span class="p">),</span><span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">detections</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">data_dir</span><span class="p">,</span> <span class="s1">&#39;saved_frames.pickle&#39;</span><span class="p">),</span><span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">frames</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>

<span class="c1"># Create frame reader and resize detections</span>
<span class="n">reader</span> <span class="o">=</span> <span class="n">FramesWithInfo</span><span class="p">(</span><span class="n">frames</span><span class="p">)</span>
<span class="n">detections</span> <span class="o">=</span> <span class="n">resize_external_detections</span><span class="p">(</span><span class="n">detections</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">ratio</span><span class="p">)</span>

<span class="c1"># Start tracking, storing intermediate tracklets</span>
<span class="n">results</span><span class="p">,</span> <span class="n">frame_to_trackers</span> <span class="o">=</span> <span class="n">track_video</span><span class="p">(</span><span class="n">reader</span><span class="p">,</span> <span class="n">detections</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">engine</span><span class="p">,</span> 
                                         <span class="n">transition_variance</span><span class="p">,</span> <span class="n">observation_variance</span><span class="p">,</span> <span class="n">return_trackers</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Write final results</span>
<span class="n">write_tracking_results_to_file</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">ratio_x</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">ratio</span><span class="p">,</span> <span class="n">ratio_y</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">ratio</span><span class="p">,</span> <span class="n">output_filename</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">output_dir</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">surfnet.track</span> <span class="kn">import</span> <span class="n">build_image_trackers</span>
<span class="c1"># Choose a few indices to display (same for our algorithm and SORT)</span>
<span class="n">idxs</span> <span class="o">=</span> <span class="p">[</span><span class="mi">108</span><span class="p">,</span> <span class="mi">112</span><span class="p">,</span> <span class="mi">117</span><span class="p">]</span>
    
<span class="n">considered_frames</span> <span class="o">=</span> <span class="p">[</span><span class="n">frames</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">idxs</span><span class="p">]</span>
<span class="n">considered_trackers</span> <span class="o">=</span> <span class="p">[</span><span class="n">frame_to_trackers</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">idxs</span><span class="p">]</span>
<span class="n">glue</span><span class="p">(</span><span class="s1">&#39;demo_ours&#39;</span><span class="p">,</span> <span class="n">build_image_trackers</span><span class="p">(</span><span class="n">considered_frames</span><span class="p">,</span> <span class="n">considered_trackers</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">reader</span><span class="p">),</span> <span class="n">display</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="figure align-default" id="demo-ours">
<p class="caption"><span class="caption-number">Fig. 1 </span><span class="caption-text"><em>Our method</em>: one object (red dot) is correctly detected at every frame and given a consistent identity throughout the sequence with low location uncertainty (red ellipse). Next to it, a false positive detection is generated at the first frame (brown dot) but immediatly lost in the following frames: the associated uncertainty grows fast (brown ellipse). In our solution, this type of track will not be counted. A third correctly detected object (pink) appears in the third frame and begins a new track.</span><a class="headerlink" href="#demo-ours" title="Permalink to this image">¶</a></p>
</div>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Tracker with SORT </span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
<span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">from</span> <span class="nn">sort.sort</span> <span class="kn">import</span> <span class="n">track</span> <span class="k">as</span> <span class="n">sort_tracker</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Tracking with SORT...&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;--- Begin SORT internal logs&#39;</span><span class="p">)</span>
<span class="n">sort_tracker</span><span class="p">(</span><span class="n">detections_dir</span><span class="o">=</span><span class="s1">&#39;data/external_detections&#39;</span><span class="p">,</span> <span class="n">output_dir</span><span class="o">=</span><span class="s1">&#39;sort/results&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;--- End&#39;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">read_sort_output</span><span class="p">(</span><span class="n">filename</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Reads the output .txt of Sort (or other tracking algorithm)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">dict_frames</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">items</span> <span class="o">=</span> <span class="n">line</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;,&quot;</span><span class="p">)</span>
            <span class="n">frame</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">items</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="n">objnum</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">items</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">x</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">items</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
            <span class="n">y</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">items</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
            <span class="n">dict_frames</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">items</span><span class="p">[</span><span class="mi">0</span><span class="p">])]</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">objnum</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
    <span class="k">return</span>  <span class="n">dict_frames</span>


<span class="k">def</span> <span class="nf">build_image</span><span class="p">(</span><span class="n">frames</span><span class="p">,</span> <span class="n">trackers</span><span class="p">,</span> <span class="n">image_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">135</span><span class="p">,</span><span class="mi">240</span><span class="p">),</span> <span class="n">downsampling</span><span class="o">=</span><span class="mi">2</span><span class="o">*</span><span class="mi">4</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Builds a full image with consecutive frames and their displayed trackers</span>
<span class="sd">    frames: a list of K np.array</span>
<span class="sd">    trackers: a list of K trackers. Each tracker is a per frame list of tracked objects</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">K</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">frames</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">trackers</span><span class="p">)</span> <span class="o">==</span> <span class="n">K</span>
    <span class="n">font</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">FONT_HERSHEY_COMPLEX</span>
    <span class="n">output_img</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">image_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">image_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">K</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
    <span class="n">object_ids</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">tracker</span> <span class="ow">in</span> <span class="n">trackers</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">detection</span> <span class="ow">in</span> <span class="n">tracker</span><span class="p">:</span>
            <span class="n">object_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">detection</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">min_object_id</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">object_ids</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
        <span class="n">frame</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">cv2</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="n">frames</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">image_shape</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2RGB</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">detection</span> <span class="ow">in</span> <span class="n">trackers</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
            <span class="n">cv2</span><span class="o">.</span><span class="n">putText</span><span class="p">(</span><span class="n">frame</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">detection</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="n">min_object_id</span> <span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">detection</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="n">downsampling</span><span class="p">)</span><span class="o">+</span><span class="mi">10</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">detection</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">/</span><span class="n">downsampling</span><span class="p">)</span><span class="o">+</span><span class="mi">10</span><span class="p">),</span> <span class="n">font</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="p">(</span><span class="mi">255</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">LINE_AA</span><span class="p">)</span>

        <span class="n">output_img</span><span class="p">[:,</span><span class="n">i</span><span class="o">*</span><span class="n">image_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">image_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],:]</span> <span class="o">=</span> <span class="n">frame</span>
    <span class="k">return</span> <span class="n">output_img</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input tag_remove-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># open sort output</span>
<span class="n">tracker_file</span> <span class="o">=</span> <span class="s2">&quot;sort/results/part_1_segment_0.txt&quot;</span>
<span class="n">frame_to_track</span> <span class="o">=</span> <span class="n">read_sort_output</span><span class="p">(</span><span class="n">tracker_file</span><span class="p">)</span>

<span class="n">condisered_frames</span> <span class="o">=</span> <span class="p">[</span><span class="n">frames</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">idxs</span><span class="p">]</span>
<span class="n">considered_tracks</span> <span class="o">=</span> <span class="p">[</span><span class="n">frame_to_track</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">idxs</span><span class="p">]</span>
<span class="n">out_img</span> <span class="o">=</span> <span class="n">build_image</span><span class="p">(</span><span class="n">condisered_frames</span><span class="p">,</span> <span class="n">considered_tracks</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">out_img</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">);</span>
<span class="n">glue</span><span class="p">(</span><span class="s1">&#39;sort_demo&#39;</span><span class="p">,</span> <span class="n">plt</span><span class="o">.</span><span class="n">gcf</span><span class="p">(),</span> <span class="n">display</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="figure align-default" id="demo-sort">
<p class="caption"><span class="caption-number">Fig. 2 </span><span class="caption-text"><em>SORT</em>: the resulting count is also 2, but both counts arise from tracks generated by the same object, the latter not re-associated at all in the second frame. Additionally, the third object is discarded (in post-processing) by their strategy.</span><a class="headerlink" href="#demo-sort" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="related-works">
<h2>Related works<a class="headerlink" href="#related-works" title="Permalink to this headline">¶</a></h2>
<div class="section" id="ai-automated-counting">
<h3>AI-automated counting<a class="headerlink" href="#ai-automated-counting" title="Permalink to this headline">¶</a></h3>
<p>Counting from images has been an ongoing challenge in computer vision.
Most works can be divided into (i) detection-based methods where objects are individually located for counting (ii) density-based methods where counts are obtained by summing a predicted density map (iii) regression-based methods where counts are directly regressed from input images <span id="id14">[<a class="reference internal" href="#id85" title="Prithvijit Chattopadhyay, Ramakrishna Vedantam, Ramprasaath R Selvaraju, Dhruv Batra, and Devi Parikh. Counting everyday objects in everyday scenes. In Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, volume 2017-Janua, 4428–4437. 2017. arXiv:1604.03505, doi:10.1109/CVPR.2017.471.">16</a>]</span>.
While some of these works tackled the problem of counting in wild scenes <span id="id15">[<a class="reference internal" href="#id92" title="Carlos Arteta, Victor Lempitsky, and Andrew Zisserman. Counting in the wild. 9911:483-498, 10 2016. doi:10.1007/978-3-319-46478-7_30.">17</a>]</span>, most are focused on pedestrian and crowd counting.
Though several works <span id="id16">[<a class="reference internal" href="#id93" title="Xingjiao Wu, Baohan Xu, Yingbin Zheng, Hao Ye, Jing Yang, and Liang He. Fast video crowd counting with a temporal aware network. Neurocomputing, 403:13-20, 2020. URL: https://www.sciencedirect.com/science/article/pii/S0925231220306561, doi:https://doi.org/10.1016/j.neucom.2020.04.071.">18</a>, <a class="reference internal" href="#id100" title="Feng Xiong, Xingjian Shi, and Dit-Yan Yeung. Spatiotemporal modeling for crowd counting in videos. In 2017 IEEE International Conference on Computer Vision (ICCV), volume, 5161-5169. 2017. doi:10.1109/ICCV.2017.551.">19</a>, <a class="reference internal" href="#id101" title="Yunqi Miao, Jungong Han, Yongsheng Gao, and Baochang Zhang. ST-CNN: Spatial-Temporal Convolutional Neural Network for crowd counting in videos. Pattern Recognition Letters, 125:113–118, jul 2019. doi:10.1016/j.patrec.2019.04.012.">20</a>]</span> showed the relevance of leveraging sequential inter-frame information to achieve better counts at every frame, none of these methods actually attempt to produce global video counts.</p>
</div>
<div class="section" id="computer-vision-for-macro-litter-monitoring">
<h3>Computer vision for macro litter monitoring<a class="headerlink" href="#computer-vision-for-macro-litter-monitoring" title="Permalink to this headline">¶</a></h3>
<p>Automatic macro litter monitoring in rivers is still a relatively nascent initiative, yet there have already been several attempts at using DNN-based object recognition tools to count plastic trash.
Recently, <span id="id17">[<a class="reference internal" href="#id95" title="Pedro F Proença and Pedro Simões. TACO: Trash Annotations in Context for Litter Detection. 2020. URL: http://tacodataset.org/ http://arxiv.org/abs/2003.06975, arXiv:2003.06975.">12</a>]</span> used a combination of two Convolutional Neural Networks (CNNs) to detect and quantify plastic litter using geospatial images from Cambodia.
In <span id="id18">[<a class="reference internal" href="#id84" title="Mattis Wolf, Katelijn van den Berg, Shungudzemwoyo Pascal Garaba, Nina Gnann, Klaus Sattler, Frederic Theodor Stahl, and Oliver Zielinski. Machine learning for aquatic plastic litter detection, classification and quantification (APLASTIC–Q). Environmental Research Letters, 2020. doi:10.1088/1748-9326/abbd01.">21</a>]</span>, reliable estimates of plastic density were obtained using Faster R-CNN <span id="id19">[<a class="reference internal" href="#id94" title="Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: towards real-time object detection with region proposal networks. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS'15, 91–99. Cambridge, MA, USA, 2015. MIT Press.">22</a>]</span> on images extracted from bridge-mounted cameras.
For underwater waste monitoring, <span id="id20">[<a class="reference internal" href="#id90" title="Colin van Lieshout, Kees van Oeveren, Tim van Emmerik, and Eric Postma. Automated River Plastic Monitoring Using Deep Learning and Cameras. Earth and Space Science, 7(8):e2019EA000960, 2020. doi:10.1029/2019EA000960.">23</a>]</span> assembled a dataset with bounding box annotations, and showed promising performance with several object detectors.
They later turned to generative models to obtain more synthetic data from a small dataset <span id="id21">[<a class="reference internal" href="#id99" title="Jungseok Hong, Michael Fulton, and Junaed Sattar. A Generative Approach Towards Improved Robotic Detection of Marine Litter. In Proceedings - IEEE International Conference on Robotics and Automation, 10525–10531. 2020. arXiv:1910.04754, doi:10.1109/ICRA40945.2020.9197575.">24</a>]</span>.
While proving the practicality of deep learning for automatic waste detection in various contexts, these works only provide counts for separate images of photographed litter.
To the best of our knowledge, no solution has been proposed to count litter directly in videos.</p>
</div>
<div class="section" id="multi-object-tracking">
<h3>Multi-object tracking<a class="headerlink" href="#multi-object-tracking" title="Permalink to this headline">¶</a></h3>
<p>Multi-object tracking usually involves object detection, data association and track management, with a very large number of methods already existing before DNNs <span id="id22">[<a class="reference internal" href="#id102" title="Wenhan Luo, Junliang Xing, Anton Milan, Xiaoqin Zhang, Wei Liu, and Tae-Kyun Kim. Multiple object tracking: a literature review. Artificial Intelligence, 293:103448, 2021. URL: https://www.sciencedirect.com/science/article/pii/S0004370220301958, doi:https://doi.org/10.1016/j.artint.2020.103448.">25</a>]</span>.
MOT approaches now mostly differ in the level of supervision they require for each step: until recently, most successful methods (like <span id="id23">[<a class="reference internal" href="#id103" title="Alex Bewley, Zongyuan Ge, Lionel Ott, Fabio Ramos, and Ben Upcroft. Simple online and realtime tracking. In Proceedings - International Conference on Image Processing, ICIP, volume 2016-Augus, 3464–3468. 2016. URL: https://github.com/abewley/sort, arXiv:1602.00763, doi:10.1109/ICIP.2016.7533003.">26</a>]</span>) have been detection-based, i.e.
involving only a DNN-based object detector trained at the image level and coupled with an unsupervised data association step.
In specific fields such as pedestrian tracking or autonomous driving, vast datasets now provide precise object localisation and identities throughout entire videos <span id="id24">[<a class="reference internal" href="#id104" title="Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. Nuscenes: A multimodal dataset for autonomous driving. In Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 11618–11628. 2020. arXiv:1903.11027, doi:10.1109/CVPR42600.2020.01164.">27</a>, <a class="reference internal" href="#id105" title="Patrick Dendorfer, Hamid Rezatofighi, Anton Milan, Javen Shi, Daniel Cremers, Ian Reid, Stefan Roth, Konrad Schindler, and Laura Leal-Taixé. MOT20: A benchmark for multi object tracking in crowded scenes. 2020. arXiv:2003.09003.">28</a>]</span>.
Current state-of-the-art methods leverage this supervision via deep visual feature extraction <span id="id25">[<a class="reference internal" href="#id106" title="Nicolai Wojke, Alex Bewley, and Dietrich Paulus. Simple online and realtime tracking with a deep association metric. In Proceedings - International Conference on Image Processing, ICIP, volume 2017-Septe, 3645–3649. 2018. arXiv:1703.07402, doi:10.1109/ICIP.2017.8296962.">29</a>, <a class="reference internal" href="#id74" title="Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng, and Wenyu Liu. Fairmot: on the fairness of detection and re-identification in multiple object tracking. International Journal of Computer Vision, pages 1–19, 2021.">30</a>]</span> or even self-attention <span id="id26">[<a class="reference internal" href="#id107" title="Peng Chu, Jiang Wang, Quanzeng You, Haibin Ling, and Zicheng Liu. TransMOT: Spatial-Temporal Graph Transformer for Multiple Object Tracking. 2021. URL: http://arxiv.org/abs/2104.00194, arXiv:2104.00194.">31</a>]</span> and graph neural networks <span id="id27">[<a class="reference internal" href="#id108" title="Yongxin Wang, Kris Kitani, and Xinshuo Weng. Joint object detection and multi-object tracking with graph neural networks. May 2021.">32</a>]</span>.
For these applications, motion prediction may be required, yet well-trained appearance models are usually enough to deal with detection failures under simple motion, therefore the linear constant-velocity assumption often prevails (<span id="id28">[<a class="reference internal" href="#id73" title="Gioele Ciaparrone, Francisco Luque Sánchez, Siham Tabik, Luigi Troiano, Roberto Tagliaferri, and Francisco Herrera. Deep learning in video multi-object tracking: A survey. Neurocomputing, 381:61–88, 2020. arXiv:1907.12740, doi:10.1016/j.neucom.2019.11.023.">13</a>]</span>).</p>
<p>In the case of macrolitter monitoring, however, available image datasets are still orders of magnitude smaller, and annotated video datasets do not exist at all.
Even more so, real shooting conditions induce chaotic movements on the boat-embedded cameras.
A close work of ours is that of <span id="id29">[<a class="reference internal" href="#id98" title="Michael Fulton, Jungseok Hong, Md Jahidul Islam, and Junaed Sattar. Robotic detection of marine litter using deep visual detection models. In 2019 International Conference on Robotics and Automation (ICRA), 5752–5758. IEEE, 2019.">33</a>]</span>, who paired Kalman filtering with optical flow to yield fruit count estimates on entire video sequences captured by moving robots.
However, their video footage is captured at night with consistent lighting conditions, backgrounds are largely similar across sequences, and camera movements are less challenging.
In our application context, we find that using MOT for the task of counting objects requires a new movement model, to take into account missing detections and large camera movements.</p>
</div>
</div>
<div class="section" id="datasets-for-training-and-evaluation">
<h2>Datasets for training and evaluation<a class="headerlink" href="#datasets-for-training-and-evaluation" title="Permalink to this headline">¶</a></h2>
<p>Our main dataset of annotated images is used to train the object detector.
Then, only for evaluation purposes, we provide videos with annotated object positions and known global counts.
Our motivation is to avoid relying on training data that requires this resource-consuming process.</p>
<div class="section" id="images">
<h3>Images<a class="headerlink" href="#images" title="Permalink to this headline">¶</a></h3>
<div class="section" id="data-collection">
<h4>Data collection<a class="headerlink" href="#data-collection" title="Permalink to this headline">¶</a></h4>
<p>With help from benevolents, we compile photographs of litter stranded on river banks after increased river discharge, shot directly from kayaks navigating at varying distances from the shore.
Images span multiple rivers with various levels of water current, on different seasons, mostly in southwestern France.
The resulting pictures depict trash items under the same conditions as the video footage we wish to count on, while spanning a wide variety of backgrounds, light conditions, viewing angles and picture quality.</p>
</div>
<div class="section" id="bounding-box-annotation">
<h4>Bounding box annotation<a class="headerlink" href="#bounding-box-annotation" title="Permalink to this headline">¶</a></h4>
<p>For object detection applications, the images are annotated using a custom online platform where each object is located using a bounding box.
In this work, we focus only on litter counting without classification, however the annotated objects are already classified into specific categories which are described in <a class="reference internal" href="#trash-categories-image"><span class="std std-ref">Trash categories defined to facilitate porting to a counting system that allows trash identification</span></a>.</p>
<p>A few samples are depicted below:</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span><span class="p">,</span> <span class="n">ExifTags</span>
<span class="kn">from</span> <span class="nn">pycocotools.coco</span> <span class="kn">import</span> <span class="n">COCO</span>


<span class="k">def</span> <span class="nf">draw_bbox</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">anns</span><span class="p">,</span> <span class="n">ratio</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Display the specified annotations.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">ann</span> <span class="ow">in</span> <span class="n">anns</span><span class="p">:</span>
        <span class="p">[</span><span class="n">bbox_x</span><span class="p">,</span> <span class="n">bbox_y</span><span class="p">,</span> <span class="n">bbox_w</span><span class="p">,</span> <span class="n">bbox_h</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">ratio</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">ann</span><span class="p">[</span><span class="s1">&#39;bbox&#39;</span><span class="p">]))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="n">cv2</span><span class="o">.</span><span class="n">rectangle</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="p">(</span><span class="n">bbox_x</span><span class="p">,</span><span class="n">bbox_y</span><span class="p">),(</span><span class="n">bbox_x</span><span class="o">+</span><span class="n">bbox_w</span><span class="p">,</span><span class="n">bbox_y</span><span class="o">+</span><span class="n">bbox_h</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">),</span><span class="n">thickness</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">image</span>

<span class="nb">dir</span> <span class="o">=</span> <span class="s1">&#39;surfnet/data/images&#39;</span>

<span class="n">ann_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">dir</span><span class="p">,</span><span class="s1">&#39;annotations&#39;</span><span class="p">)</span>
<span class="n">data_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">dir</span><span class="p">,</span><span class="s1">&#39;images&#39;</span><span class="p">)</span>
<span class="n">ann_file</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">ann_dir</span><span class="p">,</span> <span class="s1">&#39;subset_of_annotations.json&#39;</span><span class="p">)</span>
<span class="n">coco</span> <span class="o">=</span> <span class="n">COCO</span><span class="p">(</span><span class="n">ann_file</span><span class="p">)</span>

<span class="n">imgIds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">coco</span><span class="o">.</span><span class="n">getImgIds</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{}</span><span class="s1"> images loaded&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">imgIds</span><span class="p">)))</span>

<span class="k">for</span> <span class="n">imgId</span> <span class="ow">in</span> <span class="n">imgIds</span><span class="p">:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">coco</span><span class="o">.</span><span class="n">loadImgs</span><span class="p">(</span><span class="n">ids</span><span class="o">=</span><span class="p">[</span><span class="n">imgId</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span><span class="n">image</span><span class="p">[</span><span class="s1">&#39;file_name&#39;</span><span class="p">]))</span>
        <span class="c1"># Rotation of the picture in the Exif tags</span>
        <span class="k">for</span> <span class="n">orientation</span> <span class="ow">in</span> <span class="n">ExifTags</span><span class="o">.</span><span class="n">TAGS</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">ExifTags</span><span class="o">.</span><span class="n">TAGS</span><span class="p">[</span><span class="n">orientation</span><span class="p">]</span><span class="o">==</span><span class="s1">&#39;Orientation&#39;</span><span class="p">:</span>
                <span class="k">break</span>
        
        <span class="n">exif</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">_getexif</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">exif</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">exif</span><span class="p">[</span><span class="n">orientation</span><span class="p">]</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
                <span class="n">image</span><span class="o">=</span><span class="n">image</span><span class="o">.</span><span class="n">rotate</span><span class="p">(</span><span class="mi">180</span><span class="p">,</span> <span class="n">expand</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">exif</span><span class="p">[</span><span class="n">orientation</span><span class="p">]</span> <span class="o">==</span> <span class="mi">6</span><span class="p">:</span>
                <span class="n">image</span><span class="o">=</span><span class="n">image</span><span class="o">.</span><span class="n">rotate</span><span class="p">(</span><span class="mi">270</span><span class="p">,</span> <span class="n">expand</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">exif</span><span class="p">[</span><span class="n">orientation</span><span class="p">]</span> <span class="o">==</span> <span class="mi">8</span><span class="p">:</span>
                <span class="n">image</span><span class="o">=</span><span class="n">image</span><span class="o">.</span><span class="n">rotate</span><span class="p">(</span><span class="mi">90</span><span class="p">,</span> <span class="n">expand</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">except</span> <span class="p">(</span><span class="ne">AttributeError</span><span class="p">,</span> <span class="ne">KeyError</span><span class="p">,</span> <span class="ne">IndexError</span><span class="p">):</span>
        <span class="c1"># cases: image don&#39;t have getexif</span>
        <span class="k">pass</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">image</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="s1">&#39;RGB&#39;</span><span class="p">)),</span>  <span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_RGB2BGR</span><span class="p">)</span>
    <span class="n">annIds</span> <span class="o">=</span> <span class="n">coco</span><span class="o">.</span><span class="n">getAnnIds</span><span class="p">(</span><span class="n">imgIds</span><span class="o">=</span><span class="p">[</span><span class="n">imgId</span><span class="p">])</span>
    <span class="n">anns</span> <span class="o">=</span> <span class="n">coco</span><span class="o">.</span><span class="n">loadAnns</span><span class="p">(</span><span class="n">ids</span><span class="o">=</span><span class="n">annIds</span><span class="p">)</span>
    <span class="n">h</span><span class="p">,</span><span class="n">w</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">target_h</span> <span class="o">=</span> <span class="mi">1080</span>
    <span class="n">ratio</span> <span class="o">=</span> <span class="n">target_h</span><span class="o">/</span><span class="n">h</span>
    <span class="n">target_w</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">ratio</span><span class="o">*</span><span class="n">w</span><span class="p">)</span> 
    <span class="n">image</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="n">image</span><span class="p">,(</span><span class="n">target_w</span><span class="p">,</span><span class="n">target_h</span><span class="p">))</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">draw_bbox</span><span class="p">(</span><span class="n">image</span><span class="p">,</span><span class="n">anns</span><span class="p">,</span><span class="n">ratio</span><span class="p">)</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">cvtColor</span><span class="p">(</span><span class="n">image</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">COLOR_BGR2RGB</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="video-sequences">
<h3>Video sequences<a class="headerlink" href="#video-sequences" title="Permalink to this headline">¶</a></h3>
<div class="section" id="id30">
<h4>Data collection<a class="headerlink" href="#id30" title="Permalink to this headline">¶</a></h4>
<p>For evaluation, an on-field study was conducted with 20 volunteers to manually count litter along three different riverbank sections in April 2021, on the Gave d’Oloron near Auterrive (Pyrénées-Atlantiques, France), using kayaks.
The river sections, each 500 meters long, were precisely defined for their differences in background, vegetation, river current, light conditions and accessibility (see <a class="reference internal" href="#video-dataset-appendix"><span class="std std-ref">this section</span></a> for aerial views of the shooting site and details on the river sections).
In total, the three videos amount to 20 minutes of footage at 24 frames per second (fps) and a resolution of 1920x1080 pixels.</p>
</div>
<div class="section" id="track-annotation">
<h4>Track annotation<a class="headerlink" href="#track-annotation" title="Permalink to this headline">¶</a></h4>
<p>On video footage, we manually recovered all visible object trajectories on each river section using an online video annotation tool (more details <a class="reference internal" href="#video-dataset-appendix"><span class="std std-ref">here</span></a> for the precise methodology).
From that, we obtained a collection of distinct object tracks spanning the entire footage.</p>
</div>
</div>
</div>
<div class="section" id="optical-flow-based-counting-via-bayesian-filtering-and-confidence-regions">
<h2>Optical flow-based counting via Bayesian filtering and confidence regions<a class="headerlink" href="#optical-flow-based-counting-via-bayesian-filtering-and-confidence-regions" title="Permalink to this headline">¶</a></h2>
<p>Our counting method is divided into several interacting blocks.
First, a detector outputs a set of predicted positions for objects in the current frame.
The second block is a tracking module designing consistent trajectories of potential objects within the video.
At each frame, a third block links the successive detections together using confidence regions provided by the tracking module, proposing distinct tracks for each object.
A final postprocessing step only keeps the best tracks which are enumerated to yield the final count.</p>
<div class="section" id="detector">
<h3>Detector<a class="headerlink" href="#detector" title="Permalink to this headline">¶</a></h3>
<div class="section" id="center-based-anchor-free-detection">
<h4>Center-based anchor-free detection<a class="headerlink" href="#center-based-anchor-free-detection" title="Permalink to this headline">¶</a></h4>
<p>In most benchmarks, the prediction quality of object attributes like bounding boxes is often used to  improve tracking.
For counting, however, point detection is theoretically enough and advantageous in many ways.
First, to build large datasets, a method which only requires the lightest annotation format may benefit from more data due to annotation ease.
Second, contrary to previous popular methods <span id="id31">[<a class="reference internal" href="#id94" title="Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: towards real-time object detection with region proposal networks. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS'15, 91–99. Cambridge, MA, USA, 2015. MIT Press.">22</a>]</span> involving intricate mechanisms for bounding box prediction, center-based and anchor-free detectors <span id="id32">[<a class="reference internal" href="#id71" title="Xingyi Zhou, Dequan Wang, and Philipp Krähenbühl. Objects as points. apr 2019. URL: http://arxiv.org/abs/1904.07850, arXiv:1904.07850.">34</a>, <a class="reference internal" href="#id86" title="Hei Law and Jia Deng. Cornernet: detecting objects as paired keypoints. In Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss, editors, Computer Vision - ECCV 2018 - 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part XIV, volume 11218 of Lecture Notes in Computer Science, 765–781. Springer, 2018. URL: https://doi.org/10.1007/978-3-030-01264-9\_45, doi:10.1007/978-3-030-01264-9\_45.">35</a>]</span> only use additional regression heads which can simply be removed for point detection.
Adding to all this, <span id="id33">[<a class="reference internal" href="#id74" title="Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng, and Wenyu Liu. Fairmot: on the fairness of detection and re-identification in multiple object tracking. International Journal of Computer Vision, pages 1–19, 2021.">30</a>]</span> highlight conceptual and experimental reasons to favor anchor-free detection in tracking-related tasks.</p>
<p><br />
For these reasons, we use a stripped version of CenterNet <span id="id34">[<a class="reference internal" href="#id71" title="Xingyi Zhou, Dequan Wang, and Philipp Krähenbühl. Objects as points. apr 2019. URL: http://arxiv.org/abs/1904.07850, arXiv:1904.07850.">34</a>]</span> where offset and bounding box regression heads are discarded to output bare estimates of center positions on a coarse grid.
An encoder-decoder network takes an input image <span class="math notranslate nohighlight">\(I \in [0,1]^{w \times h \times 3}\)</span> (an RGB image of width <span class="math notranslate nohighlight">\(w\)</span> and height <span class="math notranslate nohighlight">\(h\)</span>), and produces a heatmap <span class="math notranslate nohighlight">\(\hat{Y} \in [0,1]^{\lfloor w/p\rfloor \times \lfloor h/p\rfloor}\)</span> such that  <span class="math notranslate nohighlight">\(\hat{Y}_{xy}\)</span> is the probability that <span class="math notranslate nohighlight">\((x,y)\)</span> is the center of an object (<span class="math notranslate nohighlight">\(p\)</span> being a stride coefficient).
At inference, peak detection and thresholding are applied to <span class="math notranslate nohighlight">\(\hat{Y}\)</span>, yielding the set of detections.
The bulk of this detector relies on the DLA34 architecture <span id="id35">[<a class="reference internal" href="#id52" title="Fisher Yu, Dequan Wang, Evan Shelhamer, and Trevor Darrell. Deep layer aggregation. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, volume, 2403-2412. 2018. doi:10.1109/CVPR.2018.00255.">36</a>]</span>.
In a video, for each frame <span class="math notranslate nohighlight">\(I_n \in [0,1]^{w \times h \times 3}\)</span> (where <span class="math notranslate nohighlight">\(n\)</span> indexes the frame number), the detector outputs a set <span class="math notranslate nohighlight">\(\detectset_n = \{z_n^i\}_{1 \leq i \leq D_n}\)</span> where each <span class="math notranslate nohighlight">\(z_n^i = (x_n^i,y_n^i)\)</span> specifies the coordinates of one of the <span class="math notranslate nohighlight">\(D_n\)</span> detected objects.</p>
</div>
<div class="section" id="training">
<span id="detector-training"></span><h4>Training<a class="headerlink" href="#training" title="Permalink to this headline">¶</a></h4>
<p>Training the detector is done similarly as in <span id="id36">[<a class="reference internal" href="#id95" title="Pedro F Proença and Pedro Simões. TACO: Trash Annotations in Context for Litter Detection. 2020. URL: http://tacodataset.org/ http://arxiv.org/abs/2003.06975, arXiv:2003.06975.">12</a>]</span>.
For every image, the corresponding set <span class="math notranslate nohighlight">\(\mathcal{B} = \{(c^w_i,c^h_i,w_i,h_i)\}_{1 \leq i\leq B}\)</span> of <span class="math notranslate nohighlight">\(B\)</span> annotated bounding boxes – <em>i.e.</em> a center <span class="math notranslate nohighlight">\((c^w_i,c^h_i)\)</span>, a width <span class="math notranslate nohighlight">\(w_i\)</span> and a height <span class="math notranslate nohighlight">\(h_i\)</span>– is rendered into a ground truth heatmap <span class="math notranslate nohighlight">\(Y \in [0,1]^{{\lfloor w/p\rfloor \times \lfloor h/p\rfloor}}\)</span> by applying kernels at the bounding box centers and taking element-wise maximum.
For all <span class="math notranslate nohighlight">\(1 \leq x \leq w/p\)</span>, <span class="math notranslate nohighlight">\(1 \leq y \leq h/p\)</span>, the ground truth at <span class="math notranslate nohighlight">\((x,y)\)</span> is</p>
<div class="math notranslate nohighlight">
\[
  Y_{xy} =  \max\limits_{1\leq i\leq B}\left(\exp\left\{-\frac{(x-c_i^w)^2+(y-c_i^h)^2}{2\sigma^2_i}\right\}\right),
\]</div>
<p>where <span class="math notranslate nohighlight">\(\sigma_i\)</span> is a parameter depending on the size of the object.
Training the detector is done by minimizing a penalty-reduced weighted focal loss</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\hat{Y},Y) = -\sum_{x,y} \gamma_{xy}^\beta\left(1-\hat{p}_{xy}\right)^\alpha \log{\left(\hat{p}_{xy}\right)},
\]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha\)</span>, <span class="math notranslate nohighlight">\(\beta\)</span> are hyperparameters and</p>
<div class="math notranslate nohighlight">
\[\begin{split}
(\hat{p}_{xy},\gamma_{xy}) = \left\{
    \begin{array}{ll}
        (\hat{Y}_{xy},1) &amp; \mbox{if } Y_{xy} = 1, \\
        (1 - \hat{Y}_{xy},1 - Y_{xy}) &amp; \mbox{otherwise}
    \end{array}
\right.
\end{split}\]</div>
</div>
</div>
<div class="section" id="bayesian-tracking-with-optical-flow">
<span id="bayesian-tracking"></span><h3>Bayesian tracking with optical flow<a class="headerlink" href="#bayesian-tracking-with-optical-flow" title="Permalink to this headline">¶</a></h3>
<div class="section" id="optical-flow">
<h4>Optical flow<a class="headerlink" href="#optical-flow" title="Permalink to this headline">¶</a></h4>
<p>Between two timesteps <span class="math notranslate nohighlight">\(n-1\)</span> and <span class="math notranslate nohighlight">\(n\)</span>, the optical flow <span class="math notranslate nohighlight">\(\Delta_n\)</span> is a mapping  satisfying the following consistency constraint <span id="id37">[<a class="reference internal" href="#id53" title="Nikos Paragios, Yunmei Chen, and Olivier D Faugeras. Handbook of mathematical models in computer vision. Springer Science &amp; Business Media, 2006.">37</a>]</span>:</p>
<div class="math notranslate nohighlight">
\[
\widetilde{I}_n[u] = \widetilde{I}_{n-1}[u+\Delta_n(u)],
\]</div>
<p>where, in our case, <span class="math notranslate nohighlight">\(\widetilde{I}_n\)</span> denotes the frame <span class="math notranslate nohighlight">\(n\)</span> downsampled to dimensions <span class="math notranslate nohighlight">\(\lfloor w/p\rfloor \times \lfloor h/p\rfloor\)</span> and <span class="math notranslate nohighlight">\(u = (x,y)\)</span> is a coordinate on that grid.
To estimate <span class="math notranslate nohighlight">\(\Delta_n\)</span>, we choose a simple unsupervised Gunner-Farneback algorithm which does not require further annotations, see <span id="id38">[<a class="reference internal" href="#id63" title="G. Farnebäck. Two-frame motion estimation based on polynomial expansion. In Scandinavian conference on Image analysis, 363–370. Springer, 2003.">38</a>]</span> for details.</p>
</div>
<div class="section" id="state-space-model">
<h4>State-space model<a class="headerlink" href="#state-space-model" title="Permalink to this headline">¶</a></h4>
<p>Using optical flow as a building block, we posit a state-space model where estimates of <span class="math notranslate nohighlight">\(\Delta_n\)</span> are used as a time and state-dependent offset for the state transition.
Let <span class="math notranslate nohighlight">\((X_k)_{k \geq 1}\)</span> and <span class="math notranslate nohighlight">\((Z_k)_{k \geq 1}\)</span> be the true (but hidden) and observed (detected) positions of a target object in <span class="math notranslate nohighlight">\(\Rset^2\)</span>, respectively.
Considering the optical flow value associated with <span class="math notranslate nohighlight">\(X_{k-1}\)</span> on the discrete grid of dimensions <span class="math notranslate nohighlight">\(\lfloor w/p\rfloor \times \lfloor h/p\rfloor\)</span>, write</p>
<div class="math notranslate nohighlight" id="equation-state-transition-eq">
<span class="eqno">(1)<a class="headerlink" href="#equation-state-transition-eq" title="Permalink to this equation">¶</a></span>\[X_k = X_{k-1} + \Delta_k(\lfloor X_{k-1} \rfloor) + \eta_k\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
Z_k = X_k + \varepsilon_k,
\]</div>
<p>where <span class="math notranslate nohighlight">\((\eta_k)_{k\geq 1}\)</span> are i.i.d. centered Gaussian random variables with covariance matrix <span class="math notranslate nohighlight">\(Q\)</span> independent of <span class="math notranslate nohighlight">\((\varepsilon_k)_{k\geq 1}\)</span> i.i.d. centered Gaussian random variables with covariance matrix <span class="math notranslate nohighlight">\(R\)</span>.
In the following, <span class="math notranslate nohighlight">\(Q\)</span> and <span class="math notranslate nohighlight">\(R\)</span> are assumed to be diagonal, and are hyperparameters set to values given in <a class="reference internal" href="#covariance-matrices"><span class="std std-ref">Covariance matrices for state and observation noises</span></a></p>
</div>
<div class="section" id="approximations-of-the-filtering-distributions">
<h4>Approximations of the filtering distributions<a class="headerlink" href="#approximations-of-the-filtering-distributions" title="Permalink to this headline">¶</a></h4>
<p>Denoting <span class="math notranslate nohighlight">\(u_{1:k} = (u_1,\ldots,u_k)\)</span> for any <span class="math notranslate nohighlight">\(k\)</span> and sequence <span class="math notranslate nohighlight">\((u_i)_{i \geq 0}\)</span>, Bayesian filtering aims at computing the conditional distribution of <span class="math notranslate nohighlight">\(X_k\)</span> given <span class="math notranslate nohighlight">\(Z_{1:k}\)</span>, referred to as the filtering distribution.
In the case of linear and Gaussian state space models, this distribution is known to be Gaussian, and Kalman filtering allows to update exactly the posterior mean <span class="math notranslate nohighlight">\(\mu_k = \esp[X_k|Z_{1:k}]\)</span> and posterior variance matrix <span class="math notranslate nohighlight">\(\Sigma_k = \var[X_k|Z_{1:k}]\)</span>.
This algorithm and its extensions are prevalent and used extensively in time-series and sequential-data analysis.
As the transition model proposed in <a class="reference internal" href="#equation-state-transition-eq">(1)</a> is nonlinear, Kalman updates cannot be implemented and solving the target tracking task requires resorting to alternatives.
Many solutions have been proposed in the literature such as extended Kalman filters (EKF), unscented Kalman filters (UKF) or Sequential Monte Carlo (SMC) methods, see <span id="id39">[<a class="reference internal" href="#id60" title="S. Särkkä. Bayesian Filtering and Smoothing. Cambridge University Press, New York, NY, USA, 2013.">39</a>]</span> and references therein.
Although UKF and SMC have been widely studied, such sample-based solutions are more computationally intensive, especially in settings where many objects have to be tracked and false positive detections involve unnecessary sampling steps. The following EKF approximation of model <a class="reference internal" href="#equation-state-transition-eq">(1)</a> is found to be computationnaly cheaper and as robust for our data:</p>
<div class="math notranslate nohighlight">
\[
X_k = X_{k-1} + \Delta_k(\lfloor \mu_{k-1} \rfloor) + \nabla_x\Delta_k(\lfloor \mu_{k-1} \rfloor)(X_{k-1}-\mu_{k-1}) + \eta_k
\]</div>
<p>This allows the implementation of Kalman updates. For completeness, we present <a class="reference internal" href="#impact-algorithm-appendix"><span class="std std-ref">here</span></a> an SMC-based solution, discuss the empirical differences and use-cases where the latter might be a more relevant choice.
In any case, the state-space model naturally accounts for missing observations, as the contribution of <span class="math notranslate nohighlight">\(\Delta_k\)</span> in every transition ensures that each filter can cope with arbitrary inter-frame motion to keep track of its target.</p>
</div>
<div class="section" id="generating-potential-object-tracks">
<h4>Generating potential object tracks<a class="headerlink" href="#generating-potential-object-tracks" title="Permalink to this headline">¶</a></h4>
<p>The full MOT algorithm consists of a set of single-object trackers following the previous model, but each provided with distinct observations at every frame.
These separate filters provide track proposals for every object detected in the video.</p>
</div>
</div>
<div class="section" id="data-association-using-confidence-regions">
<span id="data-association"></span><h3>Data association using confidence regions<a class="headerlink" href="#data-association-using-confidence-regions" title="Permalink to this headline">¶</a></h3>
<p>Throughout the video, depending on various conditions on the incoming detections, existing trackers must be updated (with or without a new observation) and others might need to be created.
This traditional Multiple Hypothesis Tracking (MHT) setup requires a third party data association block to link the incoming detections with the correct filters.
At the frame <span class="math notranslate nohighlight">\(n\)</span>, a set of <span class="math notranslate nohighlight">\(L_n\)</span> Bayesian filters track previously seen objects and a new set of detections <span class="math notranslate nohighlight">\(\mathcal{D}_n\)</span> is provided by the detector.
Denote by <span class="math notranslate nohighlight">\(1 \leq \ell \leq L_n\)</span> the index of each filter at time <span class="math notranslate nohighlight">\(n\)</span>, and by convention write <span class="math notranslate nohighlight">\(Z^\ell_{1:n-1}\)</span>  the previous observed positions associated with index <span class="math notranslate nohighlight">\(\ell\)</span> (even if no observation is available at some past times for that object).
Let <span class="math notranslate nohighlight">\(\rho \in (0,1)\)</span> be a confidence level.</p>
<ol class="simple">
<li><p>For every detected object <span class="math notranslate nohighlight">\(z_n^i \in \detectset_n\)</span> and every filter <span class="math notranslate nohighlight">\(\ell\)</span>, compute <span class="math notranslate nohighlight">\(P(i,\ell) = \prob(Z_n^\ell \in V_\delta(z_n^i)\mid Z^\ell_{1:n-1})\)</span> where <span class="math notranslate nohighlight">\(V_\delta(z)\)</span> is the neighborhood of <span class="math notranslate nohighlight">\(z\)</span> defined as the squared area of width <span class="math notranslate nohighlight">\(2\delta\)</span> centered on <span class="math notranslate nohighlight">\(z\)</span> (see <a class="reference internal" href="#confidence-regions-appendix"><span class="std std-ref">this appendix</span></a> for exact computations).</p></li>
<li><p>Using the Hungarian algorithm <span id="id40">[<a class="reference internal" href="#id64" title="H. W. Kuhn. The hungarian method for the assignment problem. Naval Research Logistics Quarterly, 2(1-2):83-97, 1955. URL: https://onlinelibrary.wiley.com/doi/abs/10.1002/nav.3800020109, arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1002/nav.3800020109, doi:https://doi.org/10.1002/nav.3800020109.">40</a>]</span>, compute the assignment between detections and filters with <span class="math notranslate nohighlight">\(P\)</span> as cost function, but discarding associations <span class="math notranslate nohighlight">\((i,\ell)\)</span> having <span class="math notranslate nohighlight">\(P(i,\ell) &lt; \rho\)</span>.
Formally, <span class="math notranslate nohighlight">\(\rho\)</span> represents the level of a confidence region centered on detections and we use <span class="math notranslate nohighlight">\(\rho = 0.5\)</span>.
Denote <span class="math notranslate nohighlight">\(a_{\rho}\)</span> the resulting assignment map defined as <span class="math notranslate nohighlight">\(a_{\rho}(i) = \ell\)</span> if <span class="math notranslate nohighlight">\(z_n^i\)</span> was associated with the <span class="math notranslate nohighlight">\(\ell\)</span>-th filter, and <span class="math notranslate nohighlight">\(a_{\rho}(i) = 0\)</span> if <span class="math notranslate nohighlight">\(z_n^i\)</span> was not associated with any filter.</p></li>
<li><p>For <span class="math notranslate nohighlight">\(1 \leq i \leq D_n\)</span>, if <span class="math notranslate nohighlight">\(a_{\rho}(i) = \ell\)</span>, use <span class="math notranslate nohighlight">\(z_n^i\)</span> as a new observation to update the <span class="math notranslate nohighlight">\(\ell\)</span>-th filter.
If <span class="math notranslate nohighlight">\(a_{\rho}(i) = 0\)</span>, create a new filter initialized from the prior distribution, i.e.
sample the true location as a Gaussian random variable with mean <span class="math notranslate nohighlight">\(z_n^i\)</span> and variance <span class="math notranslate nohighlight">\(R\)</span>.</p></li>
<li><p>For all filters <span class="math notranslate nohighlight">\(\ell'\)</span> which were not provided a new observation, update only the predictive law of <span class="math notranslate nohighlight">\(X^{\ell'}_{n}\)</span> given <span class="math notranslate nohighlight">\(Z^{\ell'}_{1:n-1}\)</span>.</p></li>
</ol>
<p>In other words, we seek to associate filters and detections by maximising a global cost built from the predictive distributions of the available filters, but an association is only valid if its corresponding predictive probability is high enough.
Though the Hungarian algorithm is a very popular algorithm in MOT, it is often used with the Euclidean distance or an Intersection-over-Union (IoU) criterion.
Using confidence regions for the distributions of <span class="math notranslate nohighlight">\(Z_n\)</span> given <span class="math notranslate nohighlight">\(Z_{1:(n - 1)}\)</span> instead allows to naturally include uncertainty in the decision process.
Note that we deactivate filters whose posterior mean estimates lie outside the image subspace in <span class="math notranslate nohighlight">\(\Rset^2\)</span>.</p>
<p>A visual depiction of the entire tracking (from detection to final association) is provided below.</p>
<div class="figure align-default" id="diagram">
<img alt="_images/diagram.png" src="_images/diagram.png" />
<p class="caption"><span class="caption-number">Fig. 3 </span><span class="caption-text">Visual representation of the tracking pipeline.</span><a class="headerlink" href="#diagram" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="counting">
<h3>Counting<a class="headerlink" href="#counting" title="Permalink to this headline">¶</a></h3>
<p>At the end of the video, the previous process returns a set of candidate tracks.
For counting purposes, we find that simple heuristics can be further applied to filter out tracks that do not follow actual objects.
More precisely, we observe that tracks of real objects usually contain more (i) observations and (ii) streams of uninterrupted observations.
Denote by <span class="math notranslate nohighlight">\(T_\ell = \left\{n \in \mathbb{N} \mid \exists  z \in \detectset_n,  Z_n^{\ell} = z\right\}\)</span> all timesteps where the <span class="math notranslate nohighlight">\(\ell\)</span>-th object is observed.
To discard false counts according to (i) and (ii), we compute the moving average <span class="math notranslate nohighlight">\(M_\ell^\kappa\)</span> of <span class="math notranslate nohighlight">\(1_{n \in T_\ell}\)</span> defined on <span class="math notranslate nohighlight">\([\![\min(T_\ell), \max(T_\ell)]\!]\)</span> using windows of size <span class="math notranslate nohighlight">\(\kappa\)</span>, and build <span class="math notranslate nohighlight">\(T_\ell^\kappa = \left\{n \in T_\ell \mid M_\ell^\kappa[n] &gt; \nu\right\}\)</span> .
Defining <span class="math notranslate nohighlight">\(\mathcal{N} = \left\{\ell \mid |T_\ell^\kappa| &gt; \tau\right\}\)</span>, the final object count is <span class="math notranslate nohighlight">\(|\mathcal{N}|\)</span>.
We choose <span class="math notranslate nohighlight">\(\nu = 0.6\)</span> while <span class="math notranslate nohighlight">\(\kappa,\tau\)</span> are optimized for best count performance (see <span class="xref myst">here</span> for a more comprehensive study).</p>
</div>
</div>
<div class="section" id="metrics-for-mot-based-counting">
<h2>Metrics for MOT-based counting<a class="headerlink" href="#metrics-for-mot-based-counting" title="Permalink to this headline">¶</a></h2>
<p>Counting in videos using embedded moving cameras is not a common task, and as such it requires a specific evaluation protocol to understand and compare the performance of competing methods.
First, not all MOT metrics are relevant, even if some do provide insights to assist evaluation of count performance.
Second, considering only raw counts on long videos gives little information on which of the final counts effectively arise from well detected objects.</p>
<div class="section" id="count-related-mot-metrics">
<h3>Count-related MOT metrics<a class="headerlink" href="#count-related-mot-metrics" title="Permalink to this headline">¶</a></h3>
<p>Popular MOT benchmarks usually report several sets of metrics such as ClearMOT <span id="id41">[<a class="reference internal" href="#id109" title="Keni Bernardin and Rainer Stiefelhagen. Evaluating multiple object tracking performance: the clear mot metrics. EURASIP Journal on Image and Video Processing, 2008:, 01 2008. doi:10.1155/2008/246309.">41</a>]</span> or IDF1 <span id="id42">[<a class="reference internal" href="#id110" title="Ergys Ristani, Francesco Solera, Roger Zou, Rita Cucchiara, and Carlo Tomasi. Performance measures and a data set for multi-target, multi-camera tracking. In European conference on computer vision, 17–35. Springer, 2016.">42</a>]</span> which can account for different components of tracking performance.
Recently, <span id="id43">[<a class="reference internal" href="#id95" title="Pedro F Proença and Pedro Simões. TACO: Trash Annotations in Context for Litter Detection. 2020. URL: http://tacodataset.org/ http://arxiv.org/abs/2003.06975, arXiv:2003.06975.">12</a>]</span> built the so-called HOTA metrics that allow separate evaluation of detection and association using the Jaccard index.
The following components of their work are relevant to our task (we provide equation numbers in the original paper for formal definitions).</p>
<div class="section" id="detection">
<h4>Detection<a class="headerlink" href="#detection" title="Permalink to this headline">¶</a></h4>
<p>First, when considering all frames independently, traditional detection recall (<span class="math notranslate nohighlight">\(\detre\)</span>, eq.
23) and precision (<span class="math notranslate nohighlight">\(\detpr\)</span>, eq.
24) can be computed to assess the capabilities of the object detector.
In classical object detection, those metrics are the main target.
In our context, as the first step of the system, this framewise performance impacts the difficulty of counting.
However, we must keep in mind that these metrics are computed framewise and might not guarantee anything at a video scale.
The next points illustrate that remark.</p>
<ol class="simple">
<li><p>If both <span class="math notranslate nohighlight">\(\detre\)</span> and <span class="math notranslate nohighlight">\(\detpr\)</span> are very high, objects are detected at nearly all frames and most detections come from actual objects.
Therefore, robustness to missing observations is high, but even in this context computing associations may fail if camera movements are nontrivial.</p></li>
<li><p>For an ideal tracking algorithm which never counts individual objects twice and does not confuse separate objects in a video, a detector capturing each object for only one frame could theoretically be used.
Thus, low <span class="math notranslate nohighlight">\(\detre\)</span> could theoretically be compensated with robust tracking.</p></li>
<li><p>If our approach can rule out faulty tracks which do not follow actual objects, then good counts can still be obtained using a detector generating many false positives.
Again, this suggests that low <span class="math notranslate nohighlight">\(\detpr\)</span> may allow decent counting performance.</p></li>
</ol>
</div>
<div class="section" id="association">
<h4>Association<a class="headerlink" href="#association" title="Permalink to this headline">¶</a></h4>
<p>HOTA association metrics are built to measure tracking performance irrespective of the detection capabilities, by comparing predicted tracks against true object trajectories.
In our experiments, we compute the Association recall (<span class="math notranslate nohighlight">\(\assre\)</span>, eq. 26) and the Association Precision (<span class="math notranslate nohighlight">\(\asspr\)</span>, eq. 27).
In brief, a low <span class="math notranslate nohighlight">\(\asspr\)</span> implies that several objects are often mingled into only one track, resulting in undercount.
A low <span class="math notranslate nohighlight">\(\assre\)</span> implies that single objects are often associated with multiple tracks.
If no method is used to discard redundant tracks this results in overcount.
Conversely, association precision (<span class="math notranslate nohighlight">\(\asspr\)</span>) measures how exclusive tracks are to each object (it decreases whenever a track covers multiple objects).
Again, it is useful to reconsider and illustrate the meaning of these metrics in the context of MOT-based counting.
Litter items are typically well separated on river banks, thus predicted tracks are not expected to interfere much.
This suggests that reaching high <span class="math notranslate nohighlight">\(\asspr\)</span> on our footage is not challenging.
Contrarily, <span class="math notranslate nohighlight">\(\assre\)</span> is a direct measurement of the capability of the tracker to avoid producing multiple tracks despite missing detections and challenging motion.
A high <span class="math notranslate nohighlight">\(\assre\)</span> therefore typically avoids multiple counts for the same object, which is a key aspect of our work.</p>
<p>Nonetheless, association metrics are only computed for predicted tracks which can effectively be matched with ground truth tracks.
Consequently, <span class="math notranslate nohighlight">\(\assre\)</span> does not account for tracks predicted from streams of false positive detections generated by the detector (e.g.
arising from rocks, water reflections, etc).
Since such tracks induce false counts, a tracker which produces the fewest is better, but MOT metrics do not measure it.</p>
</div>
</div>
<div class="section" id="count-metrics">
<h3>Count metrics<a class="headerlink" href="#count-metrics" title="Permalink to this headline">¶</a></h3>
<p>Denoting by <span class="math notranslate nohighlight">\(\hatN\)</span> and <span class="math notranslate nohighlight">\(\N\)</span> the respective predicted and ground truth counts for the validation material, the error <span class="math notranslate nohighlight">\(\hatN - \N\)</span> is misleading as no information is provided on the quality of the predicted counts.
Additionally, results on the original validation footage do not measure the statistical variability of the proposed estimators.</p>
<div class="section" id="count-decomposition">
<h4>Count decomposition<a class="headerlink" href="#count-decomposition" title="Permalink to this headline">¶</a></h4>
<p>Define <span class="math notranslate nohighlight">\(\gtlabels\)</span> and <span class="math notranslate nohighlight">\(\predlabels\)</span> the labels of the annotated ground truth tracks and the predicted tracks, respectively.
At evaluation, we assign each predicted track to either none or at most one ground truth track, writing <span class="math notranslate nohighlight">\(j \rightarrow \emptyset\)</span> or <span class="math notranslate nohighlight">\(j \rightarrow i\)</span> for the corresponding assignments.
Denote <span class="math notranslate nohighlight">\(A_i = \{\predlabels \mid j \rightarrow i\}\)</span> the set of predicted tracks assigned to the <span class="math notranslate nohighlight">\(i\)</span>-th ground truth track.
We define:</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(\Ntrue = \sum_{i=1}^{\N} 1_{|A_i| &gt; 0}\)</span> the number of ground truth objects successfully counted.</p></li>
<li><p><span class="math notranslate nohighlight">\(\Nred = \sum_{i=1}^{\N} |A_i| - \Ntrue\)</span> the number of redundant counts per ground truth object.</p></li>
<li><p><span class="math notranslate nohighlight">\(\Nmis = \N - \Ntrue\)</span> the number of ground truth objects that are never effectively counted.</p></li>
<li><p><span class="math notranslate nohighlight">\(\Nfalse = \sum_{j=1}^{\hatN} 1_{j \rightarrow \emptyset}\)</span> the number of counts which cannot be associated with any ground truth object and are therefore considered as false counts.</p></li>
</ol>
<p>Using these metrics provides a much better understanding of <span class="math notranslate nohighlight">\(\hatN\)</span> as</p>
<div class="math notranslate nohighlight">
\[
\hatN = \Ntrue + \Nred + \Nfalse
\]</div>
<p>while <span class="math notranslate nohighlight">\(\Nmis\)</span> completely summarises the number of undetected objects.
Note that these metrics and the associated decomposition are only defined if the previous assignment between predicted and ground truth tracks can be obtained.
In our case, predicted tracks never overlap with several ground truth tracks (because true objects are well separated), and therefore this assignment is straightforward.</p>
</div>
<div class="section" id="statistics">
<h4>Statistics<a class="headerlink" href="#statistics" title="Permalink to this headline">¶</a></h4>
<p>Since the original validation set comprises only a few unequally long videos, only absolute results are available.
Splitting the original sequences into shorter independent sequences of equal length allows to compute basic statistics.
For any quantity <span class="math notranslate nohighlight">\(\hatN_\bullet\)</span> defined above, we provide <span class="math notranslate nohighlight">\((\hat{\mu}_{\hatN_\bullet}, \hat{\sigma}_{\hatN_\bullet})\)</span> the associated empirical means and standard deviations computed on the set of short sequences.</p>
</div>
</div>
</div>
<div class="section" id="experiments">
<h2>Experiments<a class="headerlink" href="#experiments" title="Permalink to this headline">¶</a></h2>
<p>We denote by <span class="math notranslate nohighlight">\(S_1\)</span>, <span class="math notranslate nohighlight">\(S_2\)</span> and <span class="math notranslate nohighlight">\(S_3\)</span> the three river sections of the evaluation material and split the associated footage into independent segments of 30 seconds.</p>
<p>To demonstrate the benefits of our work, we select two multi-object trackers and build competing counting systems from them. Our first choice is SORT <span id="id44">[<a class="reference internal" href="#id103" title="Alex Bewley, Zongyuan Ge, Lionel Ott, Fabio Ramos, and Ben Upcroft. Simple online and realtime tracking. In Proceedings - International Conference on Image Processing, ICIP, volume 2016-Augus, 3464–3468. 2016. URL: https://github.com/abewley/sort, arXiv:1602.00763, doi:10.1109/ICIP.2016.7533003.">26</a>]</span>, which relies on Kalman filtering with velocity updated using the latest past estimates of object positions.Similar to our system, it only relies on image supervision for training, and though DeepSORT <span id="id45">[<a class="reference internal" href="#id106" title="Nicolai Wojke, Alex Bewley, and Dietrich Paulus. Simple online and realtime tracking with a deep association metric. In Proceedings - International Conference on Image Processing, ICIP, volume 2017-Septe, 3645–3649. 2018. arXiv:1703.07402, doi:10.1109/ICIP.2017.8296962.">29</a>]</span> is a more recent alternative with better performance, the associated deep appearance network cannot be used without additional video annotations. FairMOT <span id="id46">[<a class="reference internal" href="#id74" title="Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng, and Wenyu Liu. Fairmot: on the fairness of detection and re-identification in multiple object tracking. International Journal of Computer Vision, pages 1–19, 2021.">30</a>]</span>, a more recent alternative, is similarly intended for use with video supervision but allows self-supervised training using only an image dataset. Built as a new baseline for MOT, it combines linear constant-velocity Kalman filtering with visual features computed by an additional network branch and extracted at the position of the estimated object centers, as introduced in CenterTrack <span id="id47">[<a class="reference internal" href="#id75" title="Xingyi Zhou, Vladlen Koltun, and Philipp Krähenbühl. Tracking objects as points. pages 474-490, 10 2020. doi:10.1007/978-3-030-58548-8_28.">43</a>]</span>. We choose FairMOT to compare our method to a solution based on deep visual feature extraction.</p>
<p>Similar to our work, FairMOT uses CenterNet for the detection part and the latter is therefore trained as in <a class="reference internal" href="#detector-training"><span class="std std-ref">Training</span></a>. We train it using hyperparameters from the original paper. The detection outputs are then shared between all counting methods, allowing fair comparison of counting performance with a given object detector. We run all experiments at 12fps, an intermediate framerate to capture all objects while reducing the computational burden. Note that both SORT and FairMOT use custom postprocessing methods to filter out implausible tracks, and we leave these mechanisms untouched.</p>
<div class="section" id="id48">
<h3>Detection<a class="headerlink" href="#id48" title="Permalink to this headline">¶</a></h3>
<p>In the following section, we present the performance of the trained detector.
Having annotated all frames of the evaluation videos, we directly compute <span class="math notranslate nohighlight">\(\detre\)</span> and <span class="math notranslate nohighlight">\(\detpr\)</span> on those instead of a test split of the image dataset used for training.
This allows realistic assessment of the detection quality of our system on true videos that may include blurry frames or artifacts caused by strong motion.
We observe low <span class="math notranslate nohighlight">\(\detre\)</span>, suggesting that objects are only captured on a fraction of the frames they appear on.
To better focus on count performance in the next sections, we remove segments that do not generate any correct detection: performance on the remaining footage is increased and given by <span class="math notranslate nohighlight">\(\detre^{*}\)</span> and <span class="math notranslate nohighlight">\(\detpr^{*}\)</span>.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.display</span> <span class="kn">import</span> <span class="n">display</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">indices</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">16</span><span class="p">,</span><span class="mi">19</span><span class="p">,</span><span class="mi">27</span><span class="p">]</span>
<span class="n">fps</span> <span class="o">=</span> <span class="mi">12</span>
<span class="n">fps_str</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">fps</span><span class="si">}</span><span class="s1">fps&#39;</span>

<span class="n">gt_dir_short</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;TrackEval/data/gt/surfrider_short_segments_</span><span class="si">{</span><span class="n">fps_str</span><span class="si">}</span><span class="s1">/surfrider-test&#39;</span> 
<span class="n">eval_dir_short</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;TrackEval/data/trackers/surfrider_short_segments_</span><span class="si">{</span><span class="n">fps_str</span><span class="si">}</span><span class="s1">&#39;</span> 

<span class="n">long_segments_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;part_1_1&#39;</span><span class="p">,</span><span class="s1">&#39;part_1_2&#39;</span><span class="p">,</span><span class="s1">&#39;part_2&#39;</span><span class="p">,</span><span class="s1">&#39;part_3&#39;</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">get_det_values</span><span class="p">(</span><span class="n">index_start</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">index_stop</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>

    <span class="n">results_for_det</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">eval_dir_short</span><span class="p">,</span><span class="s1">&#39;surfrider-test&#39;</span><span class="p">,</span><span class="s1">&#39;ours_EKF_1_12fps_v0_tau_0&#39;</span><span class="p">,</span><span class="s1">&#39;pedestrian_detailed.csv&#39;</span><span class="p">))</span>
    <span class="n">results_det</span> <span class="o">=</span> <span class="n">results_for_det</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,[</span><span class="s1">&#39;DetRe___50&#39;</span><span class="p">,</span><span class="s1">&#39;DetPr___50&#39;</span><span class="p">,</span> <span class="s1">&#39;HOTA_TP___50&#39;</span><span class="p">,</span><span class="s1">&#39;HOTA_FN___50&#39;</span><span class="p">,</span><span class="s1">&#39;HOTA_FP___50&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">index_start</span><span class="p">:</span><span class="n">index_stop</span><span class="p">]</span>
    <span class="n">results_det</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;hota_det_re&#39;</span><span class="p">,</span><span class="s1">&#39;hota_det_pr&#39;</span><span class="p">,</span><span class="s1">&#39;hota_det_tp&#39;</span><span class="p">,</span><span class="s1">&#39;hota_det_fn&#39;</span><span class="p">,</span><span class="s1">&#39;hota_det_fp&#39;</span><span class="p">]</span>
    <span class="n">hota_det_re</span> <span class="o">=</span> <span class="n">results_det</span><span class="p">[</span><span class="s1">&#39;hota_det_re&#39;</span><span class="p">]</span>
    <span class="n">hota_det_pr</span> <span class="o">=</span> <span class="n">results_det</span><span class="p">[</span><span class="s1">&#39;hota_det_pr&#39;</span><span class="p">]</span>
    <span class="n">hota_det_tp</span> <span class="o">=</span> <span class="n">results_det</span><span class="p">[</span><span class="s1">&#39;hota_det_tp&#39;</span><span class="p">]</span>
    <span class="n">hota_det_fn</span> <span class="o">=</span> <span class="n">results_det</span><span class="p">[</span><span class="s1">&#39;hota_det_fn&#39;</span><span class="p">]</span>
    <span class="n">hota_det_fp</span> <span class="o">=</span> <span class="n">results_det</span><span class="p">[</span><span class="s1">&#39;hota_det_fp&#39;</span><span class="p">]</span>

    <span class="n">denom_hota_det_re</span> <span class="o">=</span> <span class="n">hota_det_tp</span> <span class="o">+</span> <span class="n">hota_det_fn</span> 
    <span class="n">denom_hota_det_pr</span> <span class="o">=</span> <span class="n">hota_det_tp</span> <span class="o">+</span> <span class="n">hota_det_fp</span> 

    <span class="n">hota_det_re_cb</span> <span class="o">=</span> <span class="p">(</span><span class="n">hota_det_re</span> <span class="o">*</span> <span class="n">denom_hota_det_re</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">denom_hota_det_re</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="n">hota_det_pr_cb</span> <span class="o">=</span> <span class="p">(</span><span class="n">hota_det_pr</span> <span class="o">*</span> <span class="n">denom_hota_det_pr</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">denom_hota_det_pr</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

    <span class="k">return</span> <span class="p">[</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">hota_det_re_cb</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">hota_det_pr_cb</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">]</span>
    
<span class="k">def</span> <span class="nf">get_table_det</span><span class="p">():</span>

    <span class="n">table_values</span> <span class="o">=</span> <span class="p">[</span><span class="n">get_det_values</span><span class="p">(</span><span class="n">index_start</span><span class="p">,</span> <span class="n">index_stop</span><span class="p">)</span> <span class="k">for</span> <span class="p">(</span><span class="n">index_start</span><span class="p">,</span> <span class="n">index_stop</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">indices</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">indices</span><span class="p">[</span><span class="mi">1</span><span class="p">:])]</span>
    <span class="n">table_values</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">get_det_values</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">table_values</span><span class="p">)</span>

<span class="n">table_det</span> <span class="o">=</span> <span class="n">get_table_det</span><span class="p">()</span>
<span class="n">table_det</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;DetRe*&#39;</span><span class="p">,</span><span class="s1">&#39;DetPr*&#39;</span><span class="p">]</span>
<span class="n">table_det</span><span class="o">.</span><span class="n">index</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;S1&#39;</span><span class="p">,</span><span class="s1">&#39;S2&#39;</span><span class="p">,</span><span class="s1">&#39;S3&#39;</span><span class="p">,</span><span class="s1">&#39;All&#39;</span><span class="p">]</span>
<span class="n">display</span><span class="p">(</span><span class="n">table_det</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="counts">
<h3>Counts<a class="headerlink" href="#counts" title="Permalink to this headline">¶</a></h3>
<p>We now compare our method against FairMOT and SORT with the count-related tracking metrics and count decompositions defined in the previous section.
We run our algorithm with <span class="math notranslate nohighlight">\(\kappa = 7,\tau = 5\)</span>, values obtained after a simple hyperparameter study described in <a class="reference internal" href="#image-dataset-appendix"><span class="std std-ref">Details on the image dataset</span></a>.</p>
<p>Across all videos and all methods, we report <span class="math notranslate nohighlight">\(\asspr\)</span> between 98.6 and 99.2 which shows that this application context is unconcerned with tracks spanning multiple ground truth objects, therefore we do not conduct a more detailed interpretation of <span class="math notranslate nohighlight">\(\asspr\)</span> values.
The remaining and most important results are summarized in the tables below. For detailed visualisation of the results, we also plot the error decompositions for all sequences.
The first row reveals the unreliability of the off-the-shelf self-supervised version of FairMOT (FMOT) as a counting system.
Though being the most recent, the lack of video supervision prevents this method from learning usable visual features.
This results in poor association performance (the lowest <span class="math notranslate nohighlight">\(\assre\)</span> of all methods) and a largely impractical and unstable number of incorrect or redundant counts.
However, we observe for FairMOT that many false tracks only last for one frame.
To mitigate the previous remarks, we apply our own postprocessing with <span class="math notranslate nohighlight">\(\kappa=1,\tau=1\)</span> and refer to the modified procedure as FairMOT* (FMOT*).
Our solution brings substantial improvements by largely decreasing the amount of incorrect and redundant counts.
For all sequences combined, we report values for <span class="math notranslate nohighlight">\(\Nfalse\)</span> and <span class="math notranslate nohighlight">\(\Nred\)</span> respectively 41% and 84% lower than those of SORT (the next best method) and our algorithm even completely removes redundant counts on the footage covering <span class="math notranslate nohighlight">\(S_2\)</span> and <span class="math notranslate nohighlight">\(S_3\)</span>.
Improvements in count quality can be traced back to the robustness of our tracking system with an increase of 12% in <span class="math notranslate nohighlight">\(\assre\)</span> over SORT.
Notably, footage on <span class="math notranslate nohighlight">\(S_2\)</span> (which includes strong motion) is only correctly handled by our method, while competitors show significant drops in <span class="math notranslate nohighlight">\(\assre\)</span>.
It is worth noting that a significant number of objects remain uncounted (<span class="math notranslate nohighlight">\(\Nmis\)</span> is high), a pitfall shared by all methods.
Our counting method also shows a statistically more stable and predictable behaviour.
For almost all metrics of interest, we lower the associated standard deviations by a significant margin, which highlights the improved consistency across the different videos and therefore a greater applicability in real-life situations.
The increased stability of all error types results in more stable overall <span class="math notranslate nohighlight">\(\hatN\)</span>, as we report <span class="math notranslate nohighlight">\(\hat{\sigma}_{\hatN} = 2.4\)</span> for our method against 3.3 and 3.5 for SORT and FairMOT*, respectively.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_summary</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">index_start</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">index_stop</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>

    <span class="n">results</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,[</span><span class="s1">&#39;Correct_IDs___50&#39;</span><span class="p">,</span><span class="s1">&#39;Redundant_IDs___50&#39;</span><span class="p">,</span><span class="s1">&#39;False_IDs___50&#39;</span><span class="p">,</span><span class="s1">&#39;Missing_IDs___50&#39;</span><span class="p">,</span><span class="s1">&#39;Fused_IDs___50&#39;</span><span class="p">,</span> <span class="s1">&#39;GT_IDs&#39;</span><span class="p">,</span><span class="s1">&#39;HOTA_TP___50&#39;</span><span class="p">,</span><span class="s1">&#39;AssRe___50&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">index_start</span><span class="p">:</span><span class="n">index_stop</span><span class="p">]</span>

    <span class="n">results</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;correct&#39;</span><span class="p">,</span><span class="s1">&#39;redundant&#39;</span><span class="p">,</span><span class="s1">&#39;false&#39;</span><span class="p">,</span><span class="s1">&#39;missing&#39;</span><span class="p">,</span><span class="s1">&#39;mingled&#39;</span><span class="p">,</span><span class="s1">&#39;gt&#39;</span><span class="p">,</span><span class="s1">&#39;hota_tp&#39;</span><span class="p">,</span><span class="s1">&#39;ass_re&#39;</span><span class="p">]</span>

    <span class="n">ass_re</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="s1">&#39;ass_re&#39;</span><span class="p">]</span>
    <span class="n">hota_tp</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="s1">&#39;hota_tp&#39;</span><span class="p">]</span>

    <span class="n">ass_re_cb</span> <span class="o">=</span> <span class="p">(</span><span class="n">ass_re</span> <span class="o">*</span> <span class="n">hota_tp</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">hota_tp</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

    <span class="n">redundant</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="s1">&#39;redundant&#39;</span><span class="p">]</span>
    <span class="n">false</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="s1">&#39;false&#39;</span><span class="p">]</span>
    <span class="n">missing</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="s1">&#39;missing&#39;</span><span class="p">]</span>
    <span class="c1"># mingled = results[&#39;mingled&#39;] </span>
    <span class="n">gt</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="s1">&#39;gt&#39;</span><span class="p">]</span>
    <span class="n">count_error</span> <span class="o">=</span> <span class="n">false</span> <span class="o">+</span> <span class="n">redundant</span> <span class="o">-</span> <span class="n">missing</span>

    <span class="n">summary</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
    <span class="n">summary</span><span class="p">[</span><span class="s1">&#39;missing&#39;</span><span class="p">],</span> <span class="n">summary</span><span class="p">[</span><span class="s1">&#39;missing_mean&#39;</span><span class="p">],</span> <span class="n">summary</span><span class="p">[</span><span class="s1">&#39;missing_std&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">missing</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">missing</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">missing</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">&#39;</span>
    <span class="n">summary</span><span class="p">[</span><span class="s1">&#39;false&#39;</span><span class="p">],</span> <span class="n">summary</span><span class="p">[</span><span class="s1">&#39;false_mean&#39;</span><span class="p">],</span> <span class="n">summary</span><span class="p">[</span><span class="s1">&#39;false_std&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">false</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">false</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">false</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">&#39;</span>
    <span class="n">summary</span><span class="p">[</span><span class="s1">&#39;redundant&#39;</span><span class="p">],</span> <span class="n">summary</span><span class="p">[</span><span class="s1">&#39;redundant_mean&#39;</span><span class="p">],</span> <span class="n">summary</span><span class="p">[</span><span class="s1">&#39;redundant_std&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">redundant</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">redundant</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">redundant</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">&#39;</span>
    <span class="n">summary</span><span class="p">[</span><span class="s1">&#39;ass_re_cb&#39;</span><span class="p">],</span> <span class="n">summary</span><span class="p">[</span><span class="s1">&#39;ass_re_mean&#39;</span><span class="p">],</span> <span class="n">summary</span><span class="p">[</span><span class="s1">&#39;ass_re_std&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">ass_re_cb</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">ass_re</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="mi">100</span><span class="o">*</span><span class="n">ass_re</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">&#39;</span>
    <span class="n">summary</span><span class="p">[</span><span class="s1">&#39;gt&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">gt</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="si">}</span><span class="s1">&#39;</span>
    <span class="n">summary</span><span class="p">[</span><span class="s1">&#39;count_error&#39;</span><span class="p">],</span> <span class="n">summary</span><span class="p">[</span><span class="s1">&#39;count_error_mean&#39;</span><span class="p">],</span> <span class="n">summary</span><span class="p">[</span><span class="s1">&#39;count_error_std&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">count_error</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">count_error</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">count_error</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="si">:</span><span class="s1">.1f</span><span class="si">}</span><span class="s1">&#39;</span>

    <span class="k">return</span> <span class="n">summary</span> 


<span class="k">def</span> <span class="nf">get_summaries</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">sequence_names</span><span class="p">):</span>

    <span class="n">summaries</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>

    <span class="k">for</span> <span class="p">(</span><span class="n">sequence_name</span><span class="p">,</span> <span class="n">index_start</span><span class="p">,</span> <span class="n">index_stop</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">sequence_names</span><span class="p">,</span> <span class="n">indices</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">indices</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>

        <span class="n">summaries</span><span class="p">[</span><span class="n">sequence_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">get_summary</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">index_start</span><span class="p">,</span> <span class="n">index_stop</span><span class="p">)</span>
    
    <span class="n">summaries</span><span class="p">[</span><span class="s1">&#39;All&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">get_summary</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">summaries</span>



<span class="n">summaries</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">tracker_name</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;fairmot_cleaned&#39;</span><span class="p">,</span> <span class="s1">&#39;sort&#39;</span><span class="p">,</span><span class="s1">&#39;ours_EKF_1_12fps_v2_7_tau_5&#39;</span><span class="p">]:</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">eval_dir_short</span><span class="p">,</span><span class="s1">&#39;surfrider-test&#39;</span><span class="p">,</span><span class="n">tracker_name</span><span class="p">,</span><span class="s1">&#39;pedestrian_detailed.csv&#39;</span><span class="p">))</span>
    <span class="n">sequence_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;S1&#39;</span><span class="p">,</span><span class="s1">&#39;S2&#39;</span><span class="p">,</span><span class="s1">&#39;S3&#39;</span><span class="p">]</span>
    <span class="n">summaries</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">get_summaries</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">sequence_names</span><span class="p">))[:</span><span class="mi">10</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>


<span class="n">fairmot_star</span><span class="p">,</span> <span class="n">sort</span><span class="p">,</span> <span class="n">ours</span> <span class="o">=</span> <span class="n">summaries</span>

<span class="n">nmis</span> <span class="o">=</span> <span class="s1">&#39;$\hat{\mathsf</span><span class="si">{N}</span><span class="s1">}_</span><span class="si">{mis}</span><span class="s1">$&#39;</span>
<span class="n">mu_nmis</span> <span class="o">=</span><span class="s1">&#39;$\hat{\mu}_{\hat{\mathsf</span><span class="si">{N}</span><span class="s1">}_</span><span class="si">{mis}</span><span class="s1">}$&#39;</span>
<span class="n">sigma_nmis</span> <span class="o">=</span> <span class="s1">&#39;$\hat{\sigma}_{\hat{\mathsf</span><span class="si">{N}</span><span class="s1">}_</span><span class="si">{mis}</span><span class="s1">}$&#39;</span>

<span class="n">nfalse</span> <span class="o">=</span> <span class="s1">&#39;$\hat{\mathsf</span><span class="si">{N}</span><span class="s1">}_</span><span class="si">{false}</span><span class="s1">$&#39;</span>
<span class="n">mu_nfalse</span> <span class="o">=</span> <span class="s1">&#39;$\hat{\mu}_{\hat{\mathsf</span><span class="si">{N}</span><span class="s1">}_</span><span class="si">{false}</span><span class="s1">}$&#39;</span>
<span class="n">sigma_nfalse</span> <span class="o">=</span> <span class="s1">&#39;$\hat{\sigma}_{\hat{\mathsf</span><span class="si">{N}</span><span class="s1">}_</span><span class="si">{false}</span><span class="s1">}$&#39;</span>

<span class="n">nred</span> <span class="o">=</span>  <span class="s1">&#39;$\hat{\mathsf</span><span class="si">{N}</span><span class="s1">}_</span><span class="si">{red}</span><span class="s1">$&#39;</span>
<span class="n">mu_nred</span> <span class="o">=</span> <span class="s1">&#39;$\hat{\mu}_{\hat{\mathsf</span><span class="si">{N}</span><span class="s1">}_</span><span class="si">{red}</span><span class="s1">}$&#39;</span>
<span class="n">sigma_nred</span> <span class="o">=</span> <span class="s1">&#39;$\hat{\sigma}_{\hat{\mathsf</span><span class="si">{N}</span><span class="s1">}_</span><span class="si">{red}</span><span class="s1">}$&#39;</span>
<span class="n">ass_re</span> <span class="o">=</span> <span class="s1">&#39;$\mathsf</span><span class="si">{AssRe}</span><span class="s1">$&#39;</span>
<span class="n">rows</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;S1&#39;</span><span class="p">,</span><span class="s1">&#39;S2&#39;</span><span class="p">,</span><span class="s1">&#39;S3&#39;</span><span class="p">,</span><span class="s1">&#39;All&#39;</span><span class="p">]</span>
<span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;nmis&#39;</span><span class="p">,</span> <span class="s1">&#39;mu_nmis&#39;</span><span class="p">,</span> <span class="s1">&#39;sigma_nmis&#39;</span><span class="p">,</span> <span class="s1">&#39;nfalse&#39;</span><span class="p">,</span> <span class="s1">&#39;mu_nfalse&#39;</span><span class="p">,</span> <span class="s1">&#39;sigma_nfalse&#39;</span><span class="p">,</span> <span class="s1">&#39;nred&#39;</span><span class="p">,</span> <span class="s1">&#39;mu_nred&#39;</span><span class="p">,</span> <span class="s1">&#39;sigma_nred&#39;</span><span class="p">,</span> <span class="s1">&#39;ass_re&#39;</span><span class="p">]</span>

<span class="k">for</span> <span class="n">summary</span> <span class="ow">in</span> <span class="n">summaries</span><span class="p">:</span>    
    <span class="n">summary</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="n">columns</span>

<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">summary</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([</span><span class="s1">&#39;fairmot_star&#39;</span><span class="p">,</span><span class="s1">&#39;sort&#39;</span><span class="p">,</span><span class="s1">&#39;ours&#39;</span><span class="p">],</span> <span class="n">summaries</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">rows</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">columns</span><span class="p">:</span> 
            <span class="n">glue</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s1">_</span><span class="si">{</span><span class="n">row</span><span class="si">}</span><span class="s1">_</span><span class="si">{</span><span class="n">col</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">summary</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">row</span><span class="p">,</span><span class="n">col</span><span class="p">],</span> <span class="n">display</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:left head"><p></p></th>
<th class="text-align:right head"><p><span class="math notranslate nohighlight">\(\hat{\mathsf{N}}_{mis}\)</span></p></th>
<th class="text-align:right head"><p><span class="math notranslate nohighlight">\(\hat{\mu}_{\hat{\mathsf{N}}_{mis}}\)</span></p></th>
<th class="text-align:right head"><p><span class="math notranslate nohighlight">\(\hat{\sigma}_{\hat{\mathsf{N}}_{mis}}\)</span></p></th>
<th class="text-align:right head"><p><span class="math notranslate nohighlight">\(\hat{\mathsf{N}}_{false}\)</span></p></th>
<th class="text-align:right head"><p><span class="math notranslate nohighlight">\(\hat{\mu}_{\hat{\mathsf{N}}_{false}}\)</span></p></th>
<th class="text-align:right head"><p><span class="math notranslate nohighlight">\(\hat{\sigma}_{\hat{\mathsf{N}}_{false}}\)</span></p></th>
<th class="text-align:right head"><p><span class="math notranslate nohighlight">\(\hat{\mathsf{N}}_{red}\)</span></p></th>
<th class="text-align:right head"><p><span class="math notranslate nohighlight">\(\hat{\mu}_{\hat{\mathsf{N}}_{red}}\)</span></p></th>
<th class="text-align:right head"><p><span class="math notranslate nohighlight">\(\hat{\sigma}_{\hat{\mathsf{N}}_{red}}\)</span></p></th>
<th class="text-align:right head"><p><span class="math notranslate nohighlight">\(\mathsf{AssRe}\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p>S1</p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>S2</p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>S3</p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>All</p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
</tr>
</tbody>
</table>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:left head"><p></p></th>
<th class="text-align:right head"><p><span class="math notranslate nohighlight">\(\hat{\mathsf{N}}_{mis}\)</span></p></th>
<th class="text-align:right head"><p><span class="math notranslate nohighlight">\(\hat{\mu}_{\hat{\mathsf{N}}_{mis}}\)</span></p></th>
<th class="text-align:right head"><p><span class="math notranslate nohighlight">\(\hat{\sigma}_{\hat{\mathsf{N}}_{mis}}\)</span></p></th>
<th class="text-align:right head"><p><span class="math notranslate nohighlight">\(\hat{\mathsf{N}}_{false}\)</span></p></th>
<th class="text-align:right head"><p><span class="math notranslate nohighlight">\(\hat{\mu}_{\hat{\mathsf{N}}_{false}}\)</span></p></th>
<th class="text-align:right head"><p><span class="math notranslate nohighlight">\(\hat{\sigma}_{\hat{\mathsf{N}}_{false}}\)</span></p></th>
<th class="text-align:right head"><p><span class="math notranslate nohighlight">\(\hat{\mathsf{N}}_{red}\)</span></p></th>
<th class="text-align:right head"><p><span class="math notranslate nohighlight">\(\hat{\mu}_{\hat{\mathsf{N}}_{red}}\)</span></p></th>
<th class="text-align:right head"><p><span class="math notranslate nohighlight">\(\hat{\sigma}_{\hat{\mathsf{N}}_{red}}\)</span></p></th>
<th class="text-align:right head"><p><span class="math notranslate nohighlight">\(\mathsf{AssRe}\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p>S1</p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>S2</p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>S3</p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>All</p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
</tr>
</tbody>
</table>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:left head"><p></p></th>
<th class="text-align:right head"><p><span class="math notranslate nohighlight">\(\hat{\mathsf{N}}_{mis}\)</span></p></th>
<th class="text-align:right head"><p><span class="math notranslate nohighlight">\(\hat{\mu}_{\hat{\mathsf{N}}_{mis}}\)</span></p></th>
<th class="text-align:right head"><p><span class="math notranslate nohighlight">\(\hat{\sigma}_{\hat{\mathsf{N}}_{mis}}\)</span></p></th>
<th class="text-align:right head"><p><span class="math notranslate nohighlight">\(\hat{\mathsf{N}}_{false}\)</span></p></th>
<th class="text-align:right head"><p><span class="math notranslate nohighlight">\(\hat{\mu}_{\hat{\mathsf{N}}_{false}}\)</span></p></th>
<th class="text-align:right head"><p><span class="math notranslate nohighlight">\(\hat{\sigma}_{\hat{\mathsf{N}}_{false}}\)</span></p></th>
<th class="text-align:right head"><p><span class="math notranslate nohighlight">\(\hat{\mathsf{N}}_{red}\)</span></p></th>
<th class="text-align:right head"><p><span class="math notranslate nohighlight">\(\hat{\mu}_{\hat{\mathsf{N}}_{red}}\)</span></p></th>
<th class="text-align:right head"><p><span class="math notranslate nohighlight">\(\hat{\sigma}_{\hat{\mathsf{N}}_{red}}\)</span></p></th>
<th class="text-align:right head"><p><span class="math notranslate nohighlight">\(\mathsf{AssRe}\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p>S1</p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>S2</p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p>S3</p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>All</p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
<td class="text-align:right"><p></p></td>
</tr>
</tbody>
</table>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">title</span><span class="p">,</span> <span class="n">tracker_name</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;FairMOT*&#39;</span><span class="p">,</span><span class="s1">&#39;SORT&#39;</span><span class="p">,</span><span class="s1">&#39;Ours&#39;</span><span class="p">]</span> <span class="p">,[</span><span class="s1">&#39;fairmot_cleaned&#39;</span><span class="p">,</span> <span class="s1">&#39;sort&#39;</span><span class="p">,</span> <span class="s1">&#39;ours_EKF_1_12fps_v2_7_tau_5&#39;</span><span class="p">]):</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">eval_dir_short</span><span class="p">,</span><span class="s1">&#39;surfrider-test&#39;</span><span class="p">,</span><span class="n">tracker_name</span><span class="p">,</span><span class="s1">&#39;pedestrian_detailed.csv&#39;</span><span class="p">))</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,[</span><span class="s1">&#39;Redundant_IDs___50&#39;</span><span class="p">,</span><span class="s1">&#39;False_IDs___50&#39;</span><span class="p">,</span><span class="s1">&#39;Missing_IDs___50&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">results</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;redundant&#39;</span><span class="p">,</span> <span class="s1">&#39;false&#39;</span><span class="p">,</span> <span class="s1">&#39;missing&#39;</span><span class="p">]</span>
    <span class="n">results</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span><span class="s1">&#39;missing&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span> <span class="n">results</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span><span class="s1">&#39;missing&#39;</span><span class="p">]</span>
    <span class="n">results</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;$\hat{\mathsf</span><span class="si">{N}</span><span class="s1">}_</span><span class="si">{red}</span><span class="s1">$&#39;</span><span class="p">,</span> <span class="s1">&#39;$\hat{\mathsf</span><span class="si">{N}</span><span class="s1">}_</span><span class="si">{false}</span><span class="s1">$&#39;</span><span class="p">,</span> <span class="s1">&#39;$-\hat{\mathsf</span><span class="si">{N}</span><span class="s1">}_</span><span class="si">{mis}</span><span class="s1">$&#39;</span><span class="p">]</span>
    <span class="n">results</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span> <span class="o">=</span> <span class="n">ax</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s1">&#39;bar&#39;</span><span class="p">,</span> <span class="n">stacked</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="s1">&#39;black&#39;</span><span class="p">],</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="n">title</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;Sequence nb&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<!-- ```{glue:} fairmot_star_tracking_results
``` --></div>
</div>
<div class="section" id="practical-impact-and-future-goals">
<h2>Practical impact and future goals<a class="headerlink" href="#practical-impact-and-future-goals" title="Permalink to this headline">¶</a></h2>
<p>We successfully tackled video object counting on river banks, in particular issues which could be adressed independently of detection quality.
Moreover the methodology developed to assess count quality enables us to precisely highlight the challenges that pertain to video object counting on river banks.
Conducted in coordination with Sufrider Foundation Europe, an NGO specialized on water preservation, our work marks an important milestone in a broader campaign for macrolitter monitoring and is already being used in a production version of a monitoring system.
That said, large amounts of litter items are still not detected.
Solving this problem is largely a question of augmenting the object detector training dataset through crowdsourced images.
A <a class="reference external" href="https://www.trashroulette.com">specific annotation platform</a> is online, thus the amount of annotated images is expected to continuously increase, while training is provided to volunteers collecting data on the field to ensure data quality.
Finally, several expeditions on different rivers are already underway and new video footage is expected to be annotated in the near future for better evaluation.
All data is made freely available.
Future goals include downsizing the algorithm, a possibility given the architectural simplicity of anchor-free detection and the relatively low computational complexity of EKF.
In a citizen science perspective, a fully embedded version for portable devices will allow a larger deployment.
The resulting field data will help better understand litter origin, allowing to model and predict litter density in non surveyed areas.
Correlations between macro litter density and environmental parameters will be studied (e.g., population density, catchment size, land use and hydromorphology).
Finally, our work naturally benefits any extension of macrolitter monitoring in other areas (urban, coastal, etc) that may rely on a similar setup of moving cameras.</p>
</div>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="supplements">
<h1>Supplements<a class="headerlink" href="#supplements" title="Permalink to this headline">¶</a></h1>
<div class="section" id="details-on-the-image-dataset">
<span id="image-dataset-appendix"></span><h2>Details on the image dataset<a class="headerlink" href="#details-on-the-image-dataset" title="Permalink to this headline">¶</a></h2>
<div class="section" id="categories">
<h3>Categories<a class="headerlink" href="#categories" title="Permalink to this headline">¶</a></h3>
<p>In this work, we do not seek to precisely predict the proportions of the different types of counted litter.
However, we build our dataset to allow classification tasks.
Though litter classifications built by experts already exist, most are based on semantic rather than visual features and do not particularily consider the problem of class imbalance, which makes statistical learning more delicate.
In conjunction with water pollution experts, we therefore define a custom macrolitter taxonomy which balances annotation ease and pragmatic decisions for computer vision applications.
This classification, depicted in <a class="reference internal" href="#trash-categories-image"><span class="std std-numref">Fig. 4</span></a> can be understood as follows.</p>
<ol class="simple">
<li><p>We define a set of frequently observed classes that annotaters can choose from, divided into:</p>
<ul class="simple">
<li><p>Classes for rigid and easily recognisable items which are often observed and have definite shapes</p></li>
<li><p>Classes for fragmented objects which are often found along river banks but whose aspects are more varied</p></li>
</ul>
</li>
<li><p>We define two supplementary categories used whenever the annotater cannot classify the item they are observing in an image using classes given in 1.</p>
<ul class="simple">
<li><p>A first category is used whenever the item is clearly identifiable but its class is not proposed.
This will ensure that our classification can be improved in the future, as images with items in this category will be checked regularly to decide whether a new class needs to be created</p></li>
<li><p>Another category is used whenever the annotater doesn’t understand the item they are seeing.
Images containing items denoted as such will not be used for applications involving classifiction.</p></li>
</ul>
</li>
</ol>
<div class="figure align-default" id="trash-categories-image">
<a class="reference internal image-reference" href="_images/trash_categories.png"><img alt="_images/trash_categories.png" src="_images/trash_categories.png" style="height: 400px;" /></a>
<p class="caption"><span class="caption-number">Fig. 4 </span><span class="caption-text">Trash categories defined to facilitate porting to a counting system that allows trash identification</span><a class="headerlink" href="#trash-categories-image" title="Permalink to this image">¶</a></p>
</div>
</div>
</div>
<div class="section" id="details-on-the-evaluation-videos">
<span id="video-dataset-appendix"></span><h2>Details on the evaluation videos<a class="headerlink" href="#details-on-the-evaluation-videos" title="Permalink to this headline">¶</a></h2>
<div class="section" id="river-segments">
<h3>River segments<a class="headerlink" href="#river-segments" title="Permalink to this headline">¶</a></h3>
<p>In this section, we provide further details on the evaluation material.
<a class="reference internal" href="#river-sections"><span class="std std-numref">Fig. 5</span></a> shows the setup and positioning of the three river segments <span class="math notranslate nohighlight">\(S_1\)</span>, <span class="math notranslate nohighlight">\(S_2\)</span> and <span class="math notranslate nohighlight">\(S_3\)</span> used to evaluate the methods.
The segments differ in the following aspects.</p>
<ul class="simple">
<li><p>Segment 1: Medium current, high and dense vegetation not obstructing vision of the right riverbank from watercrafts, extra objects installed before the field experiment.</p></li>
<li><p>Segment 2: High current, low and dense vegetation obstructing vision of the right riverbank from watercrafts.</p></li>
<li><p>Segment 3: Medium current, high and little vegetation not obstructing vision  of the left riverbank from watercrafts.</p></li>
</ul>
<div class="figure align-default" id="river-sections">
<a class="reference internal image-reference" href="_images/river_sections.png"><img alt="_images/river_sections.png" src="_images/river_sections.png" style="height: 400px;" /></a>
<p class="caption"><span class="caption-number">Fig. 5 </span><span class="caption-text">Aerial view of the three river segments of the evaluation material</span><a class="headerlink" href="#river-sections" title="Permalink to this image">¶</a></p>
</div>
</div>
<div class="section" id="track-annotation-protocol">
<h3>Track annotation protocol<a class="headerlink" href="#track-annotation-protocol" title="Permalink to this headline">¶</a></h3>
<p>To annotate tracks on the evaluation sequences, we used the online tool “CVAT” which allows to locate bounding boxes on video frames and propagate them in time.
The following items provide further details on the exact annotation process.</p>
<ul class="simple">
<li><p>Object tracks start whenever a litter item becomes fully visible and identifiable by the naked eye.</p></li>
<li><p>Positions and sizes of objects are given at nearly every second of the video with automatic interpolation for frames in-between: this yields clean tracks with precise positions at 24fps.</p></li>
<li><p>We do not provide inferred locations when an object is fully occluded, but tracks restart with the same identity whenever the object becomes visible again.</p></li>
<li><p>Tracks stop whenever an object becomes indistinguishable and will not reappear again.</p></li>
</ul>
</div>
</div>
<div class="section" id="implementation-details-for-the-tracking-module">
<span id="tracking-module-appendix"></span><h2>Implementation details for the tracking module<a class="headerlink" href="#implementation-details-for-the-tracking-module" title="Permalink to this headline">¶</a></h2>
<div class="section" id="covariance-matrices-for-state-and-observation-noises">
<span id="covariance-matrices"></span><h3>Covariance matrices for state and observation noises<a class="headerlink" href="#covariance-matrices-for-state-and-observation-noises" title="Permalink to this headline">¶</a></h3>
<p>In our state-space model, <span class="math notranslate nohighlight">\(Q\)</span> models the noise associated to the movement model we posit in <a class="reference internal" href="#bayesian-tracking"><span class="std std-ref">Bayesian tracking with optical flow</span></a> involving optical flow estimates, while <span class="math notranslate nohighlight">\(R\)</span> models the noise associated to the observation of the true position via our object detector.
An attempt to estimate the diagonal values of these matrices was the following.</p>
<ul class="simple">
<li><p>To estimate <span class="math notranslate nohighlight">\(R\)</span>, we computed a mean <span class="math notranslate nohighlight">\(L_2\)</span> error between the known positions of objects and the associated predictions by the object detector, for images in our training dataset.</p></li>
<li><p>To estimate <span class="math notranslate nohighlight">\(Q\)</span>, we built a small synthetic dataset of consecutive frames taken from videos, where positions of objects in two consecutive frames are known.
We computed a mean <span class="math notranslate nohighlight">\(L_2\)</span> error between the known positions in the second frame and the positions estimated by shifting the positions in the first frame with the estimated optical flow values.</p></li>
</ul>
<p>This led to <span class="math notranslate nohighlight">\(R_{00} = R_{11} = 1.1\)</span>, <span class="math notranslate nohighlight">\(Q_{00} = 4.7\)</span> and <span class="math notranslate nohighlight">\(Q_{11} = 0.9\)</span>, for grids of dimensions <span class="math notranslate nohighlight">\(\lfloor w/p\rfloor \times \lfloor h/p\rfloor = 480 \times 270\)</span>.
All other coefficients were not estimated and supposed to be 0.</p>
<p>An important remark is that though we use these values in practise, we found that tracking results are largely unaffected by small variations of <span class="math notranslate nohighlight">\(R\)</span> and <span class="math notranslate nohighlight">\(Q\)</span>.
As long as values are meaningful relative to the image dimensions and the size of the objects, most noise levels show relatively similar performance.</p>
</div>
<div class="section" id="computing-the-confidence-regions">
<span id="confidence-regions-appendix"></span><h3>Computing the confidence regions<a class="headerlink" href="#computing-the-confidence-regions" title="Permalink to this headline">¶</a></h3>
<p>In words, <span class="math notranslate nohighlight">\(P(i,\ell)\)</span> is the mass in <span class="math notranslate nohighlight">\(V_\delta(z_n^i) \subset \Rset^2\)</span> of the probability distribution of <span class="math notranslate nohighlight">\(Z_n^\ell\)</span> given <span class="math notranslate nohighlight">\(Z_{1:n-1}^\ell\)</span>.
Denote with <span class="math notranslate nohighlight">\(\predictdist_n^\ell\)</span> this distribution.
When EKF is used, <span class="math notranslate nohighlight">\(\predictdist_n^\ell\)</span> is a multivariate Gaussian with mean <span class="math notranslate nohighlight">\(\mathbb{E}[Z_k^\ell|Z_{1:k-1}^\ell]\)</span> and covariance matrix <span class="math notranslate nohighlight">\(\mathbb{V}[Z_k^\ell|Z_{1:k-1}^\ell]\)</span>.
In <span class="math notranslate nohighlight">\(\Rset^2\)</span>, values of the cumulative distribution function (cdf) of a multivariate Gaussian distribution are easy to compute.
Denote with <span class="math notranslate nohighlight">\(F_n^\ell\)</span> the cdf of <span class="math notranslate nohighlight">\(\predictdist_n^\ell\)</span>.
If <span class="math notranslate nohighlight">\(V_\delta(z)\)</span> is a squared neighborhood of size <span class="math notranslate nohighlight">\(\delta\)</span> and centered on <span class="math notranslate nohighlight">\(z=(x,y) \in \Rset^2\)</span>, then</p>
<div class="math notranslate nohighlight">
\[
\predictdist_n^{\ell}(V_\delta(z)) = F_n^\ell(x+\delta,y+\delta) + F_n^\ell(x-\delta,y-\delta) - \left[F_n^\ell(x+\delta,y-\delta) + F_n^\ell(x-\delta,y+\delta)\right]
\]</div>
<p>This allows easy computation of <span class="math notranslate nohighlight">\(P(i,\ell) = \predictdist_n^\ell(V_\delta(z_n^i))\)</span>.</p>
</div>
<div class="section" id="influence-of-tau-and-kappa">
<span id="tau-kappy-appendix"></span><h3>Influence of <span class="math notranslate nohighlight">\(\tau\)</span> and <span class="math notranslate nohighlight">\(\kappa\)</span><a class="headerlink" href="#influence-of-tau-and-kappa" title="Permalink to this headline">¶</a></h3>
<p>An understanding of <span class="math notranslate nohighlight">\(\kappa\)</span>, <span class="math notranslate nohighlight">\(\tau\)</span> and <span class="math notranslate nohighlight">\(\nu\)</span> can be stated as follows.
For any track, given a value for <span class="math notranslate nohighlight">\(\kappa\)</span> and <span class="math notranslate nohighlight">\(\nu\)</span>, an observation at time <span class="math notranslate nohighlight">\(n\)</span> is only kept if there are also <span class="math notranslate nohighlight">\(\nu \cdot \kappa\)</span> observations in the temporal window of size <span class="math notranslate nohighlight">\(\kappa\)</span> that surrounds <span class="math notranslate nohighlight">\(n\)</span> (windows are centered around <span class="math notranslate nohighlight">\(n\)</span> except at the start and end of the track).
The track is only counted if the remaining number of observations is strictly higher than <span class="math notranslate nohighlight">\(\tau\)</span>.
At a given <span class="math notranslate nohighlight">\(\nu &gt; 0.5\)</span>, <span class="math notranslate nohighlight">\(\kappa\)</span> and <span class="math notranslate nohighlight">\(\tau\)</span> must be chosen jointly to decrease <span class="math notranslate nohighlight">\(\Nfalse\)</span> as much as possible without increasing <span class="math notranslate nohighlight">\(\Nmis\)</span> (true objects become uncounted if tracks are discarded too easily).</p>
<p>In the following code cell, we plot the error decomposition of the counts for several values of <span class="math notranslate nohighlight">\(\kappa\)</span> and <span class="math notranslate nohighlight">\(\tau\)</span> with <span class="math notranslate nohighlight">\(\nu=0.6\)</span>.
For all values of <span class="math notranslate nohighlight">\(\tau\)</span>, increasing <span class="math notranslate nohighlight">\(\kappa\)</span> decreases <span class="math notranslate nohighlight">\(\Nfalse\)</span>, but a low <span class="math notranslate nohighlight">\(\Nmis\)</span> at the highest <span class="math notranslate nohighlight">\(\kappa = 7\)</span> is only preserved for <span class="math notranslate nohighlight">\(\tau=5\)</span>, which motivates our choice of <span class="math notranslate nohighlight">\((\kappa, \tau) = (7,5)\)</span>.
As a byproduct, this choice positively affects <span class="math notranslate nohighlight">\(\Nred\)</span>.</p>
<div class="cell tag_hide-input docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tau_values</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">10</span><span class="p">)]</span>
<span class="n">versions</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;v0&#39;</span><span class="p">,</span><span class="s1">&#39;v2_3&#39;</span><span class="p">,</span><span class="s1">&#39;v2_5&#39;</span><span class="p">,</span><span class="s1">&#39;v2_7&#39;</span><span class="p">]</span>
<span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax0</span><span class="p">,</span> <span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

<span class="k">for</span> <span class="n">version</span> <span class="ow">in</span> <span class="n">versions</span><span class="p">:</span>
    <span class="n">tracker_names</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s1">&#39;ours_EKF_1_12fps_</span><span class="si">{</span><span class="n">version</span><span class="si">}</span><span class="s1">_tau_</span><span class="si">{</span><span class="n">tau</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">tau</span> <span class="ow">in</span> <span class="n">tau_values</span><span class="p">]</span>
    <span class="n">all_results</span> <span class="o">=</span> <span class="p">{</span><span class="n">tracker_name</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">eval_dir_short</span><span class="p">,</span><span class="s1">&#39;surfrider-test&#39;</span><span class="p">,</span><span class="n">tracker_name</span><span class="p">,</span><span class="s1">&#39;pedestrian_detailed.csv&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">iloc</span><span class="p">[[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span> <span class="k">for</span> <span class="n">tracker_name</span> <span class="ow">in</span> <span class="n">tracker_names</span><span class="p">}</span>

    <span class="n">n_missing</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">n_false</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">n_redundant</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">tracker_name</span><span class="p">,</span> <span class="n">tracker_results</span> <span class="ow">in</span> <span class="n">all_results</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">missing</span> <span class="o">=</span> <span class="p">(</span><span class="n">tracker_results</span><span class="p">[</span><span class="s1">&#39;GT_IDs&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="n">tracker_results</span><span class="p">[</span><span class="s1">&#39;Correct_IDs___50&#39;</span><span class="p">])[</span><span class="mi">27</span><span class="p">]</span>
        <span class="n">false</span> <span class="o">=</span> <span class="n">tracker_results</span><span class="p">[</span><span class="s1">&#39;False_IDs___50&#39;</span><span class="p">][</span><span class="mi">27</span><span class="p">]</span>
        <span class="n">redundant</span> <span class="o">=</span> <span class="n">tracker_results</span><span class="p">[</span><span class="s1">&#39;Redundant_IDs___50&#39;</span><span class="p">][</span><span class="mi">27</span><span class="p">]</span>

        <span class="n">n_missing</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">missing</span><span class="p">)</span>
        <span class="n">n_false</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">false</span><span class="p">)</span>
        <span class="n">n_redundant</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">redundant</span><span class="p">)</span>

    <span class="n">ax0</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">tau_values</span><span class="p">,</span> <span class="n">n_missing</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">version</span><span class="p">)</span>
    <span class="n">ax0</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">tau_values</span><span class="p">,</span> <span class="n">n_missing</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">version</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">)</span>
    <span class="c1"># ax0.set_xlabel(&#39;$\\tau$&#39;)</span>
    <span class="n">ax0</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$N_</span><span class="si">{missing}</span><span class="s1">$&#39;</span><span class="p">)</span>

    <span class="n">ax1</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">tau_values</span><span class="p">,</span> <span class="n">n_false</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">version</span><span class="p">)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">tau_values</span><span class="p">,</span> <span class="n">n_false</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">version</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">)</span>

    <span class="c1"># ax1.set_xlabel(&#39;$\\tau$&#39;)</span>
    <span class="n">ax1</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$N_</span><span class="si">{incorrect}</span><span class="s1">$&#39;</span><span class="p">)</span>

    <span class="n">ax2</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">tau_values</span><span class="p">,</span> <span class="n">n_redundant</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">version</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">tau_values</span><span class="p">,</span> <span class="n">n_redundant</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">version</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;dashed&#39;</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">tau$&#39;</span><span class="p">)</span>
    <span class="n">ax2</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$N_</span><span class="si">{redundant}</span><span class="s1">$&#39;</span><span class="p">)</span>


<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="impact-of-the-filtering-algorithm">
<span id="impact-algorithm-appendix"></span><h2>Impact of the filtering algorithm<a class="headerlink" href="#impact-of-the-filtering-algorithm" title="Permalink to this headline">¶</a></h2>
<p>An advantage of the data association method proposed in <a class="reference internal" href="#data-association"><span class="std std-ref">Data association using confidence regions</span></a> is that it is very generic and does not constrain the tracking solution to any particular choice of filtering algorithm.
As for EKF, UKF implementations are already available to compute the distribution of <span class="math notranslate nohighlight">\(Z_k\)</span> given <span class="math notranslate nohighlight">\(Z_{1:k-1}\)</span> and the corresponding confidence regions (see [](tracking-module-appendix] above).
We propose a solution to compute this distribution when SMC is used, and performance comparisons between the EKF, UKF and SMC versions of our trackers are discussed.</p>
<div class="section" id="smc-based-tracking">
<h3>SMC-based tracking<a class="headerlink" href="#smc-based-tracking" title="Permalink to this headline">¶</a></h3>
<p>Denote <span class="math notranslate nohighlight">\(\filtdist_k\)</span> the filtering distribution of the HMM <span class="math notranslate nohighlight">\((X_k,Z_k)_{k \geq 1}\)</span>  (omitting the dependancy in observations for notation ease).
Using a set of samples <span class="math notranslate nohighlight">\(\{X_k^i\}_{1 \leq i \leq N}\)</span> and importance weights <span class="math notranslate nohighlight">\(\{w_k^i\}_{1 \leq i \leq N}\)</span>, SMC methods build an approximation of the following form:</p>
<div class="math notranslate nohighlight">
\[
\SMCfiltdist_k = \sum_{i=1}^N w_k^i \delta_{X_k^i} \approx \filtdist_k \eqsp.
\]</div>
<p>Contrary to EKF and UKF, the distribution <span class="math notranslate nohighlight">\(\mathbb{L}_k\)</span> of <span class="math notranslate nohighlight">\(Z_k\)</span> given <span class="math notranslate nohighlight">\(Z_{1:k-1}\)</span> is not directly available but can be obtained via an additional Monte Carlo sampling step.
Marginalizing over <span class="math notranslate nohighlight">\((X_{k-1}\)</span>, <span class="math notranslate nohighlight">\(X_k)\)</span> and using the conditional independence properties of HMMs, we decompose <span class="math notranslate nohighlight">\(\mathbb{L}_k\)</span> using the conditional state transition <span class="math notranslate nohighlight">\(\transdist_k(x,\rmd x')\)</span> and the likelihood of <span class="math notranslate nohighlight">\(Y_k\)</span> given <span class="math notranslate nohighlight">\(X_k\)</span>, denoted by <span class="math notranslate nohighlight">\(\likel_k(x, \rmd z)\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{L}_k = \int \int \likel_k(x', \cdot)\transdist_k(x, \rmd x')\filtdist_{k-1}(\rmd x) \eqsp.
\]</div>
<p>Replacing <span class="math notranslate nohighlight">\(\filtdist_k\)</span> with <span class="math notranslate nohighlight">\(\SMCfiltdist_k\)</span> into the previous equation yields</p>
<div class="math notranslate nohighlight">
\[
\SMCpredictdist_k = \sum_{k=1}^N w_k^i \int \likel_k(x,\cdot) \transdist_k(X_k^i, \rmd x)  \eqsp.
\]</div>
<p>In our model, the state transition is Gaussian and thererefore easy to sample from.
Thus an approximated predictive distribution <span class="math notranslate nohighlight">\(\MCpredictdist_k\)</span> can be obtained using Monte Carlo estimates built from random samples <span class="math notranslate nohighlight">\(\{X_k^{i,j}\}_{1 \leq i \leq N}^{1 \leq j \leq M}\)</span> drawn using the state transition, ie.
such that <span class="math notranslate nohighlight">\(\prob(X_k^{i,j} \in \rmd x) = \transdist_k(X_k^i,\rmd x)\)</span>.
This leads to</p>
<div class="math notranslate nohighlight">
\[
\MCpredictdist_k = \sum_{i=1}^N \sum_{j=1}^M w_k^i \likel_k(X_k^{i,j},\cdot) \eqsp.
\]</div>
<p>Since the observation likelihood is also Gaussian, <span class="math notranslate nohighlight">\(\MCpredictdist_k\)</span> is a Gaussian mixture, thus values of <span class="math notranslate nohighlight">\(\MCpredictdist_k(\mathsf{A})\)</span> for any <span class="math notranslate nohighlight">\(\mathsf{A} \subset \Rset^2\)</span> can be computed by applying the tools in <span class="xref myst"></span> to all mixture components.
Similar to EKF and UKF, this approximated predictive distribution is used to recover object identities via <span class="math notranslate nohighlight">\(\MCpredictdist_n^{\ell}(V_\delta(z_n^i))\)</span> computed for all incoming detections <span class="math notranslate nohighlight">\(\detectset_n = \{z_n^i\}_{1 \leq i \leq D_n}\)</span> and each of the <span class="math notranslate nohighlight">\(1 \leq \ell \leq L_n\)</span> filters, where <span class="math notranslate nohighlight">\(\MCpredictdist_n^{\ell}\)</span> is the predictive distribution associated with the <span class="math notranslate nohighlight">\(\ell\)</span>-th filter.</p>
</div>
<div class="section" id="performance-comparison">
<h3>Performance comparison<a class="headerlink" href="#performance-comparison" title="Permalink to this headline">¶</a></h3>
<p>In theory, sampling-based methods like UKF and SMC are better suited for nonlinear state-space models like the one we propose in [](bayesian-tracking}.
However, we observe very few differences in count results when upgrading from EKF to UKF to SMC.
In practise, there is no difference at all between our EKF and UKF implementations, which show strictly identical values for <span class="math notranslate nohighlight">\(\Ntrue\)</span>, <span class="math notranslate nohighlight">\(\Nfalse\)</span> and <span class="math notranslate nohighlight">\(\Nred\)</span>.
For the SMC version, values for <span class="math notranslate nohighlight">\(\Nfalse\)</span> and <span class="math notranslate nohighlight">\(\Nred\)</span> improve by a very small amount (2 and 1, respectively), but <span class="math notranslate nohighlight">\(\Nmis\)</span> is slightly worse (one more object missed), and these results depend loosely on the number of samples used to approximate the filtering distributions and the number of samples for the Monte Carlo scheme.
Therefore, our motion estimates via the optical flow <span class="math notranslate nohighlight">\(\Delta_n\)</span> prove very reliable in our application context, so much that EKF, though suboptimal, brings equivalent results.
This comforts us into keeping it as a faster and computationally simpler option.
That said, this conclusion might not hold in scenarios where camera motion is even stronger, which was our main motivation to develop a flexible tracking solution and to provide implementations of UKF and SMC versions.
This allows easier extension of our work to more challenging data.</p>
<p id="id49"><dl class="citation">
<dt class="label" id="id59"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Roland Geyer, Jenna Jambeck, and Kara Law. Production, use, and fate of all plastics ever made. <em>Science Advances</em>, 3:e1700782, 07 2017. <a class="reference external" href="https://doi.org/10.1126/sciadv.1700782">doi:10.1126/sciadv.1700782</a>.</p>
</dd>
<dt class="label" id="id57"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>Natalie Welden. The environmental impacts of plastic pollution. pages 195–222, 01 2020. <a class="reference external" href="https://doi.org/10.1016/B978-0-12-817880-5.00008-6">doi:10.1016/B978-0-12-817880-5.00008-6</a>.</p>
</dd>
<dt class="label" id="id58"><span class="brackets"><a class="fn-backref" href="#id2">3</a></span></dt>
<dd><p>Thushari Gamage and J.D.M. Senevirathna. Plastic pollution in the marine environment. <em>Heliyon</em>, 6:e04709, 08 2020. <a class="reference external" href="https://doi.org/10.1016/j.heliyon.2020.e04709">doi:10.1016/j.heliyon.2020.e04709</a>.</p>
</dd>
<dt class="label" id="id54"><span class="brackets"><a class="fn-backref" href="#id3">4</a></span></dt>
<dd><p>Chelsea Rochman, Anthony Andrady, Sarah Dudas, Joan Fabres, François Galgani, Denise lead, Valeria Hidalgo-Ruz, Sunny Hong, Peter Kershaw, Laurent Lebreton, Amy Lusher, Ramani Narayan, Sabine Pahl, James Potemra, Chelsea Rochman, Sheck Sherif, Joni Seager, Won Shim, Paula Sobral, and Linda Amaral-Zettler. Sources, fate and effects of microplastics in the marine environment: part 2 of a global assessment. pages, 12 2016.</p>
</dd>
<dt class="label" id="id56"><span class="brackets"><a class="fn-backref" href="#id4">5</a></span></dt>
<dd><p>Jenna Jambeck, Roland Geyer, Chris Wilcox, Theodore Siegler, Miriam Perryman, Anthony Andrady, Ramani Narayan, and Kara Law. Marine pollution. plastic waste inputs from land into the ocean. <em>Science (New York, N.Y.)</em>, 347:768–771, 02 2015. <a class="reference external" href="https://doi.org/10.1126/science.1260352">doi:10.1126/science.1260352</a>.</p>
</dd>
<dt class="label" id="id77"><span class="brackets"><a class="fn-backref" href="#id5">6</a></span></dt>
<dd><p>Antoine Bruge, Cristina Barreau, Jérémy Carlot, Hélène Collin, Clément Moreno, and Philippe Maison. Monitoring litter inputs from the Adour river (southwest France) to the marine environment. <em>Journal of Marine Science and Engineering</em>, 2018. <a class="reference external" href="https://doi.org/10.3390/jmse6010024">doi:10.3390/jmse6010024</a>.</p>
</dd>
<dt class="label" id="id55"><span class="brackets"><a class="fn-backref" href="#id6">7</a></span></dt>
<dd><p>D. González-Fernández, A. Cózar, G. Hanke, J. Viejo, C. Morales-Caselles, R. Bakiu, D. Barcelo, F. Bessa, Antoine Bruge, M. Cabrera, J. Castro-Jiménez, M. Constant, R. Crosti, Yuri Galletti, A. Kideyş, N. Machitadze, Joana Pereira de Brito, M. Pogojeva, N. Ratola, J. Rigueira, E. Rojo-Nieto, O. Savenko, R. I. Schöneich-Argent, G. Siedlewicz, Giuseppe Suaria, and Myrto Tourgeli. Floating macrolitter leaked from europe into the ocean. <em>Nature Sustainability</em>, 4:474 – 483, 2021.</p>
</dd>
<dt class="label" id="id88"><span class="brackets"><a class="fn-backref" href="#id7">8</a></span></dt>
<dd><p>Johnny Gasperi, Rachid Dris, Tiffany Bonin, Vincent Rocher, and Bruno Tassin. Assessment of floating plastic debris in surface water along the seine river. <em>Environmental Pollution</em>, 195:163 – 166, 2014. URL: <a class="reference external" href="http://www.sciencedirect.com/science/article/pii/S0269749114003807">http://www.sciencedirect.com/science/article/pii/S0269749114003807</a>, <a class="reference external" href="https://doi.org/https://doi.org/10.1016/j.envpol.2014.09.001">doi:https://doi.org/10.1016/j.envpol.2014.09.001</a>.</p>
</dd>
<dt class="label" id="id89"><span class="brackets"><a class="fn-backref" href="#id8">9</a></span></dt>
<dd><p>David Morritt, Paris V. Stefanoudis, Dave Pearce, Oliver A. Crimmen, and Paul F. Clark. Plastic in the thames: a river runs through it. <em>Marine Pollution Bulletin</em>, 78(1):196–200, 2014. URL: <a class="reference external" href="https://www.sciencedirect.com/science/article/pii/S0025326X13006565">https://www.sciencedirect.com/science/article/pii/S0025326X13006565</a>, <a class="reference external" href="https://doi.org/https://doi.org/10.1016/j.marpolbul.2013.10.035">doi:https://doi.org/10.1016/j.marpolbul.2013.10.035</a>.</p>
</dd>
<dt class="label" id="id78"><span class="brackets"><a class="fn-backref" href="#id9">10</a></span></dt>
<dd><p>Tim van Emmerik, Romain Tramoy, Caroline van Calcar, Soline Alligant, Robin Treilles, Bruno Tassin, and Johnny Gasperi. Seine Plastic Debris Transport Tenfolded During Increased River Discharge. <em>Frontiers in Marine Science</em>, 6(October):1–7, 2019. <a class="reference external" href="https://doi.org/10.3389/fmars.2019.00642">doi:10.3389/fmars.2019.00642</a>.</p>
</dd>
<dt class="label" id="id79"><span class="brackets"><a class="fn-backref" href="#id9">11</a></span></dt>
<dd><p>Tim van Emmerik and Anna Schwarz. Plastic debris in rivers. <em>WIREs Water</em>, 7(1):e1398, 2020. URL: <a class="reference external" href="https://onlinelibrary.wiley.com/doi/abs/10.1002/wat2.1398">https://onlinelibrary.wiley.com/doi/abs/10.1002/wat2.1398</a>, <a class="reference external" href="https://arxiv.org/abs/https://onlinelibrary.wiley.com/doi/pdf/10.1002/wat2.1398">arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1002/wat2.1398</a>, <a class="reference external" href="https://doi.org/https://doi.org/10.1002/wat2.1398">doi:https://doi.org/10.1002/wat2.1398</a>.</p>
</dd>
<dt class="label" id="id95"><span class="brackets">12</span><span class="fn-backref">(<a href="#id10">1</a>,<a href="#id17">2</a>,<a href="#id36">3</a>,<a href="#id43">4</a>)</span></dt>
<dd><p>Pedro F Proença and Pedro Simões. TACO: Trash Annotations in Context for Litter Detection. 2020. URL: <a class="reference external" href="http://tacodataset.org/ http://arxiv.org/abs/2003.06975">http://tacodataset.org/ http://arxiv.org/abs/2003.06975</a>, <a class="reference external" href="https://arxiv.org/abs/2003.06975">arXiv:2003.06975</a>.</p>
</dd>
<dt class="label" id="id73"><span class="brackets">13</span><span class="fn-backref">(<a href="#id11">1</a>,<a href="#id28">2</a>)</span></dt>
<dd><p>Gioele Ciaparrone, Francisco Luque Sánchez, Siham Tabik, Luigi Troiano, Roberto Tagliaferri, and Francisco Herrera. Deep learning in video multi-object tracking: A survey. <em>Neurocomputing</em>, 381:61–88, 2020. <a class="reference external" href="https://arxiv.org/abs/1907.12740">arXiv:1907.12740</a>, <a class="reference external" href="https://doi.org/10.1016/j.neucom.2019.11.023">doi:10.1016/j.neucom.2019.11.023</a>.</p>
</dd>
<dt class="label" id="id96"><span class="brackets"><a class="fn-backref" href="#id12">14</a></span></dt>
<dd><p>P. Bergmann, T. Meinhardt, and L. Leal-Taixe. Tracking without bells and whistles. In <em>2019 IEEE/CVF International Conference on Computer Vision (ICCV)</em>, volume, 941–951. Los Alamitos, CA, USA, nov 2019. IEEE Computer Society. URL: <a class="reference external" href="https://doi.ieeecomputersociety.org/10.1109/ICCV.2019.00103">https://doi.ieeecomputersociety.org/10.1109/ICCV.2019.00103</a>, <a class="reference external" href="https://doi.org/10.1109/ICCV.2019.00103">doi:10.1109/ICCV.2019.00103</a>.</p>
</dd>
<dt class="label" id="id97"><span class="brackets"><a class="fn-backref" href="#id13">15</a></span></dt>
<dd><p>Jonathon Luiten, Aljosa Osep, Patrick Dendorfer, Philip Torr, Andreas Geiger, Laura Leal-Taixé, and Bastian Leibe. Hota: a higher order metric for evaluating multi-object tracking. <em>International journal of computer vision</em>, 129(2):548–578, 2021.</p>
</dd>
<dt class="label" id="id85"><span class="brackets"><a class="fn-backref" href="#id14">16</a></span></dt>
<dd><p>Prithvijit Chattopadhyay, Ramakrishna Vedantam, Ramprasaath R Selvaraju, Dhruv Batra, and Devi Parikh. Counting everyday objects in everyday scenes. In <em>Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017</em>, volume 2017-Janua, 4428–4437. 2017. <a class="reference external" href="https://arxiv.org/abs/1604.03505">arXiv:1604.03505</a>, <a class="reference external" href="https://doi.org/10.1109/CVPR.2017.471">doi:10.1109/CVPR.2017.471</a>.</p>
</dd>
<dt class="label" id="id92"><span class="brackets"><a class="fn-backref" href="#id15">17</a></span></dt>
<dd><p>Carlos Arteta, Victor Lempitsky, and Andrew Zisserman. Counting in the wild. 9911:483–498, 10 2016. <a class="reference external" href="https://doi.org/10.1007/978-3-319-46478-7_30">doi:10.1007/978-3-319-46478-7_30</a>.</p>
</dd>
<dt class="label" id="id93"><span class="brackets"><a class="fn-backref" href="#id16">18</a></span></dt>
<dd><p>Xingjiao Wu, Baohan Xu, Yingbin Zheng, Hao Ye, Jing Yang, and Liang He. Fast video crowd counting with a temporal aware network. <em>Neurocomputing</em>, 403:13–20, 2020. URL: <a class="reference external" href="https://www.sciencedirect.com/science/article/pii/S0925231220306561">https://www.sciencedirect.com/science/article/pii/S0925231220306561</a>, <a class="reference external" href="https://doi.org/https://doi.org/10.1016/j.neucom.2020.04.071">doi:https://doi.org/10.1016/j.neucom.2020.04.071</a>.</p>
</dd>
<dt class="label" id="id100"><span class="brackets"><a class="fn-backref" href="#id16">19</a></span></dt>
<dd><p>Feng Xiong, Xingjian Shi, and Dit-Yan Yeung. Spatiotemporal modeling for crowd counting in videos. In <em>2017 IEEE International Conference on Computer Vision (ICCV)</em>, volume, 5161–5169. 2017. <a class="reference external" href="https://doi.org/10.1109/ICCV.2017.551">doi:10.1109/ICCV.2017.551</a>.</p>
</dd>
<dt class="label" id="id101"><span class="brackets"><a class="fn-backref" href="#id16">20</a></span></dt>
<dd><p>Yunqi Miao, Jungong Han, Yongsheng Gao, and Baochang Zhang. ST-CNN: Spatial-Temporal Convolutional Neural Network for crowd counting in videos. <em>Pattern Recognition Letters</em>, 125:113–118, jul 2019. <a class="reference external" href="https://doi.org/10.1016/j.patrec.2019.04.012">doi:10.1016/j.patrec.2019.04.012</a>.</p>
</dd>
<dt class="label" id="id84"><span class="brackets"><a class="fn-backref" href="#id18">21</a></span></dt>
<dd><p>Mattis Wolf, Katelijn van den Berg, Shungudzemwoyo Pascal Garaba, Nina Gnann, Klaus Sattler, Frederic Theodor Stahl, and Oliver Zielinski. Machine learning for aquatic plastic litter detection, classification and quantification (APLASTIC–Q). <em>Environmental Research Letters</em>, 2020. <a class="reference external" href="https://doi.org/10.1088/1748-9326/abbd01">doi:10.1088/1748-9326/abbd01</a>.</p>
</dd>
<dt class="label" id="id94"><span class="brackets">22</span><span class="fn-backref">(<a href="#id19">1</a>,<a href="#id31">2</a>)</span></dt>
<dd><p>Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: towards real-time object detection with region proposal networks. In <em>Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1</em>, NIPS'15, 91–99. Cambridge, MA, USA, 2015. MIT Press.</p>
</dd>
<dt class="label" id="id90"><span class="brackets"><a class="fn-backref" href="#id20">23</a></span></dt>
<dd><p>Colin van Lieshout, Kees van Oeveren, Tim van Emmerik, and Eric Postma. Automated River Plastic Monitoring Using Deep Learning and Cameras. <em>Earth and Space Science</em>, 7(8):e2019EA000960, 2020. <a class="reference external" href="https://doi.org/10.1029/2019EA000960">doi:10.1029/2019EA000960</a>.</p>
</dd>
<dt class="label" id="id99"><span class="brackets"><a class="fn-backref" href="#id21">24</a></span></dt>
<dd><p>Jungseok Hong, Michael Fulton, and Junaed Sattar. A Generative Approach Towards Improved Robotic Detection of Marine Litter. In <em>Proceedings - IEEE International Conference on Robotics and Automation</em>, 10525–10531. 2020. <a class="reference external" href="https://arxiv.org/abs/1910.04754">arXiv:1910.04754</a>, <a class="reference external" href="https://doi.org/10.1109/ICRA40945.2020.9197575">doi:10.1109/ICRA40945.2020.9197575</a>.</p>
</dd>
<dt class="label" id="id102"><span class="brackets"><a class="fn-backref" href="#id22">25</a></span></dt>
<dd><p>Wenhan Luo, Junliang Xing, Anton Milan, Xiaoqin Zhang, Wei Liu, and Tae-Kyun Kim. Multiple object tracking: a literature review. <em>Artificial Intelligence</em>, 293:103448, 2021. URL: <a class="reference external" href="https://www.sciencedirect.com/science/article/pii/S0004370220301958">https://www.sciencedirect.com/science/article/pii/S0004370220301958</a>, <a class="reference external" href="https://doi.org/https://doi.org/10.1016/j.artint.2020.103448">doi:https://doi.org/10.1016/j.artint.2020.103448</a>.</p>
</dd>
<dt class="label" id="id103"><span class="brackets">26</span><span class="fn-backref">(<a href="#id23">1</a>,<a href="#id44">2</a>)</span></dt>
<dd><p>Alex Bewley, Zongyuan Ge, Lionel Ott, Fabio Ramos, and Ben Upcroft. Simple online and realtime tracking. In <em>Proceedings - International Conference on Image Processing, ICIP</em>, volume 2016-Augus, 3464–3468. 2016. URL: <a class="reference external" href="https://github.com/abewley/sort">https://github.com/abewley/sort</a>, <a class="reference external" href="https://arxiv.org/abs/1602.00763">arXiv:1602.00763</a>, <a class="reference external" href="https://doi.org/10.1109/ICIP.2016.7533003">doi:10.1109/ICIP.2016.7533003</a>.</p>
</dd>
<dt class="label" id="id104"><span class="brackets"><a class="fn-backref" href="#id24">27</a></span></dt>
<dd><p>Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. Nuscenes: A multimodal dataset for autonomous driving. In <em>Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</em>, 11618–11628. 2020. <a class="reference external" href="https://arxiv.org/abs/1903.11027">arXiv:1903.11027</a>, <a class="reference external" href="https://doi.org/10.1109/CVPR42600.2020.01164">doi:10.1109/CVPR42600.2020.01164</a>.</p>
</dd>
<dt class="label" id="id105"><span class="brackets"><a class="fn-backref" href="#id24">28</a></span></dt>
<dd><p>Patrick Dendorfer, Hamid Rezatofighi, Anton Milan, Javen Shi, Daniel Cremers, Ian Reid, Stefan Roth, Konrad Schindler, and Laura Leal-Taixé. MOT20: A benchmark for multi object tracking in crowded scenes. 2020. <a class="reference external" href="https://arxiv.org/abs/2003.09003">arXiv:2003.09003</a>.</p>
</dd>
<dt class="label" id="id106"><span class="brackets">29</span><span class="fn-backref">(<a href="#id25">1</a>,<a href="#id45">2</a>)</span></dt>
<dd><p>Nicolai Wojke, Alex Bewley, and Dietrich Paulus. Simple online and realtime tracking with a deep association metric. In <em>Proceedings - International Conference on Image Processing, ICIP</em>, volume 2017-Septe, 3645–3649. 2018. <a class="reference external" href="https://arxiv.org/abs/1703.07402">arXiv:1703.07402</a>, <a class="reference external" href="https://doi.org/10.1109/ICIP.2017.8296962">doi:10.1109/ICIP.2017.8296962</a>.</p>
</dd>
<dt class="label" id="id74"><span class="brackets">30</span><span class="fn-backref">(<a href="#id25">1</a>,<a href="#id33">2</a>,<a href="#id46">3</a>)</span></dt>
<dd><p>Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng, and Wenyu Liu. Fairmot: on the fairness of detection and re-identification in multiple object tracking. <em>International Journal of Computer Vision</em>, pages 1–19, 2021.</p>
</dd>
<dt class="label" id="id107"><span class="brackets"><a class="fn-backref" href="#id26">31</a></span></dt>
<dd><p>Peng Chu, Jiang Wang, Quanzeng You, Haibin Ling, and Zicheng Liu. TransMOT: Spatial-Temporal Graph Transformer for Multiple Object Tracking. 2021. URL: <a class="reference external" href="http://arxiv.org/abs/2104.00194">http://arxiv.org/abs/2104.00194</a>, <a class="reference external" href="https://arxiv.org/abs/2104.00194">arXiv:2104.00194</a>.</p>
</dd>
<dt class="label" id="id108"><span class="brackets"><a class="fn-backref" href="#id27">32</a></span></dt>
<dd><p>Yongxin Wang, Kris Kitani, and Xinshuo Weng. Joint object detection and multi-object tracking with graph neural networks. May 2021.</p>
</dd>
<dt class="label" id="id98"><span class="brackets"><a class="fn-backref" href="#id29">33</a></span></dt>
<dd><p>Michael Fulton, Jungseok Hong, Md Jahidul Islam, and Junaed Sattar. Robotic detection of marine litter using deep visual detection models. In <em>2019 International Conference on Robotics and Automation (ICRA)</em>, 5752–5758. IEEE, 2019.</p>
</dd>
<dt class="label" id="id71"><span class="brackets">34</span><span class="fn-backref">(<a href="#id32">1</a>,<a href="#id34">2</a>)</span></dt>
<dd><p>Xingyi Zhou, Dequan Wang, and Philipp Krähenbühl. Objects as points. apr 2019. URL: <a class="reference external" href="http://arxiv.org/abs/1904.07850">http://arxiv.org/abs/1904.07850</a>, <a class="reference external" href="https://arxiv.org/abs/1904.07850">arXiv:1904.07850</a>.</p>
</dd>
<dt class="label" id="id86"><span class="brackets"><a class="fn-backref" href="#id32">35</a></span></dt>
<dd><p>Hei Law and Jia Deng. Cornernet: detecting objects as paired keypoints. In Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss, editors, <em>Computer Vision - ECCV 2018 - 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part XIV</em>, volume 11218 of Lecture Notes in Computer Science, 765–781. Springer, 2018. URL: <a class="reference external" href="https://doi.org/10.1007/978-3-030-01264-9\_45">https://doi.org/10.1007/978-3-030-01264-9\_45</a>, <a class="reference external" href="https://doi.org/10.1007/978-3-030-01264-9\_45">doi:10.1007/978-3-030-01264-9\_45</a>.</p>
</dd>
<dt class="label" id="id52"><span class="brackets"><a class="fn-backref" href="#id35">36</a></span></dt>
<dd><p>Fisher Yu, Dequan Wang, Evan Shelhamer, and Trevor Darrell. Deep layer aggregation. In <em>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, volume, 2403–2412. 2018. <a class="reference external" href="https://doi.org/10.1109/CVPR.2018.00255">doi:10.1109/CVPR.2018.00255</a>.</p>
</dd>
<dt class="label" id="id53"><span class="brackets"><a class="fn-backref" href="#id37">37</a></span></dt>
<dd><p>Nikos Paragios, Yunmei Chen, and Olivier D Faugeras. <em>Handbook of mathematical models in computer vision</em>. Springer Science &amp; Business Media, 2006.</p>
</dd>
<dt class="label" id="id63"><span class="brackets"><a class="fn-backref" href="#id38">38</a></span></dt>
<dd><p>G. Farnebäck. Two-frame motion estimation based on polynomial expansion. In <em>Scandinavian conference on Image analysis</em>, 363–370. Springer, 2003.</p>
</dd>
<dt class="label" id="id60"><span class="brackets"><a class="fn-backref" href="#id39">39</a></span></dt>
<dd><p>S. Särkkä. <em>Bayesian Filtering and Smoothing</em>. Cambridge University Press, New York, NY, USA, 2013.</p>
</dd>
<dt class="label" id="id64"><span class="brackets"><a class="fn-backref" href="#id40">40</a></span></dt>
<dd><p>H. W. Kuhn. The hungarian method for the assignment problem. <em>Naval Research Logistics Quarterly</em>, 2(1-2):83–97, 1955. URL: <a class="reference external" href="https://onlinelibrary.wiley.com/doi/abs/10.1002/nav.3800020109">https://onlinelibrary.wiley.com/doi/abs/10.1002/nav.3800020109</a>, <a class="reference external" href="https://arxiv.org/abs/https://onlinelibrary.wiley.com/doi/pdf/10.1002/nav.3800020109">arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1002/nav.3800020109</a>, <a class="reference external" href="https://doi.org/https://doi.org/10.1002/nav.3800020109">doi:https://doi.org/10.1002/nav.3800020109</a>.</p>
</dd>
<dt class="label" id="id109"><span class="brackets"><a class="fn-backref" href="#id41">41</a></span></dt>
<dd><p>Keni Bernardin and Rainer Stiefelhagen. Evaluating multiple object tracking performance: the clear mot metrics. <em>EURASIP Journal on Image and Video Processing</em>, 2008:, 01 2008. <a class="reference external" href="https://doi.org/10.1155/2008/246309">doi:10.1155/2008/246309</a>.</p>
</dd>
<dt class="label" id="id110"><span class="brackets"><a class="fn-backref" href="#id42">42</a></span></dt>
<dd><p>Ergys Ristani, Francesco Solera, Roger Zou, Rita Cucchiara, and Carlo Tomasi. Performance measures and a data set for multi-target, multi-camera tracking. In <em>European conference on computer vision</em>, 17–35. Springer, 2016.</p>
</dd>
<dt class="label" id="id75"><span class="brackets"><a class="fn-backref" href="#id47">43</a></span></dt>
<dd><p>Xingyi Zhou, Vladlen Koltun, and Philipp Krähenbühl. Tracking objects as points. pages 474–490, 10 2020. <a class="reference external" href="https://doi.org/10.1007/978-3-030-58548-8_28">doi:10.1007/978-3-030-58548-8_28</a>.</p>
</dd>
</dl>
</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By The Jupyter Book Community<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>